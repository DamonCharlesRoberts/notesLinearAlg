# Matrices, Part II

## Key points

## Matrix Norms

::: {.callout-tip title="Recall"}
- Euclidean distance:
$$
  ||v|| = \sqrt{\sum_{i=1}^N v_i^2}
$$
- Vector norms:
  - A vector's Euclidean geometric length.
:::

- A matrix norm is not the same thing as a vector norm.
- Denoted as $\parallel A \parallel$
- Similar to vector norms in that each norm is a number that characterizes a matrix.
- Two main families of matrix norms:
  - Element-wise norms
    - Computed based on the individual elements of the matrix.
    - Reflect the magnitudes of the elements in the matrix.
  - Induced norms
    - One key function of a matrix is to encode transformations on a vector.
    - Given this, the induced norm of a matrix measures how much that transformation scales (stretches or shrinks) the vector.
## Euclidean norm (an example of a element-wise norm)
- AKA Frobenius norm or the $\mathcal{L}$ norm (the regularized norm)
$$
  \paralell A \parallel_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N a_{i,j}^2}
$$
- In words, it is the square root of the sum of all matrix elements squared.
- This norm in particular, but all norms, generally, are used quite a lot in machine learning and regularization.
- Also used as a measure of distance from another matrix, "Matrix Distance":
$$
C = A - B
$$
  - Where if $B=A$ then $C=0$ because the numerical values in the matrices are similar but if $B \neq A$ then $C$ increases when the numerical values get increasingly dissimilar.

### Matrix trace

::: {.callout-note title="Definitions"}
- Trace
  - the sum of its diagonal elements.
  - denoted as $tr(A)$.
:::

- Numerical example:
  - The trace of the two following matrices is $14$.
$$
\begin{bmatrix}
  4&5&6\\
  0&1&4\\
  9&9&9\\
\end{bmatrix}, \begin{bmatrix}
  0&0&0\\
  0&8&0\\
  1&2&6\\
\end{bmatrix}
$$

- The trace of a matrix equals the sum of it's eigenvalues.
  - A measure of the volume of its eigenspace.
- The Frobenius norm can be calculated as the square root of the trace of t he matrix times its transpose. This happens because each diagonal element of the matrix $A^TA$ is defined by the dot product of each row with itself:
$$
  \paralell A \parallel_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N a_{i,j}^2} = \sqrt{tr(A^T)}
$$

## Matrix spaces

### Colum space
- Conceptualize the matrix as a set of column vectors.
- Consider the infinity of real-valued scalars instead of working with a specific set of scalars.
  - An infinite number of scalars gives an infinite number of ways to combine a set of vectors.
  - This resulting set of vectors that can be created from the weighted combination of the columns in the matrix is the column space.
- Denoted as $C(A)$
- Numerical examples:
$$
C(\begin{bmatrix}1\\3\\\end{bmatrix}) = \lambda\begin{bmatrix}1\\3\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - $\lambda$, the scalar, can be any real number as there are any scalar can produce a vector produced by $A$.

$$
C(\begin{bmatrix}1&1\\3&2\\\end{bmatrix}) = \lambda_1\begin{bmatrix}1\\3\\\end{bmatrix} + \lambda_2\begin{bmatrix}1\\2\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - What is the set of vectors that can be created with some linear combination of these two column vectors?
    - All vectors in $\mathbb{R}^2.

$$
C(\begin{bmatrix}1&2\\3&6\\\end{bmatrix}) = \lambda_1\begin{bmatrix}1\\3\\\end{bmatrix} + \lambda_2\begin{bmatrix}2\\6\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - In this example, it is not possible to be in $\mathbb{R}^2$ as these columns are linearly dependent to each other -- they are collinear. THis is because one is already a scaled version of the other.
    - This means that the column space of this matrix is just a 1D subspace.
- The dimensionality of the column space equals the number of columns only if they form a linearly independent set.

## Row space
- The same concept but we consider all possible weighted combinations of the rows instead of the columns.
- Can be $R(A)$ or $C(A^T)$.

## Null spaces
- The column space can be summarized as $Ax = b$.
  - In words, "Can we find some set of coefficients in x such that the weighted combination of columns in A produce vector b"? If yes, then $b \in C(A)$.
- The null space, however is summarized as $Ay = 0$
  - In words, "Can we find some set of coefficients in y such that the weighted combination of columns in A produces the zeros vector?"
- Numerical examples:

$$
\begin{aligned*}
  A = \begin{bmatrix}1&-1\\-2&2\end{bmatrix} \\
  Ay = 0
  y = \begin{bmatrix}-1\\1\\\end{bmatrix} \\
  N(A) = \lambda\begin{bmatrix}1\\1\\\end{bmatrix}, \lambda \in \mathbb{R}
\end{aligned*}
$$
  - There are an infinite number of vectors in y that satisfy $Ay=0$ for $A$. Thus we can express the null space matrix $N(A)$ how we did above.

$$
\begin{aligned*}
  \begin{bmatrix}1&-1\\-2&3\\\end{bmatrix} \\
  Ay = 0 \\
  y = \begin{bmatrix}\end{bmatrix} \\
  N(A) = \begin{bmatrix}\end{bmatrix}
\end{aligned*}
$$
  - This matrix does not allow us to produce a null space matrix.
- Important note:
  - Linear independent columns in a matrix produce a column space greater than 1 but no larger than the number of linearly independent columns whereas linear dependent columns produce a column space equal to 1, the unscaled vector.
  - Using terminology that is explained below: full-rank and full column-rank matrices have empty null spaces whereas reduced rank-matrices have nonempty null spaces.

### Unit vector
- They are convenient and are numerically stable. So they are common as bases for subspaces.

## Rank
- Related to the dimensionalities of matrix subspaces and is important for things like inverting matrices and determining the number of solutions to a system of equations.
- Properties of rank:
  - Rank is a nonnegative integer.
  - Every matrix has one unique rank.
  - The rank of a matrix is denoted with $r(A)$
  - The maximum possible rank of a matrix is equal to either the smaller of the row or column count, $min\{M,N\}$
  - A matrix with a maximum rank, $r = min\{M,N\}$, is called "full rank"
  - A matrix with a rank smaller than the full rank, $r < min\{M,N\}$ is called reduced rank, rank deficient, or singular.
  - Scalar multiplication does not impact matrix rank
- Interpreting the rank of a matrix
  - The largest number of columns (or rows) that form a linearly independent set.
  - OR The dimensionality of the column space.
  - OR The number of dimensions containing information in the matrix.
  - OR The number of nonzero singular values of the matrix.
