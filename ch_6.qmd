# Matrices, Part II

```{julia}
#| label: setup
# Loading packages
using LinearAlgebra
```

## Key points
- There are induced and elementwise matrix norms. The former reflects magnitudes of th elements and the latter reflects hte geometric-transformative effect of the matrix on vectors.
- The most commonly used elementwise norm is the Euclidean, Frobenius, or $\mathcal{L}$ norm.
- The trace is the sum of its diagonal elements
- There are four matrix spaces (column, row, null, right-null) and they are the linear weighted combinations of different features of it.
- The column space comprises all linear weighted combinations of the columns.
- Often we want to know if some vector, $b$ is in th column space of a matrix where we can find a vector $x$ such that $Ax=b$
- The null space is the set of vectors that linearly combines the columns producing the zeros vector. It is important for finding eigenvectors.
- The null space occurs when a set of vectors are linearly dependent and reflects a singular (rank-reduced) matrix.
- Shifting a square matrix by adding a constant to the diagonal ensures full-rank.
- The determinant is a number to describe square matrices. It is zero for all singular matrices and nonzero for full-rank matrices.
- The characteristic polynomial transforms square matricesl, shifting it by $\lambda$, into an algebraic equation equal to the determinant so that we can solve by $\lambda$

## Matrix Norms

::: {.callout-tip title="Recall"}
- Euclidean distance:
$$
  ||v|| = \sqrt{\sum_{i=1}^N v_i^2}
$$
- Vector norms:
  - A vector's Euclidean geometric length.
:::

- A matrix norm is not the same thing as a vector norm.
- Denoted as $\parallel A \parallel$
- Similar to vector norms in that each norm is a number that characterizes a matrix.
- Two main families of matrix norms:
  - Element-wise norms
    - Computed based on the individual elements of the matrix.
    - Reflect the magnitudes of the elements in the matrix.
  - Induced norms
    - One key function of a matrix is to encode transformations on a vector.
    - Given this, the induced norm of a matrix measures how much that transformation scales (stretches or shrinks) the vector.

## Euclidean norm (an example of a element-wise norm)
- AKA Frobenius norm or the $\mathcal{L}$ norm (the regularized norm)
$$
  \paralell A \parallel_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N a_{i,j}^2}
$$
- In words, it is the square root of the sum of all matrix elements squared.
- This norm in particular, but all norms, generally, are used quite a lot in machine learning and regularization.
- Also used as a measure of distance from another matrix, "Matrix Distance":
$$
C = A - B
$$
  - Where if $B=A$ then $C=0$ because the numerical values in the matrices are similar but if $B \neq A$ then $C$ increases when the numerical values get increasingly dissimilar.
- Example

```{julia}
# Define a matrix.
A = [[1,2] [3,4]]
println("A:$A")
# Calculate the Frobenius norm.
fro_norm = LinearAlgebra.norm(A)
println("||A||=$fro_norm")
```

### Matrix trace

::: {.callout-note title="Definitions"}
- Trace
  - the sum of its diagonal elements.
  - denoted as $tr(A)$.
:::

- Numerical example:
  - The trace of the two following matrices is $14$.
$$
\begin{bmatrix}
  4&5&6\\
  0&1&4\\
  9&9&9\\
\end{bmatrix}, \begin{bmatrix}
  0&0&0\\
  0&8&0\\
  1&2&6\\
\end{bmatrix}
$$

- The trace of a matrix equals the sum of it's eigenvalues.
  - A measure of the volume of its eigenspace.
- The Frobenius norm can be calculated as the square root of the trace of t he matrix times its transpose. This happens because each diagonal element of the matrix $A^TA$ is defined by the dot product of each row with itself:
$$
  \parallel A \parallel_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N a_{i,j}^2} = \sqrt{tr(A^T)}
$$

```{julia}
#| label: trace
# Define the matrix.
A = [[4,0,9] [5,1,9] [6,4,9]]
println("A:$A")
# Calculate the trace.
#   - With the function.
trace_A = LinearAlgebra.tr(A)
# Print trace.
println("tr(A) = $trace_A")
```

## Matrix spaces

### Colum space
- Conceptualize the matrix as a set of column vectors.
- Consider the infinity of real-valued scalars instead of working with a specific set of scalars.
  - An infinite number of scalars gives an infinite number of ways to combine a set of vectors.
  - This resulting set of vectors that can be created from the weighted combination of the columns in the matrix is the column space.
- Denoted as $C(A)$
- Numerical examples:
$$
C(\begin{bmatrix}1\\3\\\end{bmatrix}) = \lambda\begin{bmatrix}1\\3\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - $\lambda$, the scalar, can be any real number as there are any scalar can produce a vector produced by $A$.
$$
C(\begin{bmatrix}1&1\\3&2\\\end{bmatrix}) = \lambda_1\begin{bmatrix}1\\3\\\end{bmatrix} + \lambda_2\begin{bmatrix}1\\2\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - What is the set of vectors that can be created with some linear combination of these two column vectors?
    - All vectors in $\mathbb{R}^2.

$$
C(\begin{bmatrix}1&2\\3&6\\\end{bmatrix}) = \lambda_1\begin{bmatrix}1\\3\\\end{bmatrix} + \lambda_2\begin{bmatrix}2\\6\\\end{bmatrix}, \lambda \in \mathbb{R}
$$
  - In this example, it is not possible to be in $\mathbb{R}^2$ as these columns are linearly dependent to each other -- they are collinear. THis is because one is already a scaled version of the other.
    - This means that the column space of this matrix is just a 1D subspace.
- The dimensionality of the column space equals the number of columns only if they form a linearly independent set.

## Row space
- The same concept but we consider all possible weighted combinations of the rows instead of the columns.
- Can be $R(A)$ or $C(A^T)$.

## Null spaces
- The column space can be summarized as $Ax = b$.
  - In words, "Can we find some set of coefficients in x such that the weighted combination of columns in A produce vector b"? If yes, then $b \in C(A)$.
- The null space, however is summarized as $Ay = 0$
  - In words, "Can we find some set of coefficients in y such that the weighted combination of columns in A produces the zeros vector?"
- Numerical examples:

$$
\begin{align*}
  A = \begin{bmatrix}1&-1\\-2&2\end{bmatrix} \\
  Ay = 0
  y = \begin{bmatrix}-1\\1\\\end{bmatrix} \\
  N(A) = \lambda\begin{bmatrix}1\\1\\\end{bmatrix}, \lambda \in \mathbb{R}
\end{align*}
$$
  - There are an infinite number of vectors in y that satisfy $Ay=0$ for $A$. Thus we can express the null space matrix $N(A)$ how we did above.

$$
\begin{align*}
  \begin{bmatrix}1&-1\\-2&3\\\end{bmatrix} \\
  Ay = 0 \\
  y = \begin{bmatrix}\end{bmatrix} \\
  N(A) = \begin{bmatrix}\end{bmatrix}
\end{align*}
$$
  - This matrix does not allow us to produce a null space matrix.
- Important note:
  - Linear independent columns in a matrix produce a column space greater than 1 but no larger than the number of linearly independent columns whereas linear dependent columns produce a column space equal to 1, the unscaled vector.
  - Using terminology that is explained below: full-rank and full column-rank matrices have empty null spaces whereas reduced rank-matrices have nonempty null spaces.

### Unit vector
- They are convenient and are numerically stable. So they are common as bases for subspaces.

## Rank
- Related to the dimensionalities of matrix subspaces and is important for things like inverting matrices and determining the number of solutions to a system of equations.
- Properties of rank:
  - Rank is a nonnegative integer.
  - Every matrix has one unique rank.
  - The rank of a matrix is denoted with $r(A)$
  - The maximum possible rank of a matrix is equal to either the smaller of the row or column count, $min\{M,N\}$
  - A matrix with a maximum rank, $r = min\{M,N\}$, is called "full rank"
  - A matrix with a rank smaller than the full rank, $r < min\{M,N\}$ is called reduced rank, rank deficient, or singular.
  - Scalar multiplication does not impact matrix rank
- Interpreting the rank of a matrix
  - The largest number of columns (or rows) that form a linearly independent set.
  - OR The dimensionality of the column space.
  - OR The number of dimensions containing information in the matrix.
  - OR The number of nonzero singular values of the matrix.
- Example:

```{julia}
#| label: rank-of-matrix
# Define matrix.
A = [[1,2] [3,4]]
# Find the rank.
rank_A = LinearAlgebra.rank(A)
# Print the results.
println("rank(A) = $rank_A")
```

### Rank of special matrices
- Vectors
    - They all have a rank of 1
- Zeros matrices
    - No matter the size, it has a rank of 1.
- Identity matrices
$$
r(I_N) = N
$$
    - The rank equals the number of rows.
- Diagonal matrices
    - The rank equals the number of nonzero diagonal elements.
- Triangular matrices
    - Full tank only if there are nonzero values in all diagonal elements. If it has at least one zero in the diagonal, it will be reduced rank.
- Random matrices
    - Should be full rank, but depends a lot. This is because the probability that any of the columns in the matrix are linearly dependent is low with certain distributions, but with others it could be high.'
- Rank of added and multiplied matrices
    - The ranks of the two individual matrices provide the upper bounds for the maximum possible rank.
 $$
 rank(A + B) \leq rank(A) + rank(B) \\
 rank(AB) \leq min \{rank(A), rank(B)\}
 $$
 - Rank of shifted matrices
    - They are full rank. The reason is that this is a key goal of shifting a matrix is to increase the rank.

::: {.callout-tip title="Note"}
- Singular values
    - Every $M\times N$ matrix contains a set of $min\{M,N\} nonnegative singular values.
    - They encode the "importance" or "spaciousness" of different directions in the column and row spaces of a matrix.
    - Where thre is a singular value of 0, there is a null space.
:::

## Augmented matrices
- Add extra columns to the right-hand side of the matrix. You start with an $M \times N$ matrix and augment it with a $M \times K$ matrix. The augmented matrix will be $M \times (N + K)$
- They just need to have the same number of rows.
- Numerical example
$$
\begin{bmatrix}
    4&5&6\\
    0&1&2\\
    9&9&4\\
\end{bmatrix} \sqcup \begin{bmatrix}
    1\\
    2\\
    3\\
\end{bmatrix} = \begin{bmatrix}
    4&5&6&1\\
    0&1&2&2\\
    9&9&4&3\\
\end{bmatrix}
$$

```{julia}
#| label: augmenting-a-matrix
# Define the initial matrix.
A = [[4,0,9] [5,1,9] [6,2,4]]
println("A:$A")
# Define the matrix/vector to add.
B = [1; 2; 3]
println("B:$B")
# Augment the matrix.
C = hcat(A, B)
println("C:$C")
```
- **You can use augmented matrices in an algorithm to determine whether a particular vector is in the column space of a matrix.**
1. Augment the matrix with the given vector.
2. Compute the ranks of the two matrices
3. Compare the two ranks:
    - If the rank of the original matrix and the augmented matrix is the equal, then the vector is in the column space of the original matrix.
    - If the rank of the original matrix is less than the augmented matrix, then the vector is not in the column space of the original matrix.

## Determinants
- The determinant is the number associated with a square matrix.
    - Can be very numerically unstable for large matrices.
- It is extremely important to a lot of applications to linear algebra.
- Two key properties:
    1. It is defined only for square matrices
    2. It is zero for singular (reduced-rank) matrices.
- Denoted with $det(A)$ or $|A|$.
- The geometric interpretation: how much the matrix stretches vectors during matrix vector multiplication
    - A negative determinant means that one axis is rotated during the transformation.
- In linear algebra, it is used to find eigenvalues or invert a covariance matrix.
- Computing them:
    - It is essentially the product of the diagonal minus the product of the off-diagonal. This doesn't really hold true when it is not $M=2;N=2$.
    - Just rely on your computer to do it because this strategy doesn't work all the time; thus why it is "numerically unstable".
$$
det(\begin{bmatrix}
    a&b\\
    c&d\\
\end{bmatrix}) = \begin{bmatrix}
    a&b\\
    c&d\\
\end{bmatrix} = ad-bc
$$
- Determinants are zero for reduced-rank matrices
    - This is because any reduced-rank matrix has at least one column that is a linear combination of another. So the difference between the products of the different elements will be null.
- One reason they are useful is that they create a algebraic equation that we can use to calculate unknown parameters, especially if we know the determinate:
    - For example, if you want to determine whether two columns are linearly dependent:
$$
det(\begin{bmatrix}
    a&\lambda a\\
    c&\lambda c\\
\end{bmatrix}) = \begin{bmatrix}
    a&\lambda a\\
    c&\lambda c\\
\end{bmatrix} = a\lambda c-a\lambda c
$$
    - Characteristic polynomial:
        - This is produced when we combine matrix shifting with the determinant.
        $$
        det(A-\lambda I) = \delta
        $$
        - Its a polynomial because the shifted $M \times M$ matrix produces a $\lambda^M$ term and has $M$ solutions.
        - Numeric example with a $2 \times 2$ matrix:
        $$
        \begin{bmatrix}
            a-\lambda & b\\
            c & d-\lambda\\
        \end{bmatrix} = 0 \rightarrow \lambda^2 - (a+d)\lambda + (ad - bc) = 0
        - Solutions where $\delta = 0$ are used to evaluate the eigenvalues of a matrix.
