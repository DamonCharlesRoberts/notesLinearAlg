{
  "hash": "2e1fcd883b8c1358e532660726596dc4",
  "result": {
    "engine": "jupyter",
    "markdown": "# Singular Value Decomposition (SVD)\n\n::: {#setup .cell execution_count=1}\n``` {.julia .cell-code}\n# Load packages.\nusing LinearAlgebra\n```\n:::\n\n\n## Key points\n- SVD is a generalized form of Eigendecomposition. Unlike Eigendecomposition, we can decompose non-square matrices.\n- It produces three matrices: left singular vectors, singular values, and right singular vectors. The first and last are square and the middle is a diagonal matrix.\n- The Singular vectors communicate the direction of the variance in the data and the singular values communicate the magnitude.\n- If you sum the diagonals of the singular values matrix (where the singular values are encoded), then we get the total variance in the data.\n- We can use the singular values to give us various measures of how much the singular vectors explain the data.\n- The condition number is also very useful. It is the ratio between the largest singular value and the smallest singular value. It can be used to tell us the stability of a given matrix. Diferent interpretations of it that are common are whether it is a well-conditioned matrix and is invertible, whether it is sensitive to pertubrations of noise, and how much it would add noise to a solution given a certain amount of additional noise.\n\n## How it works\n- The goal is to decompose a given matrix, $A$, into three matrices: left singular vectors ($U$), singular values ($\\Sigma$), and the right singular vectors ($V$).\n$$\nA = U\\Sigma V^T\n$$\n- It is a generalization of Eigendecomposition that allows nonsquare matrices.\n\n## Important features of SVD.\n- Both $U$ and $V$ are square matrices, does not require $A$ to be square.\n- The matrices of singular vectors are orthogonal: $U^TU = I$ and $V^TV = I$.\n- The first $r$ columns of $U$ provide the orthogonal basis vectors for the column space of the matrix $A$ while the rest of the columns provide orthogonal basis vectors for the left-null space.\n- The first $r$ rows of $V^T$ provide orthogonal basis vectors for the row space and the rest of the rows provide orthogonal basis vectors for the null space.\n- The singular values matrix is diagonal. It is also the same size as $A$.\n- All singular values are nonnegative and have real values.\n- The number of nonzero singular values is equivalent to the matrix rank.\n- It reveals all four subspaces of a matrix: column space, left-null space, row space, right-null space.\n\n## Singular values uses\n- The sum of the signular values is the total amount of variance in a matrix.\n- The singular vectors point and the singular values say how far. Meaning the variance is all contained in the singular values because singular vectors are normalized to unit magnitude, meaning they do not provide any information about magnitude.\n- It is often useful to convert singular values to the percent of total variance explained (PTVE):\n$$\n\\tilde{\\sigma_i} = \\frac{100\\sigma_i}{\\Sigma\\sigma}\n$$\n    - This is often used in PCA to determine how much of the variance the components explain.\n\n## Condition number\n- It is the ratio of the largest to the smallest singular value\n- Denoted as $\\kappa$\n$$\n\\kappa = \\frac{\\sigma_{max}}{\\sigma_{min}}\n$$\n- It is often used as a measure of the stability of a matrix when computing the inverse and when using it to solve systems of equations. When we have a noninvertable matrix, the condition number is undetermined as the minimum singular value will be zero.\n- It can also be interpreted as an amplification factor for noise. If we have a large condition number, then it tells us how large the noise would impact the solution to a least squares problem.\n- It also can tell us the sensitivity of a solution to any perturbations in the data matrix. If we have a well-conditioned matrix then we can add more noise and it will have a minimum change to the solution. However, adding a small amount of noise to a ill-conditioned atrix will lead to massively different solutions.\n\n## SVD and the MP Pseudoinverse\n- We can use the SVD of a matrix to perform the MP pseudoinverse. If we have a square matrix that is invertable, then we can invert $\\Sigma$.\n$$\n\\begin{aligned}\n    A^{-1} = (U\\Sigma V^T)^{-1} \\\\\n           = V\\Sigma^{-1}U^{-1} \\\\\n           = V\\Sigma^{-1}U^T\\\\\n\\end{aligned}\n$$\n\n",
    "supporting": [
      "ch_14_files"
    ],
    "filters": [],
    "includes": {}
  }
}