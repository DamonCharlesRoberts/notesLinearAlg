{
  "hash": "059d665dce18e07eac571e26a0b5c3d6",
  "result": {
    "engine": "jupyter",
    "markdown": "# Matrices, Part II\n\n::: {#setup .cell execution_count=1}\n``` {.julia .cell-code}\n# Loading packages\nusing LinearAlgebra\n```\n:::\n\n\n## Key points\n- There are induced and elementwise matrix norms. The former reflects magnitudes of th elements and the latter reflects hte geometric-transformative effect of the matrix on vectors.\n- The most commonly used elementwise norm is the Euclidean, Frobenius, or $\\mathcal{L}$ norm.\n- The trace is the sum of its diagonal elements\n- There are four matrix spaces (column, row, null, right-null) and they are the linear weighted combinations of different features of it.\n- The column space comprises all linear weighted combinations of the columns.\n- Often we want to know if some vector, $b$ is in th column space of a matrix where we can find a vector $x$ such that $Ax=b$\n- The null space is the set of vectors that linearly combines the columns producing the zeros vector. It is important for finding eigenvectors.\n- The null space occurs when a set of vectors are linearly dependent and reflects a singular (rank-reduced) matrix.\n- Shifting a square matrix by adding a constant to the diagonal ensures full-rank.\n- The determinant is a number to describe square matrices. It is zero for all singular matrices and nonzero for full-rank matrices.\n- The characteristic polynomial transforms square matricesl, shifting it by $\\lambda$, into an algebraic equation equal to the determinant so that we can solve by $\\lambda$\n\n## Matrix Norms\n\n::: {.callout-tip title=\"Recall\"}\n- Euclidean distance:\n$$\n  ||v|| = \\sqrt{\\sum_{i=1}^N v_i^2}\n$$\n- Vector norms:\n  - A vector's Euclidean geometric length.\n:::\n\n- A matrix norm is not the same thing as a vector norm.\n- Denoted as $\\parallel A \\parallel$\n- Similar to vector norms in that each norm is a number that characterizes a matrix.\n- Two main families of matrix norms:\n  - Element-wise norms\n    - Computed based on the individual elements of the matrix.\n    - Reflect the magnitudes of the elements in the matrix.\n  - Induced norms\n    - One key function of a matrix is to encode transformations on a vector.\n    - Given this, the induced norm of a matrix measures how much that transformation scales (stretches or shrinks) the vector.\n\n## Euclidean norm (an example of a element-wise norm)\n- AKA Frobenius norm or the $\\mathcal{L}$ norm (the regularized norm)\n$$\n  \\paralell A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2}\n$$\n- In words, it is the square root of the sum of all matrix elements squared.\n- This norm in particular, but all norms, generally, are used quite a lot in machine learning and regularization.\n- Also used as a measure of distance from another matrix, \"Matrix Distance\":\n$$\nC = A - B\n$$\n  - Where if $B=A$ then $C=0$ because the numerical values in the matrices are similar but if $B \\neq A$ then $C$ increases when the numerical values get increasingly dissimilar.\n- Example\n\n::: {#771a61be .cell execution_count=2}\n``` {.julia .cell-code}\n# Define a matrix.\nA = [[1,2] [3,4]]\nprintln(\"A:$A\")\n# Calculate the Frobenius norm.\nfro_norm = LinearAlgebra.norm(A)\nprintln(\"||A||=$fro_norm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA:[1 3; 2 4]\n||A||=5.477225575051661\n```\n:::\n:::\n\n\n### Matrix trace\n\n::: {.callout-note title=\"Definitions\"}\n- Trace\n  - the sum of its diagonal elements.\n  - denoted as $tr(A)$.\n:::\n\n- Numerical example:\n  - The trace of the two following matrices is $14$.\n$$\n\\begin{bmatrix}\n  4&5&6\\\\\n  0&1&4\\\\\n  9&9&9\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0&0&0\\\\\n  0&8&0\\\\\n  1&2&6\\\\\n\\end{bmatrix}\n$$\n\n- The trace of a matrix equals the sum of it's eigenvalues.\n  - A measure of the volume of its eigenspace.\n- The Frobenius norm can be calculated as the square root of the trace of t he matrix times its transpose. This happens because each diagonal element of the matrix $A^TA$ is defined by the dot product of each row with itself:\n$$\n  \\parallel A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2} = \\sqrt{tr(A^T)}\n$$\n\n::: {#trace .cell execution_count=3}\n``` {.julia .cell-code}\n# Define the matrix.\nA = [[4,0,9] [5,1,9] [6,4,9]]\nprintln(\"A:$A\")\n# Calculate the trace.\n#   - With the function.\ntrace_A = LinearAlgebra.tr(A)\n# Print trace.\nprintln(\"tr(A) = $trace_A\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA:[4 5 6; 0 1 4; 9 9 9]\ntr(A) = 14\n```\n:::\n:::\n\n\n## Matrix spaces\n\n### Colum space\n- Conceptualize the matrix as a set of column vectors.\n- Consider the infinity of real-valued scalars instead of working with a specific set of scalars.\n  - An infinite number of scalars gives an infinite number of ways to combine a set of vectors.\n  - This resulting set of vectors that can be created from the weighted combination of the columns in the matrix is the column space.\n- Denoted as $C(A)$\n- Numerical examples:\n$$\nC(\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}) = \\lambda\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n$$\n  - $\\lambda$, the scalar, can be any real number as there are any scalar can produce a vector produced by $A$.\n$$\nC(\\begin{bmatrix}1&1\\\\3&2\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}1\\\\2\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n$$\n  - What is the set of vectors that can be created with some linear combination of these two column vectors?\n    - All vectors in $\\mathbb{R}^2.\n\n$$\nC(\\begin{bmatrix}1&2\\\\3&6\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}2\\\\6\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n$$\n  - In this example, it is not possible to be in $\\mathbb{R}^2$ as these columns are linearly dependent to each other -- they are collinear. THis is because one is already a scaled version of the other.\n    - This means that the column space of this matrix is just a 1D subspace.\n- The dimensionality of the column space equals the number of columns only if they form a linearly independent set.\n\n## Row space\n- The same concept but we consider all possible weighted combinations of the rows instead of the columns.\n- Can be $R(A)$ or $C(A^T)$.\n\n## Null spaces\n- The column space can be summarized as $Ax = b$.\n  - In words, \"Can we find some set of coefficients in x such that the weighted combination of columns in A produce vector b\"? If yes, then $b \\in C(A)$.\n- The null space, however is summarized as $Ay = 0$\n  - In words, \"Can we find some set of coefficients in y such that the weighted combination of columns in A produces the zeros vector?\"\n- Numerical examples:\n\n$$\n\\begin{align*}\n  A = \\begin{bmatrix}1&-1\\\\-2&2\\end{bmatrix} \\\\\n  Ay = 0\n  y = \\begin{bmatrix}-1\\\\1\\\\\\end{bmatrix} \\\\\n  N(A) = \\lambda\\begin{bmatrix}1\\\\1\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\end{align*}\n$$\n  - There are an infinite number of vectors in y that satisfy $Ay=0$ for $A$. Thus we can express the null space matrix $N(A)$ how we did above.\n\n$$\n\\begin{align*}\n  \\begin{bmatrix}1&-1\\\\-2&3\\\\\\end{bmatrix} \\\\\n  Ay = 0 \\\\\n  y = \\begin{bmatrix}\\end{bmatrix} \\\\\n  N(A) = \\begin{bmatrix}\\end{bmatrix}\n\\end{align*}\n$$\n  - This matrix does not allow us to produce a null space matrix.\n- Important note:\n  - Linear independent columns in a matrix produce a column space greater than 1 but no larger than the number of linearly independent columns whereas linear dependent columns produce a column space equal to 1, the unscaled vector.\n  - Using terminology that is explained below: full-rank and full column-rank matrices have empty null spaces whereas reduced rank-matrices have nonempty null spaces.\n\n### Unit vector\n- They are convenient and are numerically stable. So they are common as bases for subspaces.\n\n## Rank\n- Related to the dimensionalities of matrix subspaces and is important for things like inverting matrices and determining the number of solutions to a system of equations.\n- Properties of rank:\n  - Rank is a nonnegative integer.\n  - Every matrix has one unique rank.\n  - The rank of a matrix is denoted with $r(A)$\n  - The maximum possible rank of a matrix is equal to either the smaller of the row or column count, $min\\{M,N\\}$\n  - A matrix with a maximum rank, $r = min\\{M,N\\}$, is called \"full rank\"\n  - A matrix with a rank smaller than the full rank, $r < min\\{M,N\\}$ is called reduced rank, rank deficient, or singular.\n  - Scalar multiplication does not impact matrix rank\n- Interpreting the rank of a matrix\n  - The largest number of columns (or rows) that form a linearly independent set.\n  - OR The dimensionality of the column space.\n  - OR The number of dimensions containing information in the matrix.\n  - OR The number of nonzero singular values of the matrix.\n- Example:\n\n::: {#rank-of-matrix .cell execution_count=4}\n``` {.julia .cell-code}\n# Define matrix.\nA = [[1,2] [3,4]]\n# Find the rank.\nrank_A = LinearAlgebra.rank(A)\n# Print the results.\nprintln(\"rank(A) = $rank_A\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrank(A) = 2\n```\n:::\n:::\n\n\n### Rank of special matrices\n- Vectors\n    - They all have a rank of 1\n- Zeros matrices\n    - No matter the size, it has a rank of 1.\n- Identity matrices\n$$\nr(I_N) = N\n$$\n    - The rank equals the number of rows.\n- Diagonal matrices\n    - The rank equals the number of nonzero diagonal elements.\n- Triangular matrices\n    - Full tank only if there are nonzero values in all diagonal elements. If it has at least one zero in the diagonal, it will be reduced rank.\n- Random matrices\n    - Should be full rank, but depends a lot. This is because the probability that any of the columns in the matrix are linearly dependent is low with certain distributions, but with others it could be high.'\n- Rank of added and multiplied matrices\n    - The ranks of the two individual matrices provide the upper bounds for the maximum possible rank.\n $$\n rank(A + B) \\leq rank(A) + rank(B) \\\\\n rank(AB) \\leq min \\{rank(A), rank(B)\\}\n $$\n - Rank of shifted matrices\n    - They are full rank. The reason is that this is a key goal of shifting a matrix is to increase the rank.\n\n::: {.callout-tip title=\"Note\"}\n- Singular values\n    - Every $M\\times N$ matrix contains a set of $min\\{M,N\\} nonnegative singular values.\n    - They encode the \"importance\" or \"spaciousness\" of different directions in the column and row spaces of a matrix.\n    - Where thre is a singular value of 0, there is a null space.\n:::\n\n## Augmented matrices\n- Add extra columns to the right-hand side of the matrix. You start with an $M \\times N$ matrix and augment it with a $M \\times K$ matrix. The augmented matrix will be $M \\times (N + K)$\n- They just need to have the same number of rows.\n- Numerical example\n$$\n\\begin{bmatrix}\n    4&5&6\\\\\n    0&1&2\\\\\n    9&9&4\\\\\n\\end{bmatrix} \\sqcup \\begin{bmatrix}\n    1\\\\\n    2\\\\\n    3\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n    4&5&6&1\\\\\n    0&1&2&2\\\\\n    9&9&4&3\\\\\n\\end{bmatrix}\n$$\n\n::: {#augmenting-a-matrix .cell execution_count=5}\n``` {.julia .cell-code}\n# Define the initial matrix.\nA = [[4,0,9] [5,1,9] [6,2,4]]\nprintln(\"A:$A\")\n# Define the matrix/vector to add.\nB = [1; 2; 3]\nprintln(\"B:$B\")\n# Augment the matrix.\nC = hcat(A, B)\nprintln(\"C:$C\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA:[4 5 6; 0 1 2; 9 9 4]\nB:[1, 2, 3]\nC:[4 5 6 1; 0 1 2 2; 9 9 4 3]\n```\n:::\n:::\n\n\n- **You can use augmented matrices in an algorithm to determine whether a particular vector is in the column space of a matrix.**\n1. Augment the matrix with the given vector.\n2. Compute the ranks of the two matrices\n3. Compare the two ranks:\n    - If the rank of the original matrix and the augmented matrix is the equal, then the vector is in the column space of the original matrix.\n    - If the rank of the original matrix is less than the augmented matrix, then the vector is not in the column space of the original matrix.\n\n## Determinants\n- The determinant is the number associated with a square matrix.\n    - Can be very numerically unstable for large matrices.\n- It is extremely important to a lot of applications to linear algebra.\n- Two key properties:\n    1. It is defined only for square matrices\n    2. It is zero for singular (reduced-rank) matrices.\n- Denoted with $det(A)$ or $|A|$.\n- The geometric interpretation: how much the matrix stretches vectors during matrix vector multiplication\n    - A negative determinant means that one axis is rotated during the transformation.\n- In linear algebra, it is used to find eigenvalues or invert a covariance matrix.\n- Computing them:\n    - It is essentially the product of the diagonal minus the product of the off-diagonal. This doesn't really hold true when it is not $M=2;N=2$.\n    - Just rely on your computer to do it because this strategy doesn't work all the time; thus why it is \"numerically unstable\".\n$$\ndet(\\begin{bmatrix}\n    a&b\\\\\n    c&d\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n    a&b\\\\\n    c&d\\\\\n\\end{bmatrix} = ad-bc\n$$\n- Determinants are zero for reduced-rank matrices\n    - This is because any reduced-rank matrix has at least one column that is a linear combination of another. So the difference between the products of the different elements will be null.\n- One reason they are useful is that they create a algebraic equation that we can use to calculate unknown parameters, especially if we know the determinate:\n    - For example, if you want to determine whether two columns are linearly dependent:\n$$\ndet(\\begin{bmatrix}\n    a&\\lambda a\\\\\n    c&\\lambda c\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n    a&\\lambda a\\\\\n    c&\\lambda c\\\\\n\\end{bmatrix} = a\\lambda c-a\\lambda c\n$$\n    - Characteristic polynomial:\n        - This is produced when we combine matrix shifting with the determinant.\n        $$\n        det(A-\\lambda I) = \\delta\n        $$\n        - Its a polynomial because the shifted $M \\times M$ matrix produces a $\\lambda^M$ term and has $M$ solutions.\n        - Numeric example with a $2 \\times 2$ matrix:\n        $$\n        \\begin{bmatrix}\n            a-\\lambda & b\\\\\n            c & d-\\lambda\\\\\n        \\end{bmatrix} = 0 \\rightarrow \\lambda^2 - (a+d)\\lambda + (ad - bc) = 0\n        - Solutions where $\\delta = 0$ are used to evaluate the eigenvalues of a matrix.\n\n",
    "supporting": [
      "ch_6_files"
    ],
    "filters": [],
    "includes": {}
  }
}