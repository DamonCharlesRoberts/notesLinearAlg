{
  "hash": "1e63726669cb99a2894c21cf1cb9ab89",
  "result": {
    "engine": "jupyter",
    "markdown": "# Vector Applications\n\n## Key points\n\n## Correlation\n\n:::{.callout-note title=\"Definitions\"}\n\n- Correlation coefficient:\n    - A scalar that uantifies the linear relationship between two variables.\n    - [-1, 1]\n:::\n\n- Dot product with some normalizations:\n    1. Mean centering: Subtracting the average value from each data value.\n    2. Divide the dot product from the product of the vector norms. This allows one to cancel measurement units and scales.\n\n- Familiar way:\n\n$$\nr = \\frac{\\sum^n_{i=1} (x_i - \\bar{x_i})(y_i - \\bar{y_i})}{\\sqrt{\\sum^n_{i=1} (x_i - \\bar{x_i})^2} \\sqrt{\\sum^n_{i=1} (y_i - \\bar{y_i})^2}}\n$$\n\n- Linear algebra way\n$$\nr = \\frac{\\tilde{x}^T\\tilde{y}}{||\\tilde{x}|| ||\\tilde{y}||}\n$$\n\nwhere $\\tilde{x}$ and $\\tilde{y}$ are mean-centered vectors.\n\n## Cosine similarity\n\n- Alternative to way to assess similarity between vectors other than using correlation.\n- Defined as the geometric formula for the dot product when solved for the cosine term.\n\n$$\ncos(\\theta_{x,y}) = \\frac{x^Ty}{||x|| ||y||}\n$$\n\n- The difference between this and correlation is that I am not first normalizing $x$ and $y$ by mean-centering them.\n\n## Time series Filtering and Feature (Variable) detection\n\n::: {.callout-note title=\"Definitions\"}\n\n- Kernel\n\t- Refers to a smooth \"template\" series that we can use to pull a real series to to smoothen it a bit as a feature-detection method.\n:::\n\n- We can use the dot product for time series filtering.\n\t- Where filtering is basically a feature-detection method. It essentially detects patterns and signals and pulls out noise from the time series. The effect of this is to \"smoothen\" the series.\n\n- Can use it for convolution:\n1. The dot product is computed for the kernel and the time series signal.\n\t\t- Often filtering requires local feature detection and the kernel is shorter than the whole series.\n\t\t- This means that we often compute the dot product between the kernel and a shorter chunk of the sereis that is the same length of the kernel.\n2. This results in a time point of the filtered signal.\n3. We then move one time period later and compute the dot product with a different and overlapping signal segment.\n\n## k-Means clustering\n\n::: {.callout-note title=\"Definitions\"}\n\n- k-Means clustering\n\t- Unsupervised method of putting multivariate data into a relatively small number of groups or categories.\n:::\n\n- With concepts covered in the earlier chapters,this is how this procedure is done:\n\n1. Define $k$ centroids as random points in the vector space.\n\t- Each centroid is a class, or category.\n2. Compute the Euclidean distance between each observation and each centroid.\n3. Assign each data observation to the group with the closest centroid (the smallest Euclidean distance).\n4. Update the centroid location by taking the average of all data observations assigned to the centroid.\n5. Repeat steps 2-4 until a convergence criteria is satisfied or N iterations have completed.\n\t- Often these conditions are either until N iterations occur.\n\t- More sophisticated implementations are often to repeat these steps until centroid locations do not move much.\n\n::: {#kmeans-clustering .cell execution_count=1}\n``` {.julia .cell-code}\n# Try to write your own k-means clustering algorithm here.\n# Rather than writing double for-loop, can use broadcasting for efficiency.\n```\n:::\n\n\n",
    "supporting": [
      "ch_4_files"
    ],
    "filters": [],
    "includes": {}
  }
}