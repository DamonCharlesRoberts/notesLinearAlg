{
  "hash": "b5dd20b361785c44c4debb9fbfb59857",
  "result": {
    "engine": "jupyter",
    "markdown": "# General Linear Models (GLM) and Least Squares\n\n::: {#setup .cell execution_count=1}\n``` {.julia .cell-code}\n# Load packages.\nusing LinearAlgebra\nusing GLM\nusing DataFrames\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[ Info: Precompiling GLM [38e38edf-8417-5370-95a0-9cbb8c7f171a]\n[ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n```\n:::\n:::\n\n\n## GLMs\n\n$$\n\\begin{aligned}\n    X\\beta = y\\\\\n    (X^TX)^{-1}X^TB = (X^TX)^{-1}X^Ty\\\\\n    \\beta = (X^TX)^{-1}X^Ty\\\\\n    \\text{OR}\\\\\n    X^T\\epsilon = 0\\\\\n    X^T(y-X\\beta) = 0\\\\\n    X^Ty - X^TX\\beta = 0\\\\\n    X^TX\\beta = X^Ty\\\\\n    \\beta = (X^TX)^{-1}X^Ty\n\\end{aligned}\n$$\n\n- $\\beta$ refers to a vector of coefficients.\n- $X$ refers to the matrix of predictors.\n- $y$ refers to the vector of outcome variable data.\n\n- This equation is exactly solvable when $y$ is in the column space of the design matrix $X$. This rarely happens, however, as  we rarely have situations where the model accounts for 100% of the vairance in the outcome.\n- This means we need to modify the GLM equation:\n$$\n\\begin{aligned}\n    X\\beta = y + \\epsilon \\\\\n    \\epsilon = X\\beta - y\\\\\n\\end{aligned}\n$$\n- $\\epsilon$ is some residual vector that we add so that $y$ fits within the column space of the design matrix.\n- $\\hat{y}$ is then $\\hat{y} = y + \\epsilon$.\n\n## Least squares with QR\n- The previous method uses the left-inverse method. It can have numerical instability, however, because it requires computing the matrix inverse.\n- $X^TX$ can introduce difficulties as it influences the norm and condition number (see chapter 14).\n    - a high condition number can be numerically unstable and will only become less stable when squared.\n- With QR decomposition, we can solve this numerical instability issue:\n$$\n\\begin{aligned}\n    X\\beta = y\\\\\n    QR\\beta = y\\\\\n    R\\beta = Q^Ty\\\\\n    \\beta = R^{-1}Q^Ty\n\\end{aligned}\n$$\n    - Note some of the algebra above.\n        1. We can decompose $X$ into the orthogonal, $Q$, and the non-orthogonal matrix, $R$.\n        2. Then we can multiply both sides of the equation by the transpose of the orthogonal matrix, $Q^T$. Since $Q^TQ = I$, $Q$ on the left-side cancels out.\n        3. We can then multiply $R^{-1}$ to both sides and since $R^{-1}R = I$ that cancels $R$ out on the left-hand side.\n    - Some other things that happen below:\n        - We can use row swap with permutation matrices to increase numerical stability and we do not even need to invert $R$. So, we can get the solution with back substitution.\n\n::: {#ols .cell execution_count=2}\n``` {.julia .cell-code}\n# Define the data.\ndata = DataFrame(x = [1,2,3,4,5], y = [2,4,5,4,5])\nX = hcat(ones(length(data.x)), data.x)\ny = data.y\n# Do it the manual way.\nBeta = LinearAlgebra.inv(X'X) * X' * y\n# Do it with GLM package\nmodel = GLM.lm(@formula(y ~ x), data)\n# Compare results.\nprintln(\"Intercepts are approximately equal? $(Beta[1]) ≈ $(coef(model)[1])\")\nprintln(\"Coefficients are approximately equal? $(Beta[2]) ≈ $(coef(model)[2])\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercepts are approximately equal? 2.2000000000000006 ≈ 2.1999999999999993\nCoefficients are approximately equal? 0.6000000000000004 ≈ 0.6000000000000003\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ch_11_files"
    ],
    "filters": [],
    "includes": {}
  }
}