{
  "hash": "014163e9998f84d9d65a59839e5107c7",
  "result": {
    "engine": "jupyter",
    "markdown": "# Applications of Eigendecomposition and SVD\n\n::: {#d9bb0c14 .cell labels='setup' execution_count=1}\n``` {.julia .cell-code}\n# Load packages.\nusing LinearAlgebra\n```\n:::\n\n\n## PCA\n- Is used for finding the component (set of variables) that maximally explain the amount of variance in a given matrix.\n- Because of this, PCA wants to find the maximal amount of variance and find the weighted linear combination of variables that maximize it.\n- Say that $X$ is our matrix. We want to find the weighted linear combination of column vectors in $X$ that maximize the variance explained. So, we would have a vector of weights, $w$, that provide all of the weights used to linearly combine the variables.\n    - With PCA, we will want to find the set of weights in $w$ so that $Xw$ provides the maximal variance, $\\lambda$.\n    $$\n    \\lambda = \\parallel Xw \\parallel^2\n    $$\n    - Note: the squared vector norm is the same thing as variance when the data is mean centered.\n    - The problem with the existing formulation is that we can specify really large values of $w$ as it will increase the variance by generating larger weights. So, we need to scale the norm of the weighted combination by the norm of the weights so it is fair.\n    $$\n    \\lambda = \\frac{\\parallel Xw \\parallel^2}{\\parallel w \\parallel^2}\n    $$\n    - This gives us the ratio of two vector norms. Which we can expand to be represented as dotproducts to make it clearer.\n    $$\n        \\lambda = \\frac{w^TX^TXw}{w^Tw}\n    $$\n    - How do we find $w$ that maximize $\\lambda$? We can't just consider a single vector of weights, we need to consider all the possible vectors of weights so we need to consider $W$ rather than just $w$.\n    $$\n    \\begin{aligned}\n        \\Lambda = (W^TW)^{-1}W^TX^TXW\\\\\n        \\Lambda = W^{-1}W^{-T}W^TX^TXW\\\\\n        \\Lambda = W^{-1}X^TXW\\\\\n        W\\Lambda = W^{-1}X^TXW\\\\\n        W^\\Lambda = X^TXW\\\\\n    \\end{aligned}\n    $$\n    - So, we can just perform Eigendecomposition. The eigenvectors are the weights for the data variables and the eigenvalues are the variances of the data along each direction (each column of $W$).\n    - Since covariance matrices are symmetric, the eigenvectors - the principal components - are orthogonal.\n\n### The steps to PCA\n1. Compute the covariance matrix of the data. Each feature needs to be mean-centered before computing the covariance.\n2. Take the eigendecomposition of the covariance matrix.\n3. Sort the eigenvalues so that the largest are at the top. The eigenvectors should also be sorted.\n4. Compute the component scores as the weighted combination of all of the variables where the eigenvector provides the weights. The eigenvector with the largest eigenvalue is the most important component.\n5. Convert the eigenvalues to percent variance explained to help with interpreation.\n\n### PCA with SVD\n- Two options.\n    1. Take the SVD of the covariance matrix. It is identical to PCA with Eigendecomposition as SVD is just a way to do it with a nonsquare matrix.\n    2. Take the SVD of the data matrix. The right singular vectors will be the eigenvectors of the covariance matrix. The data needs to be meancentered before doing the SVD. The square root of your singular values will be the eigenvalues of the covariance matrix.\n\n## Linear Discriminant Analysis (LDA)\n- Multivariate classification technique where you try to maximize the space between multiple categories of data. In other words, group data into clusters.\n- LDA is meant to find basis vectors in the data space that maximally separate the categories of data.\n    - We do this by finding a set of weights such that the weighted combo of variables maximally separate the categories.\n    $$\n        \\lambda = \\frac{\\parallel X_Bw \\parallel^2}{\\parallel X_w w \\parallel^2}\n    $$\n    - We want to find a set of weights $w$ that maximizes the ratio of the variance of data feature $X_B$ to the variance of data feature $X_W$.\n    - We can do something like what we did with PCA above doing Eigendecomposition.\n    $$\n    \\begin{aligned}\n        C_W = X_W^TX_W\\\\\n        C_B = X_B^TX_B\\\\\n        \\Lambda = (W^TC_WW)^{-1}W^TC_BW\\\\\n        \\Lambda = W^{-1}C^{-1}_WW^{-T}W^TC_BW\\\\\n        W\\Lambda = C^{-1}_WC_BW\\\\\n        C_WW\\Lambda = C_BW\\\\\n    \\end{aligned}\n    $$\n    - Here we just do generalized eigendecomposition on two covariance matrices. The eigenvectors provide the weights and the generalized eigenvalues are the variance ratios of each ocmponent.\n    - To determine $X_B$ and $X_W$, we can use a few different options depending on what kind of LDA you ae doing and that depends on use case. However, in typical LDA, $X_B$ is the between-category covariance while $X_W$ is the within-category covariance. The within-category covariance is the average of the covariances of the data samples within each class. The between-category covariance is the result of creating a new data matrix that contains the feature averages for each class.\n\n## Low-rank appoximations with SVD\n- We can take the SVD of a data matrix or image, then reconstruct the data matrix using some subset of SVD components. We can do this by setting the selected $\\sigma$s to zero or create a new SVD matrix that are rectangular with the to-be-rejected vectors and singular values removed.\n- This allows us to compress data into a smaller size.\n\n",
    "supporting": [
      "ch_15_files"
    ],
    "filters": [],
    "includes": {}
  }
}