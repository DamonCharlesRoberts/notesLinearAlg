{
  "hash": "2f44eca84b861edb82c259d7510f14cd",
  "result": {
    "engine": "jupyter",
    "markdown": "# Row reduction and LU Decomposition\n\n::: {#setup .cell execution_count=1}\n``` {.julia .cell-code}\n# Load packages.\nusing LinearAlgebra\n```\n:::\n\n\n## Key points\n\n- When working with a system of matrices, we need to apply manipulations to both sides and in the same order -- they are noncommunicative.\n- Row reduction allows us to take the rows of matrix $A$ and scalar multiply then add them until we have an upper-triangular matrix, $U$. The linear transformations can be recorded in our transformation matrix,, $L$, which left multiplies $A$. That is $L^{-1}A = U$\n- We use row reduction in LU decomposition. The result is unique so long as there is a square and full rank matrix.\n\n## Systems of equations\n\n- LU decomposition is essentially solving a series of systems of equations. Here's some examples of how to algebraically solve them.\n\n$$\n\\begin{aligned}\n    x = 4-y\\\\\n    y = \\frac{x}{2} + 2\\\\\n    \\\\\n    x = 4 - (\\frac{1}{2}x + 2) \\\\\n    \\frac{3}{2}x = 4 - 2\\\\\n    x = \\frac{4}{3}\\\\\n    y = \\frac{4}{3}^{-2} + 2 \\\\\n    y = \\frac{2}{3} + \\frac{6}{3} = \\frac{8}{3}\n\\end{aligned}\n$$\n\n## Matrices as systems\n- Take the following system:\n$$\n\\begin{aligned}\n    x + y = 4 \\\\\n    -\\frac{x}{2} + y = 4\\\\\n\\end{aligned}\n$$\n    - In matrix form, it would be re-expressed as:\n$$\n\\begin{bmatrix}\n    1&1\\\\\n    -\\frac{1}{2}&1\n\\end{bmatrix}\\begin{bmatrix}\n    x\\\\\n    y\n\\end{bmatrix} = \\begin{bmatrix}\n    4\\\\\n    2\n\\end{bmatrix}\n$$\n- You essentially create a matrix of scalars, then a column vector of variables which is equivalent to the vector of scalars.\n- This is what is represented when we have $Ax = b$.\n    - $A$ is our matrix of scalars called our coefficients.\n    - $x$ is our vector of variables.\n    - $b$ is our vector of constants.\n\n### Working with/solving matrices as systems\n- What you do to one-side, you must do to the other.\n- The order matters. If you pre-multiply a matrix/vector on one-side you have to do it on the other.\n    - E.g.\n        - The following is valid.\n    $$\n    \\begin{aligned}\n        AX = B\\\\\n        CAX = CB\\\\\n    \\end{aligned}\n    $$\n        - The following is invalid.\n    $$\n    \\begin{aligned}\n        AX = B\\\\\n        AXC = CB\\\\\n    \\end{aligned}\n    $$\n\n### Row reduction\n\n::: {.callout-note title=\"Definitions\"}\nEchelon form matrix\n    - The resulting upper triangular matrix from row reduction\n    - The leftmost nonzero number in a row (called the pivote) is to the right of the pivote of the rows above.\n    - Any rows of all zerios are below rows containing nonzeros.\n:::\n\n- The building block for LU decomposition.\n- It is the way to solve systems of equations by hand and is very well-known in linear algebra. However, you often do not actually do it by hand. It is useful to know about as it leads to LU decomposition.\n- Row reduction\n    - You iteratively apply two operations to rows of a matrix:\n        1. scalar multiplication\n        2. scalar addition\n    - It is the matrix system way of adding equations to other equations within a system (solving a system with substitution).\n    - The goal of row reduction: transform a dense matrix into a upper-triangular matrix via row manipulations that we implement by premultiplying a transformation matrix.\n- Example:\n    $$\n    \\begin{bmatrix}\n        2&3\\\\\n        -2&2\\\\\n    \\end{bmatrix} \\xrightarrow{R_1 + R2} \\begin{bmatrix}\n        2&3\\\\\n        0&5\\\\\n    \\end{bmatrix}\n    $$\n    - We get this upper triangular matrix by adding the first row to the second row.\n    - This upper triangular matrix is called the **echelon form** of the matrix\n- Though the echelon form is different than the original matrix, it is connected to the original matrix through a linear combination. We can represent this as:\n$$\n\\begin{bmatrix}\n    1&0\\\\\n    1&1\\\\\n\\end{bmatrix}\\begin{bmatrix}\n    2&3\\\\\n    -2&2\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n    2&3\\\\\n    0&5\\\\\n\\end{bmatrix}\n$$\n    - This is often represented as $L^{-1}A = U$ where $L^{-1}$ represents the linear transformations that we have implemented as a way to keep track of what manipulations we have done with row reduction.\n- The problem with row reduction is that it is tedius and that the echelon form of a matrix is not unique.\n\n## Gaussian Elimination\n- You do not have to do matrix inversion to solve a matrix equation.\n- We can use Gaussian Elimination.\n- How?\n    - Augment the matrix of coefficeints by the vector of constants, row reduce to echelon form, then use back substitution to solve for each variable.\n\n- Example:\n\n$$\n\\begin{aligned}\n    x = 4-y\\\\\n    y = \\frac{x}{2} + 2\\\\\n    \\text{Put in matrix form:}\\\\\n    \\begin{bmatrix}\n        1&1\\\\\n        -\\frac{1}{2}&1\\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        x\\\\\n        y\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        4\\\\\n        2\\\\\n    \\end{bmatrix}\\\\\n    \\begin{bmatrix}\n        1&1\\\\\n        -\\frac{1}{2}&1\\\\\n    \\end{bmatrix} \\sqcup \\begin{bmatrix}\n        4\\\\\n        2\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        1&1&4\\\\\n        -\\frac{1}{2}&1&2\\\\\n    \\end{bmatrix}\\\\\n    \\text{Row reduce:}\\\\\n    \\begin{bmatrix}\n        1&1&4\\\\\n        -\\frac{1}{2}&1&2\\\\\n    \\end{bmatrix} \\xrightarrow{\\frac{1}{2}R_1 + R_2} \\begin{bmatrix}\n        1&1&4\\\\\n        0&\\frac{3}{2}&4\\\\\n    \\end{bmatrix}\\\\\n    \\text{Back substitute:}\\\\\n    x + y = 4\\\\\n    \\frac{3}{2}y = 4 \\Rightarrow y =  \\frac{8}{3} \\\\\n    x + \\frac{8}{3} = 4 \\Rightarrow x = \\frac{4}{3} \\\\\n\\end{aligned}\n$$\n\n## Gauss-Jordan Elimination\n- We need to keep reducing so that the left-most nonzero elements in the matrix, our pivots, are 1s.\n- An alternate to back substitution is that we can divide each row by it's pivot. So, for example, we had been here:\n$$\n\\begin{bmatrix}\n    1&1&4\\\\\n    0&1&\\frac{8}{3}\n\\end{bmatrix}\n$$\n- We can use Gauss-Jordan elimination as a trick to continue row reducing upward to get rid of elements abov each pivot.\n$$\n\\begin{bmatrix}\n    1&1&4\\\\\n    0&1&\\frac{8}{3}\n\\end{bmatrix} \\xrightarrow{-R_2 + R_1} \\begin{bmatrix}\n    (-0+1)&(-1+1)&(-\\frac{8}{3}+\\frac{12}{3})\\\\\n    0&1&\\frac{8}{3}\\\\\n\\end{bmatrix}=\\begin{bmatrix}\n    1&0&\\frac{4}{3}\\\\\n    0&1&\\frac{8}{3}\\\\\n\\end{bmatrix}\n$$\n- Like back substitution, this produces a reduced row echelon form, RREF.\n- Remember Gauss-Jordan Elimination is just the shortcut that we can use to solve a matrix system rather than having to go back to solving a system of equations.\n## Matrix inverse via Gauss-Jordan Eliminations:\n- Say that we have this:\n$$\n\\begin{bmatrix}\n    a&b\\\\\n    c&d\\\\\n\\end{bmatrix}\\begin{bmatrix}\n    x_1\\\\\n    y_1\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n    1\\\\\n    0\n\\end{bmatrix}\n$$\n- The constants vector is the first column vector for a $2\\times 2$ $I$.\n- What this implies is that applying RREF to a square full-rank matrix augmented by the first column of the identity matrix will show the linear transformation that generates the matrix into the first column of the identity matrix. This then means that the vector $[x_1 y_1]^T$ is the first column of the matrix inverse. We can then repeat this process but for the second column of the inverse.\n- Basically, this is a mechanism to find the matrix inverse without computing determinants. The draw back is that row redeuction involves a lot of division which can lead to error from lack of precision. Though it is more numerically stable than the inverse. A matrix that is close to singular or one that has a high condition number will be difficult to inverse no matter what!\n\n## LU Decomposition\n- Builds on row reduction by adding constraints. Doing this allows us to identify a unique echelon form, unlike what we get with row reduction alone. This is the RREF and $U$.\n- The goal with this procedure is to decompose a given matrix into the product of two triangular matrices.\n- It relies upon row reduction.\n- The echelon form is not necessarily unique with LU decomposition.\n    - However, by requiring that the diagonals of $L$ are 1, this makes sure that the LU decomposition is unique for a full-rank square matrix.\n$$\nA = LU\n$$\n- A numerical example:\n    - We have the following matrix:\n$$\n\\begin{bmatrix}\n    2&2&4\\\\\n    1&0&3\\\\\n    2&1&2\\\\\n\\end{bmatrix}\n$$\n    - We can do $L^{-1}A=U$ with row reduction like before to get the transformation matrix  that transforms $A$ into the echelon form, $U$ (which is an upper-triangle).\n\n$$\n\\begin{bmatrix}\n    2&2&4\\\\\n    1&0&3\\\\\n    2&1&2\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n    1&0&0\\\\\n    \\frac{1}{2}&1&0\\\\\n    1&1&1\\\\\n\\end{bmatrix}\\begin{bmatrix}\n    2&2&4\\\\\n    0&-1&1\\\\\n    0&0&-3\\\\\n\\end{bmatrix}\n$$\n\n::: {#lu-decomp .cell execution_count=2}\n``` {.julia .cell-code}\n# Define matrix.\nA = [[2,1,2] [2,0,1] [4,3,2]]\nprintln(\"A = $A\")\n# Perform LU decomposition.\nA_LU_all = LinearAlgebra.lu(A)\nL = A_LU_all.L\nU = A_LU_all.U\nprintln(\"L = $L\")\nprintln(\"U = $U\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA = [2 2 4; 1 0 3; 2 1 2]\nL = [1.0 0.0 0.0; 0.5 1.0 0.0; 1.0 1.0 1.0]\nU = [2.0 2.0 4.0; 0.0 -1.0 1.0; 0.0 0.0 -3.0]\n```\n:::\n:::\n\n\n## Row swapping with Permutation matrices\n- Some matrices are hard to transform into upper-triangular form.\n- We can swap rows to help put it in echelon form and then perform row reduction from there. This is done with a permutation matrix.\n- Numerical example:\n    - Original matrix\n    $$\n    \\begin{bmatrix}\n        3&2&1\\\\\n        0&5&5\\\\\n        0&7&2\\\\\n    \\end{bmatrix}\n    $$\n    - Row swapping with the permutation matrix, $P$:\n    $$\n    \\begin{bmatrix}\n        1&0&0\\\\\n        0&1&1\\\\\n        0&1&0\\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        3&2&1\\\\\n        0&5&5\\\\\n        0&7&2\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        3&2&1\\\\\n        0&7&2\\\\\n        0&0&5\\\\\n    \\end{bmatrix}\n    $$\n- Often this is required with LU decomposition. So technically, LU decomposition is denoted as:\n$$\nPA = LU\\\\\nA = P^TLU\n$$\n- Permutation matrices are orthogonal. Which means $P^{-1} = P^T$ and $P^TP = I$.\n\n",
    "supporting": [
      "ch_10_files"
    ],
    "filters": [],
    "includes": {}
  }
}