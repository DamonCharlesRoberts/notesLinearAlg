{
  "hash": "2cda1b9c69a3c32b1fe0d290dd3529d1",
  "result": {
    "engine": "jupyter",
    "markdown": "# Matrix Inverse\n\n::: {#setup .cell execution_count=1}\n``` {.julia .cell-code}\n# Load packages\nusing LinearAlgebra\n```\n:::\n\n\n## Key points\n- Matrix inversion is a way to decompose a matrix or split a matrix into multiple components.\n- It is numerically unstable because the calculation requires a lot of moving parts and often requires the calculation of the determinant which produces floats that can cause deviation from the true solution due to floats.\n- The conditions required for full matrix inversion are often uncommon to the applications we are using and so the less ideal solutions are often needed. In many applications, the matrices that we have are not even fully invertable and require a pseudoinverse which produces a transformation matrix that approximate, but does not equate to, the identity matrix (what we often need).\n\n## Matrix inverse\n- Any matrix that has an inverse, that inverse is unique to that matrix.\n- Denoted as: $A^{-1}.\n- $A^{-1}A = I$\n    - The inverse matrix multiplied by itself produces the identity matrix.\n    - It allows you to \"cancel\" out a matrix.\n- We often want to do this so that we can solve common problems like:\n    - $Ax=b$ where we know $A$ and $b$ but want to find $x$\n    $$\n    \\begin{aligned}\n        Ax = b \\\\\n        A^{-1}Ax = A^{-1}b \\\\\n        Ix = A^{-1}b \\\\\n        x = A^{-1}b\n    \\end{aligned}\n    $$\n- It is really complicated to compute the inverrse of a matrix and it does not always work. Not all matrices can be inverted.\n\n## Types of inverses and their conditions\n- Full inverse\n    - $A^{-1}A = AA^{-1} = I$\n    - Conditions:\n        - $A$ needs to be square.\n        - $A$ needs to be full rank.\n- One-sided inverse\n    - Can transform a full-rank rectangular matrices into $I$.\n    - If $T$ is a tall-matrix, then you can do the ***left-inverse*** to produce I, $LT=I$, but $TL \\neq I$\n    - If $W$ is a wide-matrix, then you can do the ***right-inverse*** to produce I, $WR=I$, but $RW \\neq I$\n- Pseudoinverse\n    - Every matrix has one, does not require full-rank like the other inverses do.\n    - If a matrix is square full-rank, the pseudoinverse is equal to the full inverse.\n    - If it is a tall full-rank matrix, the pseudoinverse is equal to the left inverse.\n    - If it is a wide full-rank matrix, the pseudoinverse is equal to the right inverse.\n    - Singular matrices have a pseudoinverse but it only converts it into another matrix that approximates but does not equal the identity matrix.\n\n## Computing the inverse\n- To compute a scalar inverse we can:\n$$\n\\begin{aligned}\n    \\lambda = 3 \\\\\n    \\lambda^{-1} = \\frac{1}{3} \\\\\n    \\lambda \\lambda^{-1} = 3 \\times \\frac{1}{3} = 1\n\\end{aligned}\n$$\n- With matrices, it isn't that easy.\n    - With a $2\\times 2$ matrix:\n        - Swap diagonal elements, multiply off-diagonal elements by $-1$ and divide by the determinant.\n        $$\n        \\begin{aligned}\n            A = \\begin{bmatrix}\n                a&b\\\\\n                c&d\\\\\n            \\end{bmatrix}\\\\\n            A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}\n                d&-b\\\\\n                -c&a\\\\\n            \\end{bmatrix}\\\\\n            AA^{-1} = \\begin{bmatrix}\n                a&b\\\\\n                c&d\\\\\n            \\end{bmatrix}\\frac{1}{ad-bc}\\begin{bmatrix}\n                d&-b\\\\\n                -c&a\\\\\n            \\end{bmatrix}\\\\\n            = \\frac{1}{ad-bc}\\begin{bmatrix}\n                ad-bc&0\\\\\n                0&ad-bc\\\\\n            \\end{bmatrix}\\\\\n            = \\begin{bmatrix}\n                1&0\\\\\n                0&1\\\\\n            \\end{bmatrix}\n        \\end{aligned}\n        $$\n- Example in Julia:\n\n::: {#matrix-inversion .cell execution_count=2}\n``` {.julia .cell-code}\n# Define a matrix.\nA = [[1,2] [4,7]]\nprintln(\"A = $A\")\n# Invert A.\nA_inv = LinearAlgebra.inv(A)\nprintln(\"A^-1 = $A_inv\")\n# Confirm A^1A equals I\nI = A_inv * A\nprintln(\"A^-1A = $I\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA = [1 4; 2 7]\nA^-1 = [-7.0 4.0; 2.0 -1.0]\nA^-1A = [1.0 0.0; 0.0 1.0]\n```\n:::\n:::\n\n\n- To invert any square full-rank matrix, here is the algorithm:\n    - Requires four intermediate matrices\n        - **Minors**: comprises determinants of submatrices. Produced by taking the determinant of the submatrix when you exclude the $i$th row and $j$th column.\n        - **Grid matrix**: Computed as $g_{i,j} = -1^{i+j}$ and produces a checkerboard.\n        - **Cofactors matrix**: The Hadamard multiplication of the minors matrix with the grid matrix.\n        - **Adjugate matrix**: Transpose the cofactors matrix then scalar multiply it by the inverse of the determinant of the original matrix.\n\n## One-sided inverses\n- We need to find some matrix $L$ so that $LT=I$.\n- To do this, we can first make $T$ square by doing: $T^TT$. From there, we can invert that resulting square matrix which is now $L$. So, a left inverse is:\n$$\n(T^T)^{-1}(T^TT) = I \\\\\nL = (T^TT)^{-1}T\n$$\n\n## Moore-Penrose Pseudoinverses for singular matrices\n- Pseudoinverses produce transformation matrices.\n    - These transformation matrices are not $I$ as the other inverses produce. However, they get somewhat close to $I$.\n- Pseudoinverses are not unique solutions. You can have multiple transformation matrices for one singular matrix. This is why inverses can be unstable.\n- One type is the Moore-Penrose Pseudoinverse\n    - Extremely common.\n    - Denoted as $A^\\dagger$\n- To compute them, you need to take the SVD (singular value decomposition, have notes on this later), invert the nonzero singular values and then reconstruct the matrix by multiplying $U\\sum^TV^T$%\n\n## Other reasons inverses are numerically unstable\n- We do not always have full-rank matrices or matrices that are square. As we can have multiple solutions for pseudoinverses, this is one source of instability.\n- There are a lot of floating point operations, e.g., we have to calculate determinants. Because of this, there are a lot of places where rounding comes into play and this can create departures from the true results.\n    - The larger our matrix, the more determinants we have to calculate. So the larger our matrix gets, the larger the deviation can become.\n- Instead of inverting matrices, we can use matrix decomposition to decompose a matrix into the product of another one. Decomposition is much more numerically stable.\n\n",
    "supporting": [
      "ch_8_files"
    ],
    "filters": [],
    "includes": {}
  }
}