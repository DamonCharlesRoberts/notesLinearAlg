[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Practical Linear Algebra for Data Science by Mike X Cohen",
    "section": "",
    "text": "1 What this is\nThese are my personal notes on Practical Linear Algebra for Data Science by Mike X. Cohen. There is an additional goal in which I seek to accomplish: learn Julia. So while the book uses code examples with Python, here I translate (to the best of my ability) the code examples using Julia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What this is</span>"
    ]
  },
  {
    "objectID": "index.html#notes-on-the-code",
    "href": "index.html#notes-on-the-code",
    "title": "Notes on Practical Linear Algebra for Data Science by Mike X Cohen",
    "section": "1.1 Notes on the code",
    "text": "1.1 Notes on the code\n\nNeed to have Julia installed.\n\nRelies on the LinearAlgebra pkg.\n\nUses Juypter to execute the julia code. So, I have a poetry virtual environment that contains those details. Requirements of note:\n\njupyter\njupyter-cache",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What this is</span>"
    ]
  },
  {
    "objectID": "ch_2.html",
    "href": "ch_2.html",
    "title": "2  Vectors, Part I",
    "section": "",
    "text": "2.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#key-points",
    "href": "ch_2.html#key-points",
    "title": "2  Vectors, Part I",
    "section": "",
    "text": "Vectors are ordered lists of numbers.\nThere are quite a few operations available for vectors. All of which require that vectors be the same orientation and dimensionality.\nThere are a few ways to abstract this: thinking of the linear algebra, or thinking about it geometrically.\nDot products are the basis for a lot of important tasks in linear algebra. A dot product is essentially a non-normalized coefficient as it is a way to summarize the relationship between two vectors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#defining-vectors",
    "href": "ch_2.html#defining-vectors",
    "title": "2  Vectors, Part I",
    "section": "2.2 Defining vectors",
    "text": "2.2 Defining vectors\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVectors: an ordered list of numbers\n\nOften denoted with lower-case letters.\nE.g., \\(v\\).\n\nDimensionality: Number of elements in a vector.\n\nDenoted with \\(\\mathbb{R}^N\\)\n\nOrientation: Whether it is a row or a column vector.\n\n\n\n\nCharacteristics of a vector:\n\nDimensionality\nOrientation\n\nIllustrating the dimensionality of a vector.\n\n\n# Dimensionality: 1\nR1 = [1]\nprintln(\"Dimensionality of R1: $length(R1)\")\n# Dimensionality: 2\nR2 = [1,2]\nprintln(\"Dimensionality of R2: $length(R2)\")\n# Dimensionality: 3\nR3 = [1,2,3]\nprintln(\"Dimensionality of R3: $length(R3)\")\n\nDimensionality of R1: length(R1)\nDimensionality of R2: length(R2)\nDimensionality of R3: length(R3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#geometry-of-a-vector",
    "href": "ch_2.html#geometry-of-a-vector",
    "title": "2  Vectors, Part I",
    "section": "2.3 Geometry of a vector",
    "text": "2.3 Geometry of a vector\n\n\n\n\n\n\nDefinitions\n\n\n\n\nEuclidean distance\n\nOne measure describing vectors.\nIt is the squareroot of the sum of squared vector elements. \\[\n  ||v|| = \\sqrt{\\sum^n_{i=1} v^2_i}\n  \\]\n\nMagnitude of a vector\n\nThe length of the line produced by the vector.\nAKA the geometric length or norm of a vector.\nComputed using the Euclidean distance of a vector.\nDenoted as: \\(||v||\\)\n\nAngle of a vector\n\nDirection of a vector represented with a straight line.\n\nTail of a vector\n\nThe start of the line produced by the vector.\n\nHead of a vector\n\nThe end of the line produced by the vector.\n\n\n\n\n\nSimply a graphical way to think of a vector; as opposed to the linear algebra way.\nComes with slightly different language.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#vector-operations",
    "href": "ch_2.html#vector-operations",
    "title": "2  Vectors, Part I",
    "section": "2.4 Vector operations",
    "text": "2.4 Vector operations\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector broadcasting\n\nRepeatedly perform an operation between one vector and each element of another vector.\nNot a concept in linear algebra. Only in modern linear algebra.\n\n\n\n\n\nAlmost all operations require that vectors have the same dimensionality and orientation.\n\nIf not, this can lead to broadcasting.\n\n\n\n2.4.1 Addition\n\nElement-wise addition.\nExample:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n# Add the vectors.\nsum = a + b\n# Print the result.\nprintln(\"a+b = $sum\")\n\na+b = [2, 4, 6]\n\n\n\nGeometric vector addition would be done with the following steps:\n\nPlace them such that the tail of one is at the head of the other.\nThe summed vector traverses from the tail of the first vector and the head of the second.\n\n\n\n\n2.4.2 Vector subtraction\n\nElement-wise subtraction.\nExample:\n\n\n# Define the vectors.\na = [2, 4, 6]\nb = [1, 2, 3]\n# Subtract the vectors\ndifference = a - b\n# Print the result\nprintln(\"a-b = $difference\")\n\na-b = [1, 2, 3]\n\n\n\nGeometric vector subtraction:\n\nLine up the vectors so that their tails are at the same coordinate. The difference vector is the line going from the head of the negative vector to the head of the positive vector.\n\n\n\n\n2.4.3 Vector-scalar multiplication\n\n\n\n\n\n\nDefinitions\n\n\n\n\nScalar: A single number\n\nOften denoted with lower-case greek letters.\nE.g., \\(\\lambda\\)\n\nZero’s vector: A special type of vector\n\nDenoted as \\(0\\).\nUsed to solve trivial solutions in linear algebra.\n\n\n\n\n\nMultiply each vector element by the vector.\nExample:\n\n\n# Define the scalar.\nlambda = 2\n# Define the vector\na = [1, 2, 3]\n# Compute the product.\nproduct = 2 * a\n# Print the result.\nprintln(\"2 * a = $product\")\n\n2 * a = [2, 4, 6]\n\n\n\n\n2.4.4 Transposing vectors\n\nEach vector element has a row and column index.\n\nWith row vectors, the index is (N, 0)\nWith column vectors, the index is (0, N)\n\nTransposing a vector simply switches the row and column indexes.\n\n\n\n2.4.5 Vector dot product\n\n\n\n\n\n\nDefinitions\n\n\n\n\nDot product\n\nIs a single number that represents the relationship between two vectors.\nCan be thought of as the unnormalized correlation coefficient.\nVery common in data science.\nOften debicted as: \\(a^Tb\\), \\(a \\cdot b\\), \\(\\langle a, b \\rangle\\)\n\nDistributive property: \\[\na(b+c) = ab + ac\n\\]\n\n\n\n\nTo compute the dot product, you muliply each element between two vectors and then sum the products\n\nA formulaic expression: \\[\na^Tb = \\sum^n_{i=1}a_ib_i\n\\]\n\nExample:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n\n# Use the LinearAlgebra package to compute the dot product.\nab_built = LinearAlgebra.dot(a,b)\n\n# Roll your own dot product.\n#   - Find the length of the existing vectors.\nn = min(length(a), length(b))\n#   - Pre-allocate the space needed for the vector of products.\nproducts_vector = similar(a, Int, n)\n#   - Create the products vector.\nfor i in 1:n\n    products_vector[i] = a[i] * b[i]\nend\n#   - Now sum the products.\nab_ryo = LinearAlgebra.sum(products_vector)\n\n# Confirm they both match\nprintln(\"Is LinearAlgebra.dot() == MyOwn.dot()? $(ab_built == ab_ryo)\")\n\n## Print the dot product.\nprintln(\"a^Tb: $ab_built\")\n\nIs LinearAlgebra.dot() == MyOwn.dot()? true\na^Tb: 14\n\n\n\nIf we scale a vector, then it scales the dot-product by the same amount.\nRevised example:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n\n# Define the scalar.\nlambda = 10\n\n# Rescale the vector.\na = lambda * a\n\n# Calculate the dot product.\nab = LinearAlgebra.dot(a, b)\n\n# Print the dot product.\nprintln(\"10a^Tb = $ab\")\n\n10a^Tb = 140\n\n\n\nThe dot product is distributive. \\[\na^T(b+c) = a^Tb+a^Tc\n\\]\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\nc = [10, 20, 30]\n# Calculate the dot products\nleft_side = LinearAlgebra.dot(a, b+c)\nright_side = LinearAlgebra.dot(a, b) + LinearAlgebra.dot(a, c)\n# Left-side and right-side should be equal.\nprintln(\"a^T(b+c) == a^Tb+a^Tc? $(left_side == right_side)\")\n\na^T(b+c) == a^Tb+a^Tc? true\n\n\n\nGeometric definitions of the dot product:\n\nIt is the product of the magnitudes of the two vectors which is scaled by the cosine of the angle between them. \\[\n\\alpha = cos(\\theta_{a,b}) ||v|| ||w||\n\\]\n\n\n\n\n\n\n\n\nRemember!\n\n\n\nOrthogonal vectors have a dot product of 0.\n\n\n\n\n2.4.6 Alternatives to dot products\n\nHadamard multiplication\n\nElement-wise multiplication of two vectors.\n\nOuter product\n\nEach element in a column-vector is multiplied by each element of a row-vector.\nDenoted as: \\(ab^T\\)\nExample \\[\n\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\cdot \\begin{bmatrix} d & e \\end{bmatrix} = \\begin{bmatrix} ad & ae\\\\ bd & be \\\\ cd & ce \\end{bmatrix}\n\\]\n\n\n\n\n2.4.7 Vector decompositions\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector decomposition:\n\nBreak up the matrix into multiple smaller pieces.\n\n\n\n\n\nUsed to reveal hidden information in a matrix, make it easier to work with, or to compress the data.\nCommon types of decomposition is the Gram-Schmidt procedure and QR decomposition.\nExamples of decomposition:\n\n\\(42.01 = 42 + 0.01\\). Can pull out the noise that we might think the \\(0.01\\) represents. Or it could be used to compress the data as the integer requires less memory than the floating point.\nCan also decompose 42 further into the product of the prime numbers 2, 3, and 7. This is called prime factorization.\n\nOften in data science it is used to break it into two vectors: a vector orthoganal to a reference vector and the other vector remains parallel to the reference vector.\n\n\n2.4.7.1 Orthogonal projection\n\nOften in data science it is used to break the reference vector, \\(r\\), into two vectors: a vector orthoganal to a target vector, \\(t_{\\bot r}\\), and the other vector remains parallel to the target vector, \\(t_{\\parallel r}\\).\n\nE.g., Used in ordinary least squares.\nThis means that we need to decompose the target vector into two vectors such that the two resulting vectors are the sum of the target vector and that one of them is parallel to the reference vector.\n\nGeometric way of doing this:\n\nSay that we have two vectors, \\(a\\) and \\(b\\). Now we want to find the point on \\(a\\) that is as close as possible to the head of \\(b\\). That is, project \\(b\\) on \\(a\\) so that the projection distance is minimized. This projection point on \\(a\\) will be \\(\\beta a\\). Now we need to find the scalar \\(\\beta\\).\n\nTo find the point on \\(a\\) that is closes to the head of \\(b\\) can be found by drawing a line from \\(b\\) that meets \\(a\\) at a right angle. In linear algebra terms, find the dot product between \\(a\\) and the decomposed part of \\(b\\) that is perpendicular as the dot product needs to equal zero. So what this would imply would be \\(a^T(b-\\beta a)=0\\).\nThen to find what \\(\\beta\\) is, we need to do a bit of algebra: \\[\n  \\begin{aligned}\n  a^T(b-\\beta a) = 0\\\\\n  \\beta a^Ta = a^Tb\\\\\n  \\beta = \\frac{a^Tb}{a^Ta}\\\\\n  \\end{aligned}\n  \\]\n\n\nLinear algebraic way:\n\nFirst will define the parallel component, \\(t_{\\parallel r}\\)\n\nIt can be any scaled version of \\(r\\) will be parallel to \\(r\\). \\[\n  t_{\\parallel r} = r \\frac{t^Tr}{r^Tr}\n  \\]\nWe do not want to just compute the scalar \\(\\beta\\), but rather the scaled vector \\(\\beta r\\)\n\nHow do we compute the perpendicular component?\n\nSince we already know that the two vector components need to sum to the original target fector, we can just do: \\[\n  \\begin{aligned}\n  t = t_{\\bot r} + t_{\\parallel r}\\\\\n  t_{\\bot r} = t - t_{\\parallel r}\\\\\n  \\end{aligned}\n  \\]\n\n\nExample:\n\n\n# Define the scaling parameters.\n\n# Define the vectors.\nt = [2,4]\nr = [1,2]\n\n# Compute the scaled vector of beta times b, parallel component.\nt_dot_r = LinearAlgebra.dot(t, r)\nr_dot_r = LinearAlgebra.dot(r, r)\nt_parallel_r = r * (t_dot_r / r_dot_r)\n\n# Compute the perpendicular component.\nt_perpendicular_r = t - t_parallel_r\n\n# Check that perpendicular is actually perpendicular.\nprintln(\"0 = t_perpendicular_r^Tr? $(0 == LinearAlgebra.dot(t_perpendicular_r, r))\")\n\n# Print the parallel component.\nprintln(\"Parallel component: $t_parallel_r\")\n\n# Print the perpendicular component.\nprintln(\"Perpendicular component: $t_perpendicular_r\")\n\n0 = t_perpendicular_r^Tr? true\nParallel component: [2.0, 4.0]\nPerpendicular component: [0.0, 0.0]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_3.html",
    "href": "ch_3.html",
    "title": "3  Vectors Part II",
    "section": "",
    "text": "3.1 Key points\nThe material in this chapter is a bit abstract and I have a bit of a difficult time following the author’s attempts to explain the purpose of some of these concepts. So, I’ve tried to provide some analogies. But to tie it explicitly to how I would think of this in an abstract regression setting, this is what I could come up with:\nA vector space would be like the population. There are all these possible observations (row vectors) with all of these possible responses. These vector spaces should span all possible measures. A vector would be a particular, observed, vector.\nThe basis would be a set of independent variables that can be linearly combined to explain our outcome of interest. Now, there is also a basis vector space which reflects all of the possible vectors that I could use to come up with a linear weighted combination of those vectors to produce a given outcome vector. However, the task is to find the particular basis that is most likely to produce the outcome in a given basis vector space – Bayesian statistics calculates this probability directly (think first few chapters of McElreath’s book).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#vector-sets",
    "href": "ch_3.html#vector-sets",
    "title": "3  Vectors Part II",
    "section": "3.2 Vector sets",
    "text": "3.2 Vector sets\n\nIs a collection of vectors.\nDenoted with capital italic letters.\n\nE.g., \\(S\\) or \\(V\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#linear-weighted-combinations",
    "href": "ch_3.html#linear-weighted-combinations",
    "title": "3  Vectors Part II",
    "section": "3.3 Linear weighted combinations",
    "text": "3.3 Linear weighted combinations\n\nIt is a way of mixing information from multiple variables, vectors, with some vectors contributing more information than others.\nSometimes called linear mixture or weighted combination\nCoefficients are another term for the scalar that summarizes the weight for a particular vector in the weighted combination that they provide.\nTo perform a linear weighted combination:\n\nTake some finite set of vectors, do scalar-vector multiplication and add them. \\[\n  w = \\lambda_1v_2 + \\lambda_2v_2 + \\ldots + \\lambda_iv_i\n  \\]\n\nExample:\n\n\n# Define the vectors.\nv1 = [4,5,1]\nv2 = [-4,0,-4]\nv3 = [1,3,2]\n# Define the weights.\nl1 = 1\nl2 = 2\nl3 = -3\n# Calculate the linear weighted combination.\nw = l1 * v1 + l2 * v2 + l3 * v3\n# Print the result.\nprintln(\"w = $w\")\n\nw = [-7, -4, -13]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#linear-dependence",
    "href": "ch_3.html#linear-dependence",
    "title": "3  Vectors Part II",
    "section": "3.4 Linear dependence",
    "text": "3.4 Linear dependence\nIf you can find some \\(\\lambda\\)s that make the following equation true, then the vectors in the given set, \\(V\\) are linarly dependent. \\[\n0 = \\lambda_1v_2 + \\lambda_2v_2 + \\ldots + \\lambda_iv_, \\space \\space \\lambda \\in \\mathbb{R}\n\\]\nThis excludes the trivial solution where \\(\\lambda = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#subspace-and-span",
    "href": "ch_3.html#subspace-and-span",
    "title": "3  Vectors Part II",
    "section": "3.5 Subspace and span",
    "text": "3.5 Subspace and span\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector Subspace:\n\nIf I have a set of vectors, the span is the set of all possible vectors I can obtain from adding them together and scaling them by any scalar.\n\nVector Span:\n\nThe verb of a vector subspace. The subspace a particular vector spans.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#basis",
    "href": "ch_3.html#basis",
    "title": "3  Vectors Part II",
    "section": "3.6 Basis",
    "text": "3.6 Basis\n\n\n\n\n\n\nDefinitions\n\n\n\n\nBasis:\n\nThe combination of span and vectors: a set of vectors can be a basis for sume subspace if it spans that subspace and is an independent set of vectors.\n\n\n\n\n\nWe can use a number of different ways to describe the unit of some quantified thing. For example 2000 miles versus ~3218 kilometers.\nCartesian axis:\n\nThe familiar XY plane.\nComprise vectors that are mutually orthogonal and unit length.\nExample with a 2D Cartesian graph: \\[\nS_2 = \\begin{Bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\nExample with a 3D Cartesian graph: \\[\nS_3 = \\begin{Bmatrix}\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\n\nOther Basis set for \\(\\mathbb{R}^2\\): \\[\nT = \\begin{Bmatrix}\\begin{bmatrix}3\\\\1\\end{bmatrix},\\begin{bmatrix}-3\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\n\nBoth \\(S_2\\) and \\(T\\) span the same subspace (all of \\(\\mathbb{R}^2\\)). However, these are just different ways of describing the data.\n\n\n\n\n\n\nAttempting to make it less abstract\n\n\n\nNOTE: THIS IS FROM CHATGPT\nImagine you’re navigating through a city using a map. In this analogy:\n\nVector Space: The space you’re moving around in, which could represent the city streets and intersections. Each point in this space corresponds to a location in the city.\nVector: Your position or direction of movement at any given time. For example, if you’re at the intersection of Main Street and Elm Street and you’re moving north, your position can be represented as a vector.\n\nNow, let’s talk about the basis in this context:\nBasis:\nThink of the basis as a set of fundamental directions or movements that you can use to describe any position or movement within the city. These fundamental directions should be enough to reach any point in the city and should be independent of each other.\n\nLinear Independence: Each fundamental direction should be distinct and not redundant. For example, you might choose north, south, east, and west as your fundamental directions. Each direction is distinct and cannot be represented as a combination of the others.\nSpanning: Together, these fundamental directions should cover all possible movements within the city. You should be able to reach any location by combining these fundamental movements in the right proportions.\n\nExample:\nLet’s say you’ve chosen the following basis for navigating the city: north, south, east, and west. Each of these directions represents a unit vector in the corresponding direction.\nNow, if you want to describe a specific movement within the city, such as going from your current location to the nearest park, you can express this movement as a combination of the fundamental directions in your basis. For instance, you might need to move two blocks north and then three blocks west.\nIn this example, the basis vectors (north, south, east, and west) allow you to describe any movement or position within the city by combining them appropriately. They form the building blocks that enable you to navigate and understand spatial relationships within the city.\nSo, in this less abstract example, the concept of a vector basis is akin to choosing a set of fundamental directions or movements that allow you to describe any position or movement within a given space, such as navigating through a city using cardinal directions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_4.html",
    "href": "ch_4.html",
    "title": "4  Vector Applications",
    "section": "",
    "text": "4.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#correlation",
    "href": "ch_4.html#correlation",
    "title": "4  Vector Applications",
    "section": "4.2 Correlation",
    "text": "4.2 Correlation\n\n\n\n\n\n\nDefinitions\n\n\n\n\nCorrelation coefficient:\n\nA scalar that uantifies the linear relationship between two variables.\n[-1, 1]\n\n\n\n\n\nDot product with some normalizations:\n\nMean centering: Subtracting the average value from each data value.\nDivide the dot product from the product of the vector norms. This allows one to cancel measurement units and scales.\n\nFamiliar way:\n\n\\[\nr = \\frac{\\sum^n_{i=1} (x_i - \\bar{x_i})(y_i - \\bar{y_i})}{\\sqrt{\\sum^n_{i=1} (x_i - \\bar{x_i})^2} \\sqrt{\\sum^n_{i=1} (y_i - \\bar{y_i})^2}}\n\\]\n\nLinear algebra way \\[\nr = \\frac{\\tilde{x}^T\\tilde{y}}{||\\tilde{x}|| ||\\tilde{y}||}\n\\]\n\nwhere \\(\\tilde{x}\\) and \\(\\tilde{y}\\) are mean-centered vectors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#cosine-similarity",
    "href": "ch_4.html#cosine-similarity",
    "title": "4  Vector Applications",
    "section": "4.3 Cosine similarity",
    "text": "4.3 Cosine similarity\n\nAlternative to way to assess similarity between vectors other than using correlation.\nDefined as the geometric formula for the dot product when solved for the cosine term.\n\n\\[\ncos(\\theta_{x,y}) = \\frac{x^Ty}{||x|| ||y||}\n\\]\n\nThe difference between this and correlation is that I am not first normalizing \\(x\\) and \\(y\\) by mean-centering them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#time-series-filtering-and-feature-variable-detection",
    "href": "ch_4.html#time-series-filtering-and-feature-variable-detection",
    "title": "4  Vector Applications",
    "section": "4.4 Time series Filtering and Feature (Variable) detection",
    "text": "4.4 Time series Filtering and Feature (Variable) detection\n\n\n\n\n\n\nDefinitions\n\n\n\n\nKernel\n\nRefers to a smooth “template” series that we can use to pull a real series to to smoothen it a bit as a feature-detection method.\n\n\n\n\n\nWe can use the dot product for time series filtering.\n\nWhere filtering is basically a feature-detection method. It essentially detects patterns and signals and pulls out noise from the time series. The effect of this is to “smoothen” the series.\n\nCan use it for convolution:\n\n\nThe dot product is computed for the kernel and the time series signal. - Often filtering requires local feature detection and the kernel is shorter than the whole series. - This means that we often compute the dot product between the kernel and a shorter chunk of the sereis that is the same length of the kernel.\nThis results in a time point of the filtered signal.\nWe then move one time period later and compute the dot product with a different and overlapping signal segment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#k-means-clustering",
    "href": "ch_4.html#k-means-clustering",
    "title": "4  Vector Applications",
    "section": "4.5 k-Means clustering",
    "text": "4.5 k-Means clustering\n\n\n\n\n\n\nDefinitions\n\n\n\n\nk-Means clustering\n\nUnsupervised method of putting multivariate data into a relatively small number of groups or categories.\n\n\n\n\n\nWith concepts covered in the earlier chapters,this is how this procedure is done:\n\n\nDefine \\(k\\) centroids as random points in the vector space.\n\nEach centroid is a class, or category.\n\nCompute the Euclidean distance between each observation and each centroid.\nAssign each data observation to the group with the closest centroid (the smallest Euclidean distance).\nUpdate the centroid location by taking the average of all data observations assigned to the centroid.\nRepeat steps 2-4 until a convergence criteria is satisfied or N iterations have completed.\n\nOften these conditions are either until N iterations occur.\nMore sophisticated implementations are often to repeat these steps until centroid locations do not move much.\n\n\n\n# Try to write your own k-means clustering algorithm here.\n# Rather than writing double for-loop, can use broadcasting for efficiency.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_5.html",
    "href": "ch_5.html",
    "title": "5  Matrices, Part I",
    "section": "",
    "text": "5.1 Matrices\n# Define a 3x5 matrix.\nA = [[1,0,1] [3,2,4] [5,4,7] [7,6,8] [9,8,9]]\nprintln(\"A, a 3x5 Matrix:$A\")\n\n# Ensure that it is 3x5\nnum_rows = size(A, 1)\nnum_columns = size(A, 2)\nprintln(\"A has $num_rows rows.\")\nprintln(\"A has $num_columns columns.\")\n# Slicing\n#     - Slice the first row.\nfirst_row = A[1, :]\nprintln(\"A's first row is: $first_row\")\n#     - Slice the first column.\nfirst_column = A[:, 1]\nprintln(\"A's first column is: $first_column\")\n#    - Slice the last row.\n#           - Note that there are shortcuts.\nlast_row = A[end, :]\nprintln(\"A's last row is: $last_row\")\n#           - Slice the last column.\n#               - Note the shortcut.\nlast_column = A[:, end]\nprintln(\"A's last column is: $last_column\")\n\nA, a 3x5 Matrix:[1 3 5 7 9; 0 2 4 6 8; 1 4 7 8 9]\nA has 3 rows.\nA has 5 columns.\nA's first row is: [1, 3, 5, 7, 9]\nA's first column is: [1, 0, 1]\nA's last row is: [1, 4, 7, 8, 9]\nA's last column is: [9, 8, 9]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrices",
    "href": "ch_5.html#matrices",
    "title": "5  Matrices, Part I",
    "section": "",
    "text": "Defintions\n\n\n\n\nMatrices:\n\nHighly versitile mathematical objects.\nIn DS, they are often conceptualized as a horizontally stacked set of column vectors.\n\nOften appear in the observations-by-feature format.\n\nRows are observations.\nColumns are features (variables).\n\nIndices are often denoted as Row \\(\\times\\) Column.\n\nDenoted with bold-faced capital letters.\n\nE.g., A or M\n\n\n\n\n\n\nExample of matrices:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#types-of-matrices",
    "href": "ch_5.html#types-of-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.2 Types of matrices",
    "text": "5.2 Types of matrices\n\nThere are an infinite number of matrices. However, there are certain classes that are relatively common.\n\n\n5.2.1 Random numbers matrix\n\nContains numbers drawn at random from some distribution.\nExample:\n\n\n# There are many packages available.\n\n# Create a 3x3 matrix randomly generated from Gaussian dist.\n#       - Create a Gaussian distribution with mean = 5 and std. dev = 2.\nmu = 5\nsigma_squared = 2\ndist = Distributions.Normal(mu, sigma_squared)\n#   - Generate the matrix pulling from the dist.\nnum_rows = 3\nnum_features = 4\nmatrix = Random.rand(dist, num_rows, num_features)\nmatrix\n\n3×4 Matrix{Float64}:\n 6.3592   3.61815  2.68209  3.80779\n 7.73537  5.25992  5.50092  3.50644\n 4.8147   5.20514  3.63537  5.56604\n\n\n\n\n5.2.2 Square and rectangular matrices\n\nThe Square matrix is \\(\\mathbb{R}^{N \\times N}\\)\n\nThe matrix has the same number of rows as columns.\n\nThe Rectangular matrix is \\(\\mathbb{R}^{M \\times N}\\) or \\(\\mathbb{R}^{N \\times M}\\) where \\(M &gt; N\\).\n\nTall rectangular matrices are the former (\\(M \\times N\\))\nWide rectangular matrices are the latter (\\(N \\times M\\))\n\nExample of square and rectangular matrices.\n\n\n# Define a Square matrix.\nsquare_matrix = Random.rand(3,3)\nprintln(\"Example of a Square Matrix:\")\nsquare_matrix\n\nExample of a Square Matrix:\n\n\n3×3 Matrix{Float64}:\n 0.570851  0.798623  0.240766\n 0.933675  0.832721  0.310331\n 0.586793  0.160646  0.523838\n\n\n\n# Define a Tall matrix.\ntall_matrix = Random.rand(4,3)\nprintln(\"Example of a Tall Matrix:\")\ntall_matrix\n\nExample of a Tall Matrix:\n\n\n4×3 Matrix{Float64}:\n 0.720362  0.33162   0.325086\n 0.68186   0.296971  0.515731\n 0.514356  0.415962  0.0568227\n 0.982175  0.747093  0.265896\n\n\n\n# Define a Wide matrix.\nwide_matrix = Random.rand(3, 4)\nprintln(\"Example of a Wide Matrix:\")\nwide_matrix\n\nExample of a Wide Matrix:\n\n\n3×4 Matrix{Float64}:\n 0.633416  0.508978  0.238866  0.6797\n 0.882501  0.62274   0.82069   0.925353\n 0.442554  0.829215  0.396217  0.302257\n\n\n\n\n5.2.3 Diagonal matrix\n\nA square matrix that has zeros on all of the off-diagonal elements.\nExample\n\n\n# Create a diagonal matrix.\n#       - Define the elements on the diagonal.\ndiag_ele = [1, 2, 3, 4]\n#       - Create the matrix.\nD = LinearAlgebra.Diagonal(diag_ele)\nprintln(\"Example of a Diagonal Matrix:\")\nD\n\nExample of a Diagonal Matrix:\n\n\n4×4 Diagonal{Int64, Vector{Int64}}:\n 1  ⋅  ⋅  ⋅\n ⋅  2  ⋅  ⋅\n ⋅  ⋅  3  ⋅\n ⋅  ⋅  ⋅  4\n\n\n\n\n5.2.4 Identity Matrix\n\nA special type of a square diagonal matrix where the elements on the diagonal are 1’s.\nUsually denoted as I.\nExample\n\n\n# Create a identity matrix.\n#       - Define the size of the matrix.\nn = 4\n#   - Create the matrix.\nI = LinearAlgebra.I(n)\nprintln(\"Example of a Identity Matrix:\")\nI\n\nExample of a Identity Matrix:\n\n\n4×4 Diagonal{Bool, Vector{Bool}}:\n 1  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅\n ⋅  ⋅  1  ⋅\n ⋅  ⋅  ⋅  1\n\n\n\n\n5.2.5 Triangular Matrix\n\nA matrix contains all zeros either above (Upper Triangular) or below (Lower Triangular) the diagonal.\n\n\n# Create a diagonal matrix.\ndiag_ele = [1, 2, 3, 4]\nD = LinearAlgebra.Diagonal(diag_ele)\nprintln(\"Diagonal Matrix:\")\nD\n#   The Upper Triangle.\nUT = LinearAlgebra.triu(D)\nprintln(\"Example of an Upper Triangle Matrix:\")\nUT\n# The Lower Triangle.\nLT = LinearAlgebra.tril(D)\nprintln(\"Example of a Lower Triangle Matrix:\")\nLT\n\nDiagonal Matrix:\nExample of an Upper Triangle Matrix:\nExample of a Lower Triangle Matrix:\n\n\n4×4 Diagonal{Int64, Vector{Int64}}:\n 1  ⋅  ⋅  ⋅\n ⋅  2  ⋅  ⋅\n ⋅  ⋅  3  ⋅\n ⋅  ⋅  ⋅  4\n\n\n\n\n5.2.6 Zeros matrix\n\nIt is a Matrix of all Zeros.\nUsually denoted as 0.\n\n\n# Create a 4x4 Zeros matrix.\nO = LinearAlgebra.zeros(4,4)\nprintln(\"Example of a 4x4 Zero's Matrix:\")\nO\n\nExample of a 4x4 Zero's Matrix:\n\n\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-addition",
    "href": "ch_5.html#matrix-addition",
    "title": "5  Matrices, Part I",
    "section": "5.3 Matrix Addition",
    "text": "5.3 Matrix Addition\n\nTo add two matrices, you add their corresponding elements. That is:\n\n\\[\nA+B = \\begin{bmatrix}(A11 + B11) & (A21 + B21)\\\\ (A12 + B12) & (A22 + B22)\\\\\\end{bmatrix}\n\\]\n\nAs usual, they must be of the same size.\nExample\n\n\n# Defining the matrices.\nA = [[1, 2] [3, 4]]\nB = [[5, 6] [7, 8]]\nprintln(\"Matrix A:\")\nA\n\nMatrix A:\n\n\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\nprintln(\"Matrix B:\")\nB\n\nMatrix B:\n\n\n2×2 Matrix{Int64}:\n 5  7\n 6  8\n\n\n\n# Add the matrices\nC = A + B\nprintln(\"Result of adding the two matrices:\")\nC\n\nResult of adding the two matrices:\n\n\n2×2 Matrix{Int64}:\n 6  10\n 8  12",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#shifting-a-matrix",
    "href": "ch_5.html#shifting-a-matrix",
    "title": "5  Matrices, Part I",
    "section": "5.4 “Shifting” a matrix",
    "text": "5.4 “Shifting” a matrix\n\nYou cannot formally add a scalar to a matrix as in \\(\\lambda + A\\)\nYou can broadcast a scalar to a square matrix. This is what is referred to as “Shifting” a matrix.\n\nTo do this, you take the scalar and multiply it to an identity matrix and then add the identity matrix to the resulting matrix. \\[\nA + \\lambda I\n\\]\n\nOnly the diagonal elements change. You don’t want to shift much of the matrix. How much you shift a matrix depends a lot and is somewhat relative.\nShifting has two really important applications: finding eigenvalues of a matrix and regularizing matrices when fitting models to data.\nHere’s a numerical example\n\n\\[\n\\begin{bmatrix}4 & 5 & 1\\\\ 0 & 1 & 11 \\\\ 4 & 9 & 7\\\\\\end{bmatrix} + 6 \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\\\\\end{bmatrix} = \\begin{bmatrix}10 & 5 & 1\\\\ 0 & 7 & 11\\\\ 4 & 9 & 13\\\\\\end{bmatrix}\n\\]\n\nHere’s how to do it in Julia.\n\n\n# Define the matrix.\nA = [[1, 2, 3] [4, 5, 6] [7, 8, 9]]\nprintln(\"Matrix A:\")\nA\n# Define the scalar.\nlambda = 2\n\n# Shift A.\nB = A + (lambda * LinearAlgebra.I(size(A, 1)))\nprintln(\"Shifted version of A + 2I\")\nB\n\nMatrix A:\nShifted version of A + 2I\n\n\n3×3 Matrix{Int64}:\n 3  4   7\n 2  7   8\n 3  6  11",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#scalar-and-hadamard-multiplications",
    "href": "ch_5.html#scalar-and-hadamard-multiplications",
    "title": "5  Matrices, Part I",
    "section": "5.5 Scalar and Hadamard multiplications",
    "text": "5.5 Scalar and Hadamard multiplications\n\nBoth types of multiplication work element-wise.\nHadamard multiplication is identified with \\(A \\odot B\\).\nExample of scalar multiplication:\n\n\n# Define the matrix.\nA = [[1,2] [1,2]]\nprintln(\"A:$A\")\n# Define the scalar.\nlambda = 2\n# Print the result of the scalar-multiplied A.\nlambda * A\n\nA:[1 1; 2 2]\n\n\n2×2 Matrix{Int64}:\n 2  2\n 4  4\n\n\n\nExample of Hadamard multiplication:\n\n\n# Define the two matrices.\nA = [[1, 1] [1, 1]]\nB = [[1, 2] [1, 2]]\n# Multiply the matrices.\n#   - Use the .* to do element-wise multiplication.\nA .* B\n\n2×2 Matrix{Int64}:\n 1  1\n 2  2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-multiplication",
    "href": "ch_5.html#matrix-multiplication",
    "title": "5  Matrices, Part I",
    "section": "5.6 Matrix multiplication",
    "text": "5.6 Matrix multiplication\n\nRecall the dot product between two vectors:\n\n\\[\na^Tb = \\sum^n_{i=1}a_ib_i\n\\]\n\nMatrix multiplication is essentially the collection of dot products between rows of one matrix and columns of the other matrix.\n\nSpecifically, the \\((i,j)\\)th element in the product matrix is the dot product between the \\(i\\)th row of the left matrix and the \\(j\\)th column in the right matrix.\n\nConditions:\n\nIf \\(A\\) is \\(\\mathbb{R}^{M \\times N}\\) then \\(B\\) must be \\(\\mathbb{R}^{N \\times K}\\).\n\nIn words, the inner dimensions must mactch\n\nThe result produces the outer dimensions, \\(AB\\) is \\(\\mathbb{R}^{M \\times K}\\).\nDoes not obey the commutative law. So \\(AB\\) may be valid but \\(BA\\) will not be valid as \\(K \\neq M\\).\n\nInterpreting it: The product matrix is one that stores all of the pairwise linear relationships between the rows of the left matrix and the columns of the right matrix. This is the basis for computing covariance and correlation matrices, GLM, and many other applications.\nNumerical example\n\n\\[\n\\begin{bmatrix}2&3\\\\4&5\\\\\\end{bmatrix}\\begin{bmatrix}a&b\\\\c&d\\\\\\end{bmatrix} = \\begin{bmatrix}(2a + 3c) & (2b + 3d)\\\\(4a + 5c) & (4b + 5d)\\\\\\end{bmatrix}\n\\]\n\nExample in julia\n\n\n# Define the matrices.\nA = [[2,4] [3,5]]\nB = [[2,4] [3,5]]\nprintln(\"A:$A\")\nprintln(\"B:$B\")\n# Multiply the matrices.\nA*B\n\nA:[2 3; 4 5]\nB:[2 3; 4 5]\n\n\n2×2 Matrix{Int64}:\n 16  21\n 28  37",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-vector-multiplication",
    "href": "ch_5.html#matrix-vector-multiplication",
    "title": "5  Matrices, Part I",
    "section": "5.7 Matrix-vector multiplication",
    "text": "5.7 Matrix-vector multiplication\n\nEssentially matrix multiplication where one matrix happens to be a vector.\nExtending the rules from matrix multiplication:\n\nA matrix can be right-multiplied by a column vector but not a row vector. \\(Av\\) and \\(v^TA\\) are valid but \\(Av^T\\) and \\(vA\\) are not valid.\nThe product is always a vector and the orientation of the vector depends on the orientation of the multiplicand vector: premultiplying a row vector produces a row vector and postmultiplying a matrix by a column vector produces a column vector.\n\nThis is often used when obtaining the model-predicted values by multiplying the design matrix by the regression coefficients, \\(X\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#linear-weighted-combinations",
    "href": "ch_5.html#linear-weighted-combinations",
    "title": "5  Matrices, Part I",
    "section": "5.8 Linear weighted combinations",
    "text": "5.8 Linear weighted combinations\n\nIn the previous chapter we calculated these with separate scalars and vectors. Now we can do this in a more compact and scalable method: put the vectors in a matrix, put the weights into corresponding elements of a vector, then multiply.\nNumerical example:\n\n\\[\n4 \\begin{bmatrix}3\\\\0\\\\6\\\\\\end{bmatrix} + 3 \\begin{bmatrix}1\\\\2\\\\5\\\\\\end{bmatrix} \\implies \\begin{bmatrix}3&1\\\\0&2\\\\6&5\\\\\\end{bmatrix}\\begin{bmatrix}4\\\\3\\end{bmatrix}\\\\\n= \\begin{bmatrix}(12 + 3)\\\\(0 + 6)\\\\ (24 + 15)\\\\\\end{bmatrix}\n\\]\n\nNumerical example in Julia:\n\n\n# Define the matrices.\nA = [[3,0,6] [1,2,5]]\nb = [4,3]\n# Multiply them.\nprintln(\"A*b: \", A*b)\n\nA*b: [15, 6, 39]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#transposing-matrices",
    "href": "ch_5.html#transposing-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.9 Transposing matrices",
    "text": "5.9 Transposing matrices\n\nYou swap the row and column indices.\nDenoted with \\(A^T\\).\nNumerical example \\[\n\\begin{bmatrix}\n  3&0&4\\\\\n  9&8&3\\\\\n\\end{bmatrix}^T\n= \\begin{bmatrix}\n  3&9\\\\\n  0&8\\\\\n  4&3\\\\\n\\end{bmatrix}\n\\]\nExample in Julia\n\n\n# Define the matrix\nA = [[3,9] [0,8] [4,3]]\n# Transpose the matrix\nA'\n\n3×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 3  9\n 0  8\n 4  3\n\n\n\nFor two column vectors of \\(M\\times 1\\), transposing the vector and not the second gives a \\(1 \\times M\\) matrix and a \\(M \\times 1\\) matrix. The inner dimensions match and the product is \\(1 \\times 1\\). This is why the dot product is denoted with \\(a^Tb\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#order-of-operations-live-evil",
    "href": "ch_5.html#order-of-operations-live-evil",
    "title": "5  Matrices, Part I",
    "section": "5.10 Order of operations: LIVE EVIL",
    "text": "5.10 Order of operations: LIVE EVIL\n\nThe transose of multiplied matrices is the same as the individual matrices transposed and multiplied in rversed order. \\[\n(LIVE)^T = E^TV^TI^TL^T\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#symmetric-matrices",
    "href": "ch_5.html#symmetric-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.11 Symmetric matrices",
    "text": "5.11 Symmetric matrices\n\nThe corresponding rows and columns are equal. This means that when you swap rows and columns, nothing changes.\n\n\\(A^T = A\\)\n\nNeeds to be a square matrix.\nTo create a symmetric matrix, you need to multipy any matrix (including nonsquare and nonsymmetric) by its transpose will produce a square symmetric matrix.\n\n\\(A^TA\\) and \\(AA^T\\) are both square symmetric.\n\nThe proof:\n\nIf \\(A\\) is \\(\\mathbb{R}^{M \\times N}\\), then \\(A^T\\) is \\(\\mathbb{R}^{N \\times M}\\). \\(A^TA\\) must then be \\(\\mathbb{R}^{N \\times N}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_6.html",
    "href": "ch_6.html",
    "title": "6  Matrices, Part II",
    "section": "",
    "text": "6.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#key-points",
    "href": "ch_6.html#key-points",
    "title": "6  Matrices, Part II",
    "section": "",
    "text": "There are induced and elementwise matrix norms. The former reflects magnitudes of th elements and the latter reflects hte geometric-transformative effect of the matrix on vectors.\nThe most commonly used elementwise norm is the Euclidean, Frobenius, or \\(\\mathcal{L}\\) norm.\nThe trace is the sum of its diagonal elements\nThere are four matrix spaces (column, row, null, right-null) and they are the linear weighted combinations of different features of it.\nThe column space comprises all linear weighted combinations of the columns.\nOften we want to know if some vector, \\(b\\) is in th column space of a matrix where we can find a vector \\(x\\) such that \\(Ax=b\\)\nThe null space is the set of vectors that linearly combines the columns producing the zeros vector. It is important for finding eigenvectors.\nThe null space occurs when a set of vectors are linearly dependent and reflects a singular (rank-reduced) matrix.\nShifting a square matrix by adding a constant to the diagonal ensures full-rank.\nThe determinant is a number to describe square matrices. It is zero for all singular matrices and nonzero for full-rank matrices.\nThe characteristic polynomial transforms square matricesl, shifting it by \\(\\lambda\\), into an algebraic equation equal to the determinant so that we can solve by \\(\\lambda\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#matrix-norms",
    "href": "ch_6.html#matrix-norms",
    "title": "6  Matrices, Part II",
    "section": "6.2 Matrix Norms",
    "text": "6.2 Matrix Norms\n\n\n\n\n\n\nRecall\n\n\n\n\nEuclidean distance: \\[\n||v|| = \\sqrt{\\sum_{i=1}^N v_i^2}\n\\]\nVector norms:\n\nA vector’s Euclidean geometric length.\n\n\n\n\n\nA matrix norm is not the same thing as a vector norm.\nDenoted as \\(\\parallel A \\parallel\\)\nSimilar to vector norms in that each norm is a number that characterizes a matrix.\nTwo main families of matrix norms:\n\nElement-wise norms\n\nComputed based on the individual elements of the matrix.\nReflect the magnitudes of the elements in the matrix.\n\nInduced norms\n\nOne key function of a matrix is to encode transformations on a vector.\nGiven this, the induced norm of a matrix measures how much that transformation scales (stretches or shrinks) the vector.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#euclidean-norm-an-example-of-a-element-wise-norm",
    "href": "ch_6.html#euclidean-norm-an-example-of-a-element-wise-norm",
    "title": "6  Matrices, Part II",
    "section": "6.3 Euclidean norm (an example of a element-wise norm)",
    "text": "6.3 Euclidean norm (an example of a element-wise norm)\n\nAKA Frobenius norm or the \\(\\mathcal{L}\\) norm (the regularized norm) \\[\n\\paralell A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2}\n\\]\nIn words, it is the square root of the sum of all matrix elements squared.\nThis norm in particular, but all norms, generally, are used quite a lot in machine learning and regularization.\nAlso used as a measure of distance from another matrix, “Matrix Distance”: \\[\nC = A - B\n\\]\n\nWhere if \\(B=A\\) then \\(C=0\\) because the numerical values in the matrices are similar but if \\(B \\neq A\\) then \\(C\\) increases when the numerical values get increasingly dissimilar.\n\nExample\n\n\n# Define a matrix.\nA = [[1,2] [3,4]]\nprintln(\"A:$A\")\n# Calculate the Frobenius norm.\nfro_norm = LinearAlgebra.norm(A)\nprintln(\"||A||=$fro_norm\")\n\nA:[1 3; 2 4]\n||A||=5.477225575051661\n\n\n\n6.3.1 Matrix trace\n\n\n\n\n\n\nDefinitions\n\n\n\n\nTrace\n\nthe sum of its diagonal elements.\ndenoted as \\(tr(A)\\).\n\n\n\n\n\nNumerical example:\n\nThe trace of the two following matrices is \\(14\\). \\[\n\\begin{bmatrix}\n4&5&6\\\\\n0&1&4\\\\\n9&9&9\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n0&0&0\\\\\n0&8&0\\\\\n1&2&6\\\\\n\\end{bmatrix}\n\\]\n\nThe trace of a matrix equals the sum of it’s eigenvalues.\n\nA measure of the volume of its eigenspace.\n\nThe Frobenius norm can be calculated as the square root of the trace of t he matrix times its transpose. This happens because each diagonal element of the matrix \\(A^TA\\) is defined by the dot product of each row with itself: \\[\n\\parallel A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2} = \\sqrt{tr(A^T)}\n\\]\n\n\n# Define the matrix.\nA = [[4,0,9] [5,1,9] [6,4,9]]\nprintln(\"A:$A\")\n# Calculate the trace.\n#   - With the function.\ntrace_A = LinearAlgebra.tr(A)\n# Print trace.\nprintln(\"tr(A) = $trace_A\")\n\nA:[4 5 6; 0 1 4; 9 9 9]\ntr(A) = 14",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#matrix-spaces",
    "href": "ch_6.html#matrix-spaces",
    "title": "6  Matrices, Part II",
    "section": "6.4 Matrix spaces",
    "text": "6.4 Matrix spaces\n\n6.4.1 Colum space\n\nConceptualize the matrix as a set of column vectors.\nConsider the infinity of real-valued scalars instead of working with a specific set of scalars.\n\nAn infinite number of scalars gives an infinite number of ways to combine a set of vectors.\nThis resulting set of vectors that can be created from the weighted combination of the columns in the matrix is the column space.\n\nDenoted as \\(C(A)\\)\nNumerical examples: \\[\nC(\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}) = \\lambda\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\]\n\n\\(\\lambda\\), the scalar, can be any real number as there are any scalar can produce a vector produced by \\(A\\). \\[\nC(\\begin{bmatrix}1&1\\\\3&2\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}1\\\\2\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\]\nWhat is the set of vectors that can be created with some linear combination of these two column vectors?\n\nAll vectors in $^2.\n\n\n\n\\[\nC(\\begin{bmatrix}1&2\\\\3&6\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}2\\\\6\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\] - In this example, it is not possible to be in \\(\\mathbb{R}^2\\) as these columns are linearly dependent to each other – they are collinear. THis is because one is already a scaled version of the other. - This means that the column space of this matrix is just a 1D subspace. - The dimensionality of the column space equals the number of columns only if they form a linearly independent set.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#row-space",
    "href": "ch_6.html#row-space",
    "title": "6  Matrices, Part II",
    "section": "6.5 Row space",
    "text": "6.5 Row space\n\nThe same concept but we consider all possible weighted combinations of the rows instead of the columns.\nCan be \\(R(A)\\) or \\(C(A^T)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#null-spaces",
    "href": "ch_6.html#null-spaces",
    "title": "6  Matrices, Part II",
    "section": "6.6 Null spaces",
    "text": "6.6 Null spaces\n\nThe column space can be summarized as \\(Ax = b\\).\n\nIn words, “Can we find some set of coefficients in x such that the weighted combination of columns in A produce vector b”? If yes, then \\(b \\in C(A)\\).\n\nThe null space, however is summarized as \\(Ay = 0\\)\n\nIn words, “Can we find some set of coefficients in y such that the weighted combination of columns in A produces the zeros vector?”\n\nNumerical examples:\n\n\\[\n\\begin{align*}\n  A = \\begin{bmatrix}1&-1\\\\-2&2\\end{bmatrix} \\\\\n  Ay = 0\n  y = \\begin{bmatrix}-1\\\\1\\\\\\end{bmatrix} \\\\\n  N(A) = \\lambda\\begin{bmatrix}1\\\\1\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\end{align*}\n\\] - There are an infinite number of vectors in y that satisfy \\(Ay=0\\) for \\(A\\). Thus we can express the null space matrix \\(N(A)\\) how we did above.\n\\[\n\\begin{align*}\n  \\begin{bmatrix}1&-1\\\\-2&3\\\\\\end{bmatrix} \\\\\n  Ay = 0 \\\\\n  y = \\begin{bmatrix}\\end{bmatrix} \\\\\n  N(A) = \\begin{bmatrix}\\end{bmatrix}\n\\end{align*}\n\\] - This matrix does not allow us to produce a null space matrix. - Important note: - Linear independent columns in a matrix produce a column space greater than 1 but no larger than the number of linearly independent columns whereas linear dependent columns produce a column space equal to 1, the unscaled vector. - Using terminology that is explained below: full-rank and full column-rank matrices have empty null spaces whereas reduced rank-matrices have nonempty null spaces.\n\n6.6.1 Unit vector\n\nThey are convenient and are numerically stable. So they are common as bases for subspaces.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#rank",
    "href": "ch_6.html#rank",
    "title": "6  Matrices, Part II",
    "section": "6.7 Rank",
    "text": "6.7 Rank\n\nRelated to the dimensionalities of matrix subspaces and is important for things like inverting matrices and determining the number of solutions to a system of equations.\nProperties of rank:\n\nRank is a nonnegative integer.\nEvery matrix has one unique rank.\nThe rank of a matrix is denoted with \\(r(A)\\)\nThe maximum possible rank of a matrix is equal to either the smaller of the row or column count, \\(min\\{M,N\\}\\)\nA matrix with a maximum rank, \\(r = min\\{M,N\\}\\), is called “full rank”\nA matrix with a rank smaller than the full rank, \\(r &lt; min\\{M,N\\}\\) is called reduced rank, rank deficient, or singular.\nScalar multiplication does not impact matrix rank\n\nInterpreting the rank of a matrix\n\nThe largest number of columns (or rows) that form a linearly independent set.\nOR The dimensionality of the column space.\nOR The number of dimensions containing information in the matrix.\nOR The number of nonzero singular values of the matrix.\n\nExample:\n\n\n# Define matrix.\nA = [[1,2] [3,4]]\n# Find the rank.\nrank_A = LinearAlgebra.rank(A)\n# Print the results.\nprintln(\"rank(A) = $rank_A\")\n\nrank(A) = 2\n\n\n\n6.7.1 Rank of special matrices\n\nVectors\n\nThey all have a rank of 1\n\nZeros matrices\n\nNo matter the size, it has a rank of 1.\n\nIdentity matrices \\[\nr(I_N) = N\n\\]\n\nThe rank equals the number of rows.\n\nDiagonal matrices\n\nThe rank equals the number of nonzero diagonal elements.\n\nTriangular matrices\n\nFull tank only if there are nonzero values in all diagonal elements. If it has at least one zero in the diagonal, it will be reduced rank.\n\nRandom matrices\n\nShould be full rank, but depends a lot. This is because the probability that any of the columns in the matrix are linearly dependent is low with certain distributions, but with others it could be high.’\n\nRank of added and multiplied matrices\n\nThe ranks of the two individual matrices provide the upper bounds for the maximum possible rank. \\[\nrank(A + B) \\leq rank(A) + rank(B) \\\\\nrank(AB) \\leq min \\{rank(A), rank(B)\\}\n\\]\n\nRank of shifted matrices\n\nThey are full rank. The reason is that this is a key goal of shifting a matrix is to increase the rank.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingular values\n\nEvery \\(M\\times N\\) matrix contains a set of $min{M,N} nonnegative singular values.\nThey encode the “importance” or “spaciousness” of different directions in the column and row spaces of a matrix.\nWhere thre is a singular value of 0, there is a null space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#augmented-matrices",
    "href": "ch_6.html#augmented-matrices",
    "title": "6  Matrices, Part II",
    "section": "6.8 Augmented matrices",
    "text": "6.8 Augmented matrices\n\nAdd extra columns to the right-hand side of the matrix. You start with an \\(M \\times N\\) matrix and augment it with a \\(M \\times K\\) matrix. The augmented matrix will be \\(M \\times (N + K)\\)\nThey just need to have the same number of rows.\nNumerical example \\[\n\\begin{bmatrix}\n  4&5&6\\\\\n  0&1&2\\\\\n  9&9&4\\\\\n\\end{bmatrix} \\sqcup \\begin{bmatrix}\n  1\\\\\n  2\\\\\n  3\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n  4&5&6&1\\\\\n  0&1&2&2\\\\\n  9&9&4&3\\\\\n\\end{bmatrix}\n\\]\n\n\n# Define the initial matrix.\nA = [[4,0,9] [5,1,9] [6,2,4]]\nprintln(\"A:$A\")\n# Define the matrix/vector to add.\nB = [1; 2; 3]\nprintln(\"B:$B\")\n# Augment the matrix.\nC = hcat(A, B)\nprintln(\"C:$C\")\n\nA:[4 5 6; 0 1 2; 9 9 4]\nB:[1, 2, 3]\nC:[4 5 6 1; 0 1 2 2; 9 9 4 3]\n\n\n\nYou can use augmented matrices in an algorithm to determine whether a particular vector is in the column space of a matrix.\n\n\nAugment the matrix with the given vector.\nCompute the ranks of the two matrices\nCompare the two ranks:\n\nIf the rank of the original matrix and the augmented matrix is the equal, then the vector is in the column space of the original matrix.\nIf the rank of the original matrix is less than the augmented matrix, then the vector is not in the column space of the original matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#determinants",
    "href": "ch_6.html#determinants",
    "title": "6  Matrices, Part II",
    "section": "6.9 Determinants",
    "text": "6.9 Determinants\n\nThe determinant is the number associated with a square matrix.\n\nCan be very numerically unstable for large matrices.\n\nIt is extremely important to a lot of applications to linear algebra.\nTwo key properties:\n\nIt is defined only for square matrices\nIt is zero for singular (reduced-rank) matrices.\n\nDenoted with \\(det(A)\\) or \\(|A|\\).\nThe geometric interpretation: how much the matrix stretches vectors during matrix vector multiplication\n\nA negative determinant means that one axis is rotated during the transformation.\n\nIn linear algebra, it is used to find eigenvalues or invert a covariance matrix.\nComputing them:\n\nIt is essentially the product of the diagonal minus the product of the off-diagonal. This doesn’t really hold true when it is not \\(M=2;N=2\\).\nJust rely on your computer to do it because this strategy doesn’t work all the time; thus why it is “numerically unstable”. \\[\ndet(\\begin{bmatrix}\n  a&b\\\\\n  c&d\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n  a&b\\\\\n  c&d\\\\\n\\end{bmatrix} = ad-bc\n\\]\n\nDeterminants are zero for reduced-rank matrices\n\nThis is because any reduced-rank matrix has at least one column that is a linear combination of another. So the difference between the products of the different elements will be null.\n\nOne reason they are useful is that they create a algebraic equation that we can use to calculate unknown parameters, especially if we know the determinate:\n\nFor example, if you want to determine whether two columns are linearly dependent: \\[\ndet(\\begin{bmatrix}\n  a&\\lambda a\\\\\n  c&\\lambda c\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n  a&\\lambda a\\\\\n  c&\\lambda c\\\\\n\\end{bmatrix} = a\\lambda c-a\\lambda c\n\\]\nCharacteristic polynomial:\n\nThis is produced when we combine matrix shifting with the determinant. \\[\n  det(A-\\lambda I) = \\delta\n  \\]\nIts a polynomial because the shifted \\(M \\times M\\) matrix produces a \\(\\lambda^M\\) term and has \\(M\\) solutions.\nNumeric example with a \\(2 \\times 2\\) matrix: $$\n\\[\\begin{bmatrix}\n  a-\\lambda & b\\\\\n  c & d-\\lambda\\\\\n  \\end{bmatrix}\\]\n= 0 ^2 - (a+d)+ (ad - bc) = 0\nSolutions where \\(\\delta = 0\\) are used to evaluate the eigenvalues of a matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_7.html",
    "href": "ch_7.html",
    "title": "7  Matrix applications",
    "section": "",
    "text": "7.1 Covariance",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  },
  {
    "objectID": "ch_7.html#covariance",
    "href": "ch_7.html#covariance",
    "title": "7  Matrix applications",
    "section": "",
    "text": "Covariance is the numerator of the correlation equation – it is the dot product between two mean-centered variables – but retains the scale of the data.\n\nPositive values indicate that they move together.\nNegative values indicate when they move apart.\n0 indicates that there is no linear relationship\n\nHow it works with linear algebra:\n\nCreate a matrix where each column corresponds to each variable. Let’s call a given column \\(X\\).\nTo find the covariance, we can do matrix multiplication to get the dot products between them. However, we cannot do \\(XX\\) as they won’t have the right dimensions.\nSo, we transpose \\(X\\) to get \\(X^T\\).\nNow, we do matrix multiplication to get \\(X^TX\\). This generates the dot product between each column (assuming we meancentered the columns).\nThen we can multiply by \\(\\frac{1}{n-1}\\) to get the covariance matrix.\n\nSummary of the steps. \\[\nC = X^TX\\frac{1}{n-1}\n\\]\nFeatures of \\(C\\):\n\nThe covariance matrix will be symmetric as any matrix multiplied by its transpose produces a symmetric matrix.\nThe diagonal elements of \\(C\\) are the covariances for each variable with itself. So, the variance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  },
  {
    "objectID": "ch_7.html#geometric-transformations-through-matrix-vector-multiplication",
    "href": "ch_7.html#geometric-transformations-through-matrix-vector-multiplication",
    "title": "7  Matrix applications",
    "section": "7.2 Geometric transformations through matrix-vector multiplication",
    "text": "7.2 Geometric transformations through matrix-vector multiplication\n\nPure rotation matrices\n\nRotates a vector but preserves its length \\[\n  T = \\begin{bmatrix}\n  cos(\\theta) & sin(\\theta)\\\\\n  -sin(\\theta) & cos(\\theta)\\\\\n  \\end{bmatrix}\n  \\]\nIt is a type of orthogonal matrix.\nHow?\n\nThe colums of \\(T\\) are orthogonal as their dot product is 0\n\nGiven that \\(cos^2(\\theta) + sin^2(\\theta) = 1\\), \\(cos(\\theta)sin(\\theta) - sin(\\theta)cos(\\theta) = 0\\)\n\n\nTo use the transofmration matrix, set \\(\\theta\\) to some angle for clockwise rotation and multiply matrix \\(T\\) by a \\(2 \\times N\\) matrix of geometric points where each column are \\((X, Y) corrdinates\\) ## Image feature detection\n\nIs an extension of time series filtering.\n\ndesign a Kernel and then create a time series of dot products between the kernel and the overlapping segments of the signal.\nHere we use 2D instead of 1D kernels and create a new image by computing the dot products between the kernel and overlapping windows of the image.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  },
  {
    "objectID": "ch_8.html",
    "href": "ch_8.html",
    "title": "8  Matrix Inverse",
    "section": "",
    "text": "8.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#key-points",
    "href": "ch_8.html#key-points",
    "title": "8  Matrix Inverse",
    "section": "",
    "text": "Matrix inversion is a way to decompose a matrix or split a matrix into multiple components.\nIt is numerically unstable because the calculation requires a lot of moving parts and often requires the calculation of the determinant which produces floats that can cause deviation from the true solution due to floats.\nThe conditions required for full matrix inversion are often uncommon to the applications we are using and so the less ideal solutions are often needed. In many applications, the matrices that we have are not even fully invertable and require a pseudoinverse which produces a transformation matrix that approximate, but does not equate to, the identity matrix (what we often need).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#matrix-inverse",
    "href": "ch_8.html#matrix-inverse",
    "title": "8  Matrix Inverse",
    "section": "8.2 Matrix inverse",
    "text": "8.2 Matrix inverse\n\nAny matrix that has an inverse, that inverse is unique to that matrix.\nDenoted as: $A^{-1}.\n\\(A^{-1}A = I\\)\n\nThe inverse matrix multiplied by itself produces the identity matrix.\nIt allows you to “cancel” out a matrix.\n\nWe often want to do this so that we can solve common problems like:\n\n\\(Ax=b\\) where we know \\(A\\) and \\(b\\) but want to find \\(x\\) \\[\n  \\begin{aligned}\n  Ax = b \\\\\n  A^{-1}Ax = A^{-1}b \\\\\n  Ix = A^{-1}b \\\\\n  x = A^{-1}b\n  \\end{aligned}\n  \\]\n\nIt is really complicated to compute the inverrse of a matrix and it does not always work. Not all matrices can be inverted.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#types-of-inverses-and-their-conditions",
    "href": "ch_8.html#types-of-inverses-and-their-conditions",
    "title": "8  Matrix Inverse",
    "section": "8.3 Types of inverses and their conditions",
    "text": "8.3 Types of inverses and their conditions\n\nFull inverse\n\n\\(A^{-1}A = AA^{-1} = I\\)\nConditions:\n\n\\(A\\) needs to be square.\n\\(A\\) needs to be full rank.\n\n\nOne-sided inverse\n\nCan transform a full-rank rectangular matrices into \\(I\\).\nIf \\(T\\) is a tall-matrix, then you can do the left-inverse to produce I, \\(LT=I\\), but \\(TL \\neq I\\)\nIf \\(W\\) is a wide-matrix, then you can do the right-inverse to produce I, \\(WR=I\\), but \\(RW \\neq I\\)\n\nPseudoinverse\n\nEvery matrix has one, does not require full-rank like the other inverses do.\nIf a matrix is square full-rank, the pseudoinverse is equal to the full inverse.\nIf it is a tall full-rank matrix, the pseudoinverse is equal to the left inverse.\nIf it is a wide full-rank matrix, the pseudoinverse is equal to the right inverse.\nSingular matrices have a pseudoinverse but it only converts it into another matrix that approximates but does not equal the identity matrix.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#computing-the-inverse",
    "href": "ch_8.html#computing-the-inverse",
    "title": "8  Matrix Inverse",
    "section": "8.4 Computing the inverse",
    "text": "8.4 Computing the inverse\n\nTo compute a scalar inverse we can: \\[\n\\begin{aligned}\n  \\lambda = 3 \\\\\n  \\lambda^{-1} = \\frac{1}{3} \\\\\n  \\lambda \\lambda^{-1} = 3 \\times \\frac{1}{3} = 1\n\\end{aligned}\n\\]\nWith matrices, it isn’t that easy.\n\nWith a \\(2\\times 2\\) matrix:\n\nSwap diagonal elements, multiply off-diagonal elements by \\(-1\\) and divide by the determinant. \\[\n  \\begin{aligned}\n  A = \\begin{bmatrix}\n      a&b\\\\\n      c&d\\\\\n  \\end{bmatrix}\\\\\n  A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}\n      d&-b\\\\\n      -c&a\\\\\n  \\end{bmatrix}\\\\\n  AA^{-1} = \\begin{bmatrix}\n      a&b\\\\\n      c&d\\\\\n  \\end{bmatrix}\\frac{1}{ad-bc}\\begin{bmatrix}\n      d&-b\\\\\n      -c&a\\\\\n  \\end{bmatrix}\\\\\n  = \\frac{1}{ad-bc}\\begin{bmatrix}\n      ad-bc&0\\\\\n      0&ad-bc\\\\\n  \\end{bmatrix}\\\\\n  = \\begin{bmatrix}\n      1&0\\\\\n      0&1\\\\\n  \\end{bmatrix}\n  \\end{aligned}\n  \\]\n\n\nExample in Julia:\n\n\n# Define a matrix.\nA = [[1,2] [4,7]]\nprintln(\"A = $A\")\n# Invert A.\nA_inv = LinearAlgebra.inv(A)\nprintln(\"A^-1 = $A_inv\")\n# Confirm A^1A equals I\nI = A_inv * A\nprintln(\"A^-1A = $I\")\n\nA = [1 4; 2 7]\nA^-1 = [-7.0 4.0; 2.0 -1.0]\nA^-1A = [1.0 0.0; 0.0 1.0]\n\n\n\nTo invert any square full-rank matrix, here is the algorithm:\n\nRequires four intermediate matrices\n\nMinors: comprises determinants of submatrices. Produced by taking the determinant of the submatrix when you exclude the \\(i\\)th row and \\(j\\)th column.\nGrid matrix: Computed as \\(g_{i,j} = -1^{i+j}\\) and produces a checkerboard.\nCofactors matrix: The Hadamard multiplication of the minors matrix with the grid matrix.\nAdjugate matrix: Transpose the cofactors matrix then scalar multiply it by the inverse of the determinant of the original matrix.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#one-sided-inverses",
    "href": "ch_8.html#one-sided-inverses",
    "title": "8  Matrix Inverse",
    "section": "8.5 One-sided inverses",
    "text": "8.5 One-sided inverses\n\nWe need to find some matrix \\(L\\) so that \\(LT=I\\).\nTo do this, we can first make \\(T\\) square by doing: \\(T^TT\\). From there, we can invert that resulting square matrix which is now \\(L\\). So, a left inverse is: \\[\n(T^T)^{-1}(T^TT) = I \\\\\nL = (T^TT)^{-1}T\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#moore-penrose-pseudoinverses-for-singular-matrices",
    "href": "ch_8.html#moore-penrose-pseudoinverses-for-singular-matrices",
    "title": "8  Matrix Inverse",
    "section": "8.6 Moore-Penrose Pseudoinverses for singular matrices",
    "text": "8.6 Moore-Penrose Pseudoinverses for singular matrices\n\nPseudoinverses produce transformation matrices.\n\nThese transformation matrices are not \\(I\\) as the other inverses produce. However, they get somewhat close to \\(I\\).\n\nPseudoinverses are not unique solutions. You can have multiple transformation matrices for one singular matrix. This is why inverses can be unstable.\nOne type is the Moore-Penrose Pseudoinverse\n\nExtremely common.\nDenoted as \\(A^\\dagger\\)\n\nTo compute them, you need to take the SVD (singular value decomposition, have notes on this later), invert the nonzero singular values and then reconstruct the matrix by multiplying \\(U\\sum^TV^T\\)%",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_8.html#other-reasons-inverses-are-numerically-unstable",
    "href": "ch_8.html#other-reasons-inverses-are-numerically-unstable",
    "title": "8  Matrix Inverse",
    "section": "8.7 Other reasons inverses are numerically unstable",
    "text": "8.7 Other reasons inverses are numerically unstable\n\nWe do not always have full-rank matrices or matrices that are square. As we can have multiple solutions for pseudoinverses, this is one source of instability.\nThere are a lot of floating point operations, e.g., we have to calculate determinants. Because of this, there are a lot of places where rounding comes into play and this can create departures from the true results.\n\nThe larger our matrix, the more determinants we have to calculate. So the larger our matrix gets, the larger the deviation can become.\n\nInstead of inverting matrices, we can use matrix decomposition to decompose a matrix into the product of another one. Decomposition is much more numerically stable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "ch_9.html",
    "href": "ch_9.html",
    "title": "9  Orthogonal matrices and QR decomposition.",
    "section": "",
    "text": "9.1 Orthogonal matrices",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Orthogonal matrices and QR decomposition.</span>"
    ]
  },
  {
    "objectID": "ch_9.html#orthogonal-matrices",
    "href": "ch_9.html#orthogonal-matrices",
    "title": "9  Orthogonal matrices and QR decomposition.",
    "section": "",
    "text": "A type of matrix that is important to many applications of statistics.\n\nUsed in QR decomposition, eigendecomposition, singular value decomposition, etc.\n\nDenoted as \\(Q\\)\nPosses two properties: \\[\n  \\langle q_i, q_j \\rangle = \\begin{cases}\n      0\\text{,}&\\text{if }i\\neq j\\\\\n      1\\text{,}&\\text{if }i = j\\\\\n  \\end{cases}\n  \\]\n\nAll columns are pair-wise orthogonal to each other (linearly independent).\nThe norm of each column is exactly 1.\n\nA key feature of an orthogonal matrix is: \\[\nQ^TQ = I\n\\]\n\nThis is the same as a matrix inverse. This means, that the inverse of an orthogonal matrix is its transpose. Since the inverse is unstable and tedius to compute, we can leverage the transpose of the orthogonal matrix to get around having to deal with matrix inverses. This means that we should be decomposing our matrices to find the orthogonal matrix of any matrix we have!\n\nExamples of orthogonal matrices\n\nIdentity matrix\nPure rotation matrices\nPermutation matrices (notes on LU decomposition in next chapter)\n\nTo compute Orthogonal matrices from any nonorthogonal matrix, we can use QR decomposition, which is a type of the Gram-Schmidt procedure.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Orthogonal matrices and QR decomposition.</span>"
    ]
  },
  {
    "objectID": "ch_9.html#gram-schmidt-procedure",
    "href": "ch_9.html#gram-schmidt-procedure",
    "title": "9  Orthogonal matrices and QR decomposition.",
    "section": "9.2 Gram-Schmidt procedure",
    "text": "9.2 Gram-Schmidt procedure\n\nA procedure that lets us transform nonorthogonal matrices into an orthogonal matrix.\nIt is not used very often in practice and has been expanded upon with methods like QR decomposition.\n\nThis is because it requires a lot of dividing and multiplying matrices which can be numerically unstable.\n\nComputing an orthogonal matrix with Gram-Schmidt from a non-orthogonal matrix, \\(V\\) with \\(v_1\\) through \\(v_n\\) columns.\n\nFor all \\(v_n\\) in \\(V\\), we use orthogonal vector decomposition. In otherwords, compute the component of \\(v_k\\) that is perpendicular to \\(q_{k-1}, q_{k-2}\\)\nNormalize \\(v^*_k\\) to unit length.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Orthogonal matrices and QR decomposition.</span>"
    ]
  },
  {
    "objectID": "ch_9.html#qr-decomposition",
    "href": "ch_9.html#qr-decomposition",
    "title": "9  Orthogonal matrices and QR decomposition.",
    "section": "9.3 QR decomposition",
    "text": "9.3 QR decomposition\n\n\n\n\n\n\n“Definition\n\n\n\n\nEconomy QR decomposition:\n\nWe have a tall matrix and it produces a Tall Q.\n\nFull QR decomposition\n\nWe have have a tall matrix, but we can produce a square Q.\n\n\n\n\n\nGram-Schmidt can be a bit unstable and can be really inefficient as we have to orthogonalize over all pairwise combinations of \\(v_n, v_{n+1}\\).\nThis is an alternative way to transform a matrix into \\(Q\\). \\[\nA = QR\n\\]\nIn the above equation, we can split \\(A\\) into two pieces, \\(Q\\) the orthogonal matrix and \\(R\\). \\(Q\\) of course loses information not related to the orthogonal part of \\(A\\), but we want to keep that information somewhere and so we put it in \\(R\\).\nHow do we get \\(R\\)? \\[\n\\begin{aligned}\n  A = QR\\\\\n  Q^TA = Q^TQR\\\\\n  Q^TA = IR \\\\\n  Q^TA = R\\\\\n\\end{aligned}\n\\]\nThis is really simple. We can use the nice property of \\(Q\\) to find \\(R\\) without needing to do any matrix inversion.\n\\(Q\\) will always be the maximum possible rank. The rank of \\(R\\) is the same as the rank of \\(A\\)\n\nThe difference in rank for \\(A\\) and \\(Q\\) is that the rank of \\(Q\\) spans all of \\(\\mathbb{R}^M\\) even if the column space of \\(A\\) is a subspace of \\(R^M\\). A key idea to why SVD is useful.\n\nOne property of \\(R\\) is that it is always upper triangular.\n\n\\(R\\) comes from \\(Q^TA = R\\)\nThe lower triangle of a product matrix are the dot products between later rows of the left matrix and earlier columns of the right matrix.\nThe rows of \\(Q^T\\) are the columns of \\(Q\\).\nThis means that the lower triangle of \\(R\\) comes from pairs of vectors that are orthogonalized whereas earlier columns in \\(Q\\) are not orthogonalized to later columns of \\(A\\) so their dot products are not zero.\n\nQR decomposition is not unique for all matrix sizes and ranks. You can get multiple \\(Q\\)’s and \\(R\\)’s depending on the size of \\(A\\). We can make it unique if we have extra constraints like positive values on the diagonals of \\(R\\).\n\n\n9.3.1 QR and inverses\n\nWay more stable way to compute a matrix inverse:\n\nUsing LIVE EVIL: \\[\n  \\begin{aligned}\n  A = QR \\\\\n  A^{-1} = (QR)^{-1} \\\\\n  A^{-1} = R^{-1}Q^{-1}\\\\\n  A^{-1} = R^{-1}Q^T\\\\\n  \\end{aligned}\n  \\]\nQ is stable due to the Householder reflection algo (the transposition of \\(Q\\) is equal to its inverse)\nR is stable because inverting a right-triangle uses back substitution which is really stable numerically.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Orthogonal matrices and QR decomposition.</span>"
    ]
  },
  {
    "objectID": "ch_10.html",
    "href": "ch_10.html",
    "title": "10  Row reduction and LU Decomposition",
    "section": "",
    "text": "10.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#key-points",
    "href": "ch_10.html#key-points",
    "title": "10  Row reduction and LU Decomposition",
    "section": "",
    "text": "When working with a system of matrices, we need to apply manipulations to both sides and in the same order – they are noncommunicative.\nRow reduction allows us to take the rows of matrix \\(A\\) and scalar multiply then add them until we have an upper-triangular matrix, \\(U\\). The linear transformations can be recorded in our transformation matrix,, \\(L\\), which left multiplies \\(A\\). That is \\(L^{-1}A = U\\)\nWe use row reduction in LU decomposition. The result is unique so long as there is a square and full rank matrix.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#systems-of-equations",
    "href": "ch_10.html#systems-of-equations",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.2 Systems of equations",
    "text": "10.2 Systems of equations\n\nLU decomposition is essentially solving a series of systems of equations. Here’s some examples of how to algebraically solve them.\n\n\\[\n\\begin{aligned}\n    x = 4-y\\\\\n    y = \\frac{x}{2} + 2\\\\\n    \\\\\n    x = 4 - (\\frac{1}{2}x + 2) \\\\\n    \\frac{3}{2}x = 4 - 2\\\\\n    x = \\frac{4}{3}\\\\\n    y = \\frac{4}{3}^{-2} + 2 \\\\\n    y = \\frac{2}{3} + \\frac{6}{3} = \\frac{8}{3}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#matrices-as-systems",
    "href": "ch_10.html#matrices-as-systems",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.3 Matrices as systems",
    "text": "10.3 Matrices as systems\n\nTake the following system: \\[\n\\begin{aligned}\n  x + y = 4 \\\\\n  -\\frac{x}{2} + y = 4\\\\\n\\end{aligned}\n\\]\n\nIn matrix form, it would be re-expressed as: \\[\n\\begin{bmatrix}\n  1&1\\\\\n  -\\frac{1}{2}&1\n\\end{bmatrix}\\begin{bmatrix}\n  x\\\\\n  y\n\\end{bmatrix} = \\begin{bmatrix}\n  4\\\\\n  2\n\\end{bmatrix}\n\\]\n\nYou essentially create a matrix of scalars, then a column vector of variables which is equivalent to the vector of scalars.\nThis is what is represented when we have \\(Ax = b\\).\n\n\\(A\\) is our matrix of scalars called our coefficients.\n\\(x\\) is our vector of variables.\n\\(b\\) is our vector of constants.\n\n\n\n10.3.1 Working with/solving matrices as systems\n\nWhat you do to one-side, you must do to the other.\nThe order matters. If you pre-multiply a matrix/vector on one-side you have to do it on the other.\n\nE.g.\n\nThe following is valid. \\[\n  \\begin{aligned}\n  AX = B\\\\\n  CAX = CB\\\\\n  \\end{aligned}\n  \\]\nThe following is invalid. \\[\n  \\begin{aligned}\n  AX = B\\\\\n  AXC = CB\\\\\n  \\end{aligned}\n  \\]\n\n\n\n\n\n10.3.2 Row reduction\n\n\n\n\n\n\nDefinitions\n\n\n\nEchelon form matrix - The resulting upper triangular matrix from row reduction - The leftmost nonzero number in a row (called the pivote) is to the right of the pivote of the rows above. - Any rows of all zerios are below rows containing nonzeros.\n\n\n\nThe building block for LU decomposition.\nIt is the way to solve systems of equations by hand and is very well-known in linear algebra. However, you often do not actually do it by hand. It is useful to know about as it leads to LU decomposition.\nRow reduction\n\nYou iteratively apply two operations to rows of a matrix:\n\nscalar multiplication\nscalar addition\n\nIt is the matrix system way of adding equations to other equations within a system (solving a system with substitution).\nThe goal of row reduction: transform a dense matrix into a upper-triangular matrix via row manipulations that we implement by premultiplying a transformation matrix.\n\nExample: \\[\n  \\begin{bmatrix}\n      2&3\\\\\n      -2&2\\\\\n  \\end{bmatrix} \\xrightarrow{R_1 + R2} \\begin{bmatrix}\n      2&3\\\\\n      0&5\\\\\n  \\end{bmatrix}\n  \\]\n\nWe get this upper triangular matrix by adding the first row to the second row.\nThis upper triangular matrix is called the echelon form of the matrix\n\nThough the echelon form is different than the original matrix, it is connected to the original matrix through a linear combination. We can represent this as: \\[\n\\begin{bmatrix}\n  1&0\\\\\n  1&1\\\\\n\\end{bmatrix}\\begin{bmatrix}\n  2&3\\\\\n  -2&2\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n  2&3\\\\\n  0&5\\\\\n\\end{bmatrix}\n\\]\n\nThis is often represented as \\(L^{-1}A = U\\) where \\(L^{-1}\\) represents the linear transformations that we have implemented as a way to keep track of what manipulations we have done with row reduction.\n\nThe problem with row reduction is that it is tedius and that the echelon form of a matrix is not unique.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#gaussian-elimination",
    "href": "ch_10.html#gaussian-elimination",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.4 Gaussian Elimination",
    "text": "10.4 Gaussian Elimination\n\nYou do not have to do matrix inversion to solve a matrix equation.\nWe can use Gaussian Elimination.\nHow?\n\nAugment the matrix of coefficeints by the vector of constants, row reduce to echelon form, then use back substitution to solve for each variable.\n\nExample:\n\n\\[\n\\begin{aligned}\n    x = 4-y\\\\\n    y = \\frac{x}{2} + 2\\\\\n    \\text{Put in matrix form:}\\\\\n    \\begin{bmatrix}\n        1&1\\\\\n        -\\frac{1}{2}&1\\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        x\\\\\n        y\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        4\\\\\n        2\\\\\n    \\end{bmatrix}\\\\\n    \\begin{bmatrix}\n        1&1\\\\\n        -\\frac{1}{2}&1\\\\\n    \\end{bmatrix} \\sqcup \\begin{bmatrix}\n        4\\\\\n        2\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        1&1&4\\\\\n        -\\frac{1}{2}&1&2\\\\\n    \\end{bmatrix}\\\\\n    \\text{Row reduce:}\\\\\n    \\begin{bmatrix}\n        1&1&4\\\\\n        -\\frac{1}{2}&1&2\\\\\n    \\end{bmatrix} \\xrightarrow{\\frac{1}{2}R_1 + R_2} \\begin{bmatrix}\n        1&1&4\\\\\n        0&\\frac{3}{2}&4\\\\\n    \\end{bmatrix}\\\\\n    \\text{Back substitute:}\\\\\n    x + y = 4\\\\\n    \\frac{3}{2}y = 4 \\Rightarrow y =  \\frac{8}{3} \\\\\n    x + \\frac{8}{3} = 4 \\Rightarrow x = \\frac{4}{3} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#gauss-jordan-elimination",
    "href": "ch_10.html#gauss-jordan-elimination",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.5 Gauss-Jordan Elimination",
    "text": "10.5 Gauss-Jordan Elimination\n\nWe need to keep reducing so that the left-most nonzero elements in the matrix, our pivots, are 1s.\nAn alternate to back substitution is that we can divide each row by it’s pivot. So, for example, we had been here: \\[\n\\begin{bmatrix}\n  1&1&4\\\\\n  0&1&\\frac{8}{3}\n\\end{bmatrix}\n\\]\nWe can use Gauss-Jordan elimination as a trick to continue row reducing upward to get rid of elements abov each pivot. \\[\n\\begin{bmatrix}\n  1&1&4\\\\\n  0&1&\\frac{8}{3}\n\\end{bmatrix} \\xrightarrow{-R_2 + R_1} \\begin{bmatrix}\n  (-0+1)&(-1+1)&(-\\frac{8}{3}+\\frac{12}{3})\\\\\n  0&1&\\frac{8}{3}\\\\\n\\end{bmatrix}=\\begin{bmatrix}\n  1&0&\\frac{4}{3}\\\\\n  0&1&\\frac{8}{3}\\\\\n\\end{bmatrix}\n\\]\nLike back substitution, this produces a reduced row echelon form, RREF.\nRemember Gauss-Jordan Elimination is just the shortcut that we can use to solve a matrix system rather than having to go back to solving a system of equations. ## Matrix inverse via Gauss-Jordan Eliminations:\nSay that we have this: \\[\n\\begin{bmatrix}\n  a&b\\\\\n  c&d\\\\\n\\end{bmatrix}\\begin{bmatrix}\n  x_1\\\\\n  y_1\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n  1\\\\\n  0\n\\end{bmatrix}\n\\]\nThe constants vector is the first column vector for a \\(2\\times 2\\) \\(I\\).\nWhat this implies is that applying RREF to a square full-rank matrix augmented by the first column of the identity matrix will show the linear transformation that generates the matrix into the first column of the identity matrix. This then means that the vector \\([x_1 y_1]^T\\) is the first column of the matrix inverse. We can then repeat this process but for the second column of the inverse.\nBasically, this is a mechanism to find the matrix inverse without computing determinants. The draw back is that row redeuction involves a lot of division which can lead to error from lack of precision. Though it is more numerically stable than the inverse. A matrix that is close to singular or one that has a high condition number will be difficult to inverse no matter what!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#lu-decomposition",
    "href": "ch_10.html#lu-decomposition",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.6 LU Decomposition",
    "text": "10.6 LU Decomposition\n\nBuilds on row reduction by adding constraints. Doing this allows us to identify a unique echelon form, unlike what we get with row reduction alone. This is the RREF and \\(U\\).\nThe goal with this procedure is to decompose a given matrix into the product of two triangular matrices.\nIt relies upon row reduction.\nThe echelon form is not necessarily unique with LU decomposition.\n\nHowever, by requiring that the diagonals of \\(L\\) are 1, this makes sure that the LU decomposition is unique for a full-rank square matrix. \\[\nA = LU\n\\]\n\nA numerical example:\n\nWe have the following matrix: \\[\n\\begin{bmatrix}\n  2&2&4\\\\\n  1&0&3\\\\\n  2&1&2\\\\\n\\end{bmatrix}\n\\]\nWe can do \\(L^{-1}A=U\\) with row reduction like before to get the transformation matrix that transforms \\(A\\) into the echelon form, \\(U\\) (which is an upper-triangle).\n\n\n\\[\n\\begin{bmatrix}\n    2&2&4\\\\\n    1&0&3\\\\\n    2&1&2\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n    1&0&0\\\\\n    \\frac{1}{2}&1&0\\\\\n    1&1&1\\\\\n\\end{bmatrix}\\begin{bmatrix}\n    2&2&4\\\\\n    0&-1&1\\\\\n    0&0&-3\\\\\n\\end{bmatrix}\n\\]\n\n# Define matrix.\nA = [[2,1,2] [2,0,1] [4,3,2]]\nprintln(\"A = $A\")\n# Perform LU decomposition.\nA_LU_all = LinearAlgebra.lu(A)\nL = A_LU_all.L\nU = A_LU_all.U\nprintln(\"L = $L\")\nprintln(\"U = $U\")\n\nA = [2 2 4; 1 0 3; 2 1 2]\nL = [1.0 0.0 0.0; 0.5 1.0 0.0; 1.0 1.0 1.0]\nU = [2.0 2.0 4.0; 0.0 -1.0 1.0; 0.0 0.0 -3.0]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_10.html#row-swapping-with-permutation-matrices",
    "href": "ch_10.html#row-swapping-with-permutation-matrices",
    "title": "10  Row reduction and LU Decomposition",
    "section": "10.7 Row swapping with Permutation matrices",
    "text": "10.7 Row swapping with Permutation matrices\n\nSome matrices are hard to transform into upper-triangular form.\nWe can swap rows to help put it in echelon form and then perform row reduction from there. This is done with a permutation matrix.\nNumerical example:\n\nOriginal matrix \\[\n  \\begin{bmatrix}\n  3&2&1\\\\\n  0&5&5\\\\\n  0&7&2\\\\\n  \\end{bmatrix}\n  \\]\nRow swapping with the permutation matrix, \\(P\\): \\[\n  \\begin{bmatrix}\n  1&0&0\\\\\n  0&1&1\\\\\n  0&1&0\\\\\n  \\end{bmatrix}\\begin{bmatrix}\n  3&2&1\\\\\n  0&5&5\\\\\n  0&7&2\\\\\n  \\end{bmatrix} = \\begin{bmatrix}\n  3&2&1\\\\\n  0&7&2\\\\\n  0&0&5\\\\\n  \\end{bmatrix}\n  \\]\n\nOften this is required with LU decomposition. So technically, LU decomposition is denoted as: \\[\nPA = LU\\\\\nA = P^TLU\n\\]\nPermutation matrices are orthogonal. Which means \\(P^{-1} = P^T\\) and \\(P^TP = I\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Row reduction and LU Decomposition</span>"
    ]
  },
  {
    "objectID": "ch_11.html",
    "href": "ch_11.html",
    "title": "11  General Linear Models (GLM) and Least Squares",
    "section": "",
    "text": "11.1 GLMs\n\\[\n\\begin{aligned}\n    X\\beta = y\\\\\n    (X^TX)^{-1}X^TB = (X^TX)^{-1}X^Ty\\\\\n    \\beta = (X^TX)^{-1}X^Ty\\\\\n    \\text{OR}\\\\\n    X^T\\epsilon = 0\\\\\n    X^T(y-X\\beta) = 0\\\\\n    X^Ty - X^TX\\beta = 0\\\\\n    X^TX\\beta = X^Ty\\\\\n    \\beta = (X^TX)^{-1}X^Ty\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Linear Models (GLM) and Least Squares</span>"
    ]
  },
  {
    "objectID": "ch_11.html#glms",
    "href": "ch_11.html#glms",
    "title": "11  General Linear Models (GLM) and Least Squares",
    "section": "",
    "text": "\\(\\beta\\) refers to a vector of coefficients.\n\\(X\\) refers to the matrix of predictors.\n\\(y\\) refers to the vector of outcome variable data.\nThis equation is exactly solvable when \\(y\\) is in the column space of the design matrix \\(X\\). This rarely happens, however, as we rarely have situations where the model accounts for 100% of the vairance in the outcome.\nThis means we need to modify the GLM equation: \\[\n\\begin{aligned}\n  X\\beta = y + \\epsilon \\\\\n  \\epsilon = X\\beta - y\\\\\n\\end{aligned}\n\\]\n\\(\\epsilon\\) is some residual vector that we add so that \\(y\\) fits within the column space of the design matrix.\n\\(\\hat{y}\\) is then \\(\\hat{y} = y + \\epsilon\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Linear Models (GLM) and Least Squares</span>"
    ]
  },
  {
    "objectID": "ch_11.html#least-squares-with-qr",
    "href": "ch_11.html#least-squares-with-qr",
    "title": "11  General Linear Models (GLM) and Least Squares",
    "section": "11.2 Least squares with QR",
    "text": "11.2 Least squares with QR\n\nThe previous method uses the left-inverse method. It can have numerical instability, however, because it requires computing the matrix inverse.\n\\(X^TX\\) can introduce difficulties as it influences the norm and condition number (see chapter 14).\n\na high condition number can be numerically unstable and will only become less stable when squared.\n\nWith QR decomposition, we can solve this numerical instability issue: \\[\n\\begin{aligned}\n  X\\beta = y\\\\\n  QR\\beta = y\\\\\n  R\\beta = Q^Ty\\\\\n  \\beta = R^{-1}Q^Ty\n\\end{aligned}\n\\]\n\nNote some of the algebra above.\n\nWe can decompose \\(X\\) into the orthogonal, \\(Q\\), and the non-orthogonal matrix, \\(R\\).\nThen we can multiply both sides of the equation by the transpose of the orthogonal matrix, \\(Q^T\\). Since \\(Q^TQ = I\\), \\(Q\\) on the left-side cancels out.\nWe can then multiply \\(R^{-1}\\) to both sides and since \\(R^{-1}R = I\\) that cancels \\(R\\) out on the left-hand side.\n\nSome other things that happen below:\n\nWe can use row swap with permutation matrices to increase numerical stability and we do not even need to invert \\(R\\). So, we can get the solution with back substitution.\n\n\n\n\n# Define the data.\ndata = DataFrame(x = [1,2,3,4,5], y = [2,4,5,4,5])\nX = hcat(ones(length(data.x)), data.x)\ny = data.y\n# Do it the manual way.\nBeta = LinearAlgebra.inv(X'X) * X' * y\n# Do it with GLM package\nmodel = GLM.lm(@formula(y ~ x), data)\n# Compare results.\nprintln(\"Intercepts are approximately equal? $(Beta[1]) ≈ $(coef(model)[1])\")\nprintln(\"Coefficients are approximately equal? $(Beta[2]) ≈ $(coef(model)[2])\")\n\nIntercepts are approximately equal? 2.2000000000000006 ≈ 2.1999999999999993\nCoefficients are approximately equal? 0.6000000000000004 ≈ 0.6000000000000003",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>General Linear Models (GLM) and Least Squares</span>"
    ]
  },
  {
    "objectID": "ch_12.html",
    "href": "ch_12.html",
    "title": "12  Least Squares applications",
    "section": "",
    "text": "Multicollinearity occurs when the design matrix is singular. We cannot have a left-inverse of the design matrix when it is singular.\nRegularization is essentially just a way to improve numerical stability, transform singular matrices to full rank or improving the generalizability by reducing overfitting.\n\nThere are several types of regularization such as Ridge (L2), Lasso (L1), Tikhonov and shrinkage.\nMost work by shifting the design matrix by some amount.\n\n\\(A + \\lambda I\\)\n\nWith L2, we shift it by the Frobenius norm.\n\n\\(\\beta = (X^TX + \\lambda \\parallel X\\parallel_F^2I)^{-1}X^Ty\\)\n\n\nPolynomial regression\n\nIt is the same as regular regression but we raise the values of the independent variables to higher powers.\nWe can still use the left-inverse of the design matrix (or more numerically stable decompositions). We just change the design matrix.\n\nGrid search for model parameters\n\nThe left-inverse in least squares allows us to fit a model to data. This only works for linear model fitting, however.\nGrid search samples the parameter space, computing the model fit to the data with each parameter value and then selects the parameter value that provides the best model fit.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Least Squares applications</span>"
    ]
  },
  {
    "objectID": "ch_13.html",
    "href": "ch_13.html",
    "title": "13  Eigendecomposition",
    "section": "",
    "text": "13.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#key-points",
    "href": "ch_13.html#key-points",
    "title": "13  Eigendecomposition",
    "section": "",
    "text": "Identifies M scalar/vector pairs of a square, \\(M\\times M\\) matrix.\nThese pairs of eigenvalues/eigenvector reflect the directions in the matrix.\nTo find eigenvalues, you assume the matrix is shifted by some unknown scalar, \\(\\lambda\\), assuming the matrix is singular, and setting the determinant to zero (the characteristic polynomial), then solving for all \\(\\lambda\\)s.\nTo find the eigenvector, you find the basis vector for the null space of the \\(\\lambda\\)-shifted matrix\nTo diagonalize a matrix, you are representing it as \\(V^{-1}\\Lambda V\\), where \\(V\\) is a matrix of eigenvectors in the columns and \\(\\Lambda\\) is a diagonal matrix with eiigenvalues in the diagonal elements.\nSymmetric matrices are really helpful and have nice properties in eigendecomposition: all eigenvectors are pair-wise orthogonal. Meaning the matrix of eigenvectors is an orthogonal matrix which allows us to invert it by transposing it.\nThe definiteness of a matrix indicates the signs of the eigenvalues.\nWhen we multiply a matrix by its transpose, it is always positive semidefinite.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#what-is-eigendecomposition",
    "href": "ch_13.html#what-is-eigendecomposition",
    "title": "13  Eigendecomposition",
    "section": "13.2 What is eigendecomposition?",
    "text": "13.2 What is eigendecomposition?\n\nEigendecomposition tells us how much a combination of individuals covary.\n\nOne way to think about it: in a bivariate regression, we can see how an individual variable explains the system. However, when we have multiple variables, we can see their individual contributions, but not necessarily how they operate in the global system.\n\nIt is only defined for square matrices.\n\nFor nonsquare matrices, you would use SVD.\n\nApplications\n\nIt is commonly used in PCA and other feature detection applications.\nAlso used in noise reduction. You identify the eigenvalues and eigenvectors. Assuming that random variance is a small component of a data space, you would ignore the small eigenvalues of the eigenvector and then “project out” by reconstructing the dataset after setting those small eigenvalues to zero.\n\nTo perform eigendecomposition, we need to find the eigenvalues and the resulting eigenvector that produce the nullspace of a given matrix.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#finding-eigenvalues",
    "href": "ch_13.html#finding-eigenvalues",
    "title": "13  Eigendecomposition",
    "section": "13.3 Finding eigenvalues",
    "text": "13.3 Finding eigenvalues\n\nThe equation to find an eigenvalue. \\[\nAv = \\lambda v\n\\]\n\\(A\\) = a square matrix.\n\\(v\\) = a vector of eigen values (called the eigenvector)\n\\(\\lambda\\) = a scalar\nWhat this equation implies is that the effect of the matrix on the vector as the same as the effect of the scalar on the same vector.\nNow we do some algebra to identify particular eigenvalues.\n\n\\[\n\\begin{aligned}\n    Av = \\lambda v\\\\\n    Av-\\lambda v = 0\\\\\n    (A-\\lambda I)v = 0\\\\\n\\end{aligned}\n\\] - This implies that the eigenvector is in the null space of the matrix when it is shifted by its eigenvalue. This means that the matrix shifted b y its eigenvalue is singular because it has a null space. - This also means that the determinant is zero as it is a singular matrix. \\[\n|A - \\lambda I| = 0\n\\] - With this, we can then solve for \\(\\lambda\\) \\[\n\\begin{aligned}\n        \\left\\vert \\begin{bmatrix}\n            a&b\\\\\n            c&d\\\\\n        \\end{bmatrix} - \\lambda \\begin{bmatrix}\n            1&0\\\\\n            0&1\\\\\n        \\end{bmatrix} \\right\\vert = 0 \\\\\n        \\begin{vmatrix}\n            a-\\lambda & b\\\\\n            c&d-\\lambda\\\\\n        \\end{vmatrix} = 0 \\\\\n        (a-\\lambda)(d-\\lambda) - bc = 0 \\\\\n        \\lambda^2 - (a+d)\\lambda + (ad-bc) = 0\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#finding-eigenvectors",
    "href": "ch_13.html#finding-eigenvectors",
    "title": "13  Eigendecomposition",
    "section": "13.4 Finding eigenvectors",
    "text": "13.4 Finding eigenvectors\n\nOnce you have found the eigenvalues, you will want to find the vector \\(v\\) that is in the null space of the matrix shifted by \\(\\lambda\\)\nNumerical example:\n\n\\[\n\\begin{aligned}\n    \\text{Find the eigenvalues:}\n    \\begin{bmatrix}\n        1&2\\\\\n        2&1\\\\\n    \\end{bmatrix} \\Rightarrow \\lambda_1 = 3, \\lambda_2 = -1 \\\\\n    \\text{Find the eigenvector:}\n    \\begin{bmatrix}\n        1-3&2\\\\\n        2&1-3\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        -2&2\\\\\n        2&-2\\\\\n    \\end{bmatrix} \\Rightarrow \\begin{bmatrix}\n        -2&2\\\\\n        2&-2\\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        1\\\\\n        1\\\\\n    \\end{bmatrix} = \\begin{bmatrix}\n        0\\\\\n        0\n    \\end{bmatrix}\n\\end{aligned}\n\\] - We can find these null space vectors using Gauss-Jordan where the coefficients matrix is the \\(\\lambda\\)-shifted matrix and the constants vector is the zeros vector. Often QR decomposition is used instead as it is more stable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#sign-and-scale-indeterminacy",
    "href": "ch_13.html#sign-and-scale-indeterminacy",
    "title": "13  Eigendecomposition",
    "section": "13.5 Sign and scale indeterminacy",
    "text": "13.5 Sign and scale indeterminacy\n\nIf \\(v\\) is an eigenvector of a matrix, then so is \\(\\alpha v\\) for any real-valued \\(\\alpha\\) except when \\(\\alpha = 0\\).\nEigenvectors are used because of their direction rather than their magnitude. So, \\(\\alpha\\) does not really influence the determinacy of the eigenvector.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#special-features-of-symmetric-matrices",
    "href": "ch_13.html#special-features-of-symmetric-matrices",
    "title": "13  Eigendecomposition",
    "section": "13.6 Special features of symmetric matrices",
    "text": "13.6 Special features of symmetric matrices\n\n13.6.1 Orthogonal eigenvectors.\n\nSymmetric matrices produce orthogonal eigenvectors.\n\nThis is significant as all eigenvectors of a matrix are pair-wise orthogonal. Meaning, the dot product between any pair of eigenvectors is zero while the dot product of an eigenvector with itself is nonzero. This makes an eigenvector matrix extremely easy to invert as we can just transpose it.\n\nWe do not constrain eigenvectors to be orthogonal for nonsymmetric vectors however. They will be linearly independent for each distinct eigenvalue.\n\n\n\n13.6.2 Real-valued eigenvalues\n\nSymmetric matrices have real-valued eigenvalues and real-valued eigenvectors.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#eigendecomposition-of-singular-matrices",
    "href": "ch_13.html#eigendecomposition-of-singular-matrices",
    "title": "13  Eigendecomposition",
    "section": "13.7 Eigendecomposition of singular matrices",
    "text": "13.7 Eigendecomposition of singular matrices\n\nWe CAN eigendecompose a singular matrix.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#quadratic-form-definiteness-and-eigenvalues",
    "href": "ch_13.html#quadratic-form-definiteness-and-eigenvalues",
    "title": "13  Eigendecomposition",
    "section": "13.8 Quadratic form, definiteness, and eigenvalues",
    "text": "13.8 Quadratic form, definiteness, and eigenvalues\n\n13.8.1 Quadratic form of a matrix\n\nNotice when we were using the determinant above to compute the eigenvector, we ended up with polynomial \\(\\lambda\\) terms. One feature of matrices is that: \\[\nw^TAw = \\alpha\n\\]\nIf we pre-andpostmultiply a square matrix by the same vector \\(w\\), we get a scalar.\nExample: \\[\n\\begin{bmatrix}\n  x&y\\\\\n\\end{bmatrix}\\begin{bmatrix}\n  2&4\\\\\n  0&3\\\\\n\\end{bmatrix}\\begin{bmatrix}\n  x\\\\\n  y\\\\\n\\end{bmatrix} = 2x^2 + (0+4)xy + 3y^2\n\\]\nWhether that scalar is positive or negative will influence definiteness.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#definiteness",
    "href": "ch_13.html#definiteness",
    "title": "13  Eigendecomposition",
    "section": "13.9 Definiteness",
    "text": "13.9 Definiteness\n\nA characteristic of a square matrix\nDefined by the signs of the eigenvalues of a matrix (equivalent to the signs of the quadratic form results)\nIt influences the invertibility of a matrix as well as whether we can use generalized eigendecomposition.\n\n\n\n\n\n\n\n\n\n\nCategory\nQuadratic form\nEigenvalues\nInvertible\n\n\n\n\nPositive definite\nPositive\n+\nYes\n\n\nPositive semidefinite\nNonnegative\n+ and 0\nNo\n\n\nIndefinite\nPositive and negative\n+ and -\nDepends\n\n\nNegative semidefinite\nNonpositive\n- and 0\nNo\n\n\nNegative definite\nNegative\n-\nYes\n\n\n\n\nAll eigenvalues are positive when the data matrix is full rank\nThere will be at least one zero-valued eigenvalue if a matrix is singular.\n\n\n13.9.1 A^TA is Positive semidefinite\n\nThis is because covariance matrices are semipositive definite. They will have non-negative eigenvalues.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_13.html#generalized-eigendecomposition",
    "href": "ch_13.html#generalized-eigendecomposition",
    "title": "13  Eigendecomposition",
    "section": "13.10 Generalized eigendecomposition",
    "text": "13.10 Generalized eigendecomposition\n\nAlso referred to as simultaneious diagonalization of two matrices\nIt involves replacing the identity matrix with another matrix besides the identity or zeros matrix. \\[\nAv = \\lambdaBv\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "ch_14.html",
    "href": "ch_14.html",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "",
    "text": "14.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#key-points",
    "href": "ch_14.html#key-points",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "",
    "text": "SVD is a generalized form of Eigendecomposition. Unlike Eigendecomposition, we can decompose non-square matrices.\nIt produces three matrices: left singular vectors, singular values, and right singular vectors. The first and last are square and the middle is a diagonal matrix.\nThe Singular vectors communicate the direction of the variance in the data and the singular values communicate the magnitude.\nIf you sum the diagonals of the singular values matrix (where the singular values are encoded), then we get the total variance in the data.\nWe can use the singular values to give us various measures of how much the singular vectors explain the data.\nThe condition number is also very useful. It is the ratio between the largest singular value and the smallest singular value. It can be used to tell us the stability of a given matrix. Diferent interpretations of it that are common are whether it is a well-conditioned matrix and is invertible, whether it is sensitive to pertubrations of noise, and how much it would add noise to a solution given a certain amount of additional noise.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#how-it-works",
    "href": "ch_14.html#how-it-works",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "14.2 How it works",
    "text": "14.2 How it works\n\nThe goal is to decompose a given matrix, \\(A\\), into three matrices: left singular vectors (\\(U\\)), singular values (\\(\\Sigma\\)), and the right singular vectors (\\(V\\)). \\[\nA = U\\Sigma V^T\n\\]\nIt is a generalization of Eigendecomposition that allows nonsquare matrices.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#important-features-of-svd.",
    "href": "ch_14.html#important-features-of-svd.",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "14.3 Important features of SVD.",
    "text": "14.3 Important features of SVD.\n\nBoth \\(U\\) and \\(V\\) are square matrices, does not require \\(A\\) to be square.\nThe matrices of singular vectors are orthogonal: \\(U^TU = I\\) and \\(V^TV = I\\).\nThe first \\(r\\) columns of \\(U\\) provide the orthogonal basis vectors for the column space of the matrix \\(A\\) while the rest of the columns provide orthogonal basis vectors for the left-null space.\nThe first \\(r\\) rows of \\(V^T\\) provide orthogonal basis vectors for the row space and the rest of the rows provide orthogonal basis vectors for the null space.\nThe singular values matrix is diagonal. It is also the same size as \\(A\\).\nAll singular values are nonnegative and have real values.\nThe number of nonzero singular values is equivalent to the matrix rank.\nIt reveals all four subspaces of a matrix: column space, left-null space, row space, right-null space.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#singular-values-uses",
    "href": "ch_14.html#singular-values-uses",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "14.4 Singular values uses",
    "text": "14.4 Singular values uses\n\nThe sum of the signular values is the total amount of variance in a matrix.\nThe singular vectors point and the singular values say how far. Meaning the variance is all contained in the singular values because singular vectors are normalized to unit magnitude, meaning they do not provide any information about magnitude.\nIt is often useful to convert singular values to the percent of total variance explained (PTVE): \\[\n\\tilde{\\sigma_i} = \\frac{100\\sigma_i}{\\Sigma\\sigma}\n\\]\n\nThis is often used in PCA to determine how much of the variance the components explain.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#condition-number",
    "href": "ch_14.html#condition-number",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "14.5 Condition number",
    "text": "14.5 Condition number\n\nIt is the ratio of the largest to the smallest singular value\nDenoted as \\(\\kappa\\) \\[\n\\kappa = \\frac{\\sigma_{max}}{\\sigma_{min}}\n\\]\nIt is often used as a measure of the stability of a matrix when computing the inverse and when using it to solve systems of equations. When we have a noninvertable matrix, the condition number is undetermined as the minimum singular value will be zero.\nIt can also be interpreted as an amplification factor for noise. If we have a large condition number, then it tells us how large the noise would impact the solution to a least squares problem.\nIt also can tell us the sensitivity of a solution to any perturbations in the data matrix. If we have a well-conditioned matrix then we can add more noise and it will have a minimum change to the solution. However, adding a small amount of noise to a ill-conditioned atrix will lead to massively different solutions.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_14.html#svd-and-the-mp-pseudoinverse",
    "href": "ch_14.html#svd-and-the-mp-pseudoinverse",
    "title": "14  Singular Value Decomposition (SVD)",
    "section": "14.6 SVD and the MP Pseudoinverse",
    "text": "14.6 SVD and the MP Pseudoinverse\n\nWe can use the SVD of a matrix to perform the MP pseudoinverse. If we have a square matrix that is invertable, then we can invert \\(\\Sigma\\). \\[\n\\begin{aligned}\n  A^{-1} = (U\\Sigma V^T)^{-1} \\\\\n         = V\\Sigma^{-1}U^{-1} \\\\\n         = V\\Sigma^{-1}U^T\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Singular Value Decomposition (SVD)</span>"
    ]
  },
  {
    "objectID": "ch_15.html",
    "href": "ch_15.html",
    "title": "15  Applications of Eigendecomposition and SVD",
    "section": "",
    "text": "15.1 PCA",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Applications of Eigendecomposition and SVD</span>"
    ]
  },
  {
    "objectID": "ch_15.html#pca",
    "href": "ch_15.html#pca",
    "title": "15  Applications of Eigendecomposition and SVD",
    "section": "",
    "text": "Is used for finding the component (set of variables) that maximally explain the amount of variance in a given matrix.\nBecause of this, PCA wants to find the maximal amount of variance and find the weighted linear combination of variables that maximize it.\nSay that \\(X\\) is our matrix. We want to find the weighted linear combination of column vectors in \\(X\\) that maximize the variance explained. So, we would have a vector of weights, \\(w\\), that provide all of the weights used to linearly combine the variables.\n\nWith PCA, we will want to find the set of weights in \\(w\\) so that \\(Xw\\) provides the maximal variance, \\(\\lambda\\). \\[\n  \\lambda = \\parallel Xw \\parallel^2\n  \\]\nNote: the squared vector norm is the same thing as variance when the data is mean centered.\nThe problem with the existing formulation is that we can specify really large values of \\(w\\) as it will increase the variance by generating larger weights. So, we need to scale the norm of the weighted combination by the norm of the weights so it is fair. \\[\n  \\lambda = \\frac{\\parallel Xw \\parallel^2}{\\parallel w \\parallel^2}\n  \\]\nThis gives us the ratio of two vector norms. Which we can expand to be represented as dotproducts to make it clearer. \\[\n  \\lambda = \\frac{w^TX^TXw}{w^Tw}\n  \\]\nHow do we find \\(w\\) that maximize \\(\\lambda\\)? We can’t just consider a single vector of weights, we need to consider all the possible vectors of weights so we need to consider \\(W\\) rather than just \\(w\\). \\[\n  \\begin{aligned}\n  \\Lambda = (W^TW)^{-1}W^TX^TXW\\\\\n  \\Lambda = W^{-1}W^{-T}W^TX^TXW\\\\\n  \\Lambda = W^{-1}X^TXW\\\\\n  W\\Lambda = W^{-1}X^TXW\\\\\n  W^\\Lambda = X^TXW\\\\\n  \\end{aligned}\n  \\]\nSo, we can just perform Eigendecomposition. The eigenvectors are the weights for the data variables and the eigenvalues are the variances of the data along each direction (each column of \\(W\\)).\nSince covariance matrices are symmetric, the eigenvectors - the principal components - are orthogonal.\n\n\n\n15.1.1 The steps to PCA\n\nCompute the covariance matrix of the data. Each feature needs to be mean-centered before computing the covariance.\nTake the eigendecomposition of the covariance matrix.\nSort the eigenvalues so that the largest are at the top. The eigenvectors should also be sorted.\nCompute the component scores as the weighted combination of all of the variables where the eigenvector provides the weights. The eigenvector with the largest eigenvalue is the most important component.\nConvert the eigenvalues to percent variance explained to help with interpreation.\n\n\n\n15.1.2 PCA with SVD\n\nTwo options.\n\nTake the SVD of the covariance matrix. It is identical to PCA with Eigendecomposition as SVD is just a way to do it with a nonsquare matrix.\nTake the SVD of the data matrix. The right singular vectors will be the eigenvectors of the covariance matrix. The data needs to be meancentered before doing the SVD. The square root of your singular values will be the eigenvalues of the covariance matrix.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Applications of Eigendecomposition and SVD</span>"
    ]
  },
  {
    "objectID": "ch_15.html#linear-discriminant-analysis-lda",
    "href": "ch_15.html#linear-discriminant-analysis-lda",
    "title": "15  Applications of Eigendecomposition and SVD",
    "section": "15.2 Linear Discriminant Analysis (LDA)",
    "text": "15.2 Linear Discriminant Analysis (LDA)\n\nMultivariate classification technique where you try to maximize the space between multiple categories of data. In other words, group data into clusters.\nLDA is meant to find basis vectors in the data space that maximally separate the categories of data.\n\nWe do this by finding a set of weights such that the weighted combo of variables maximally separate the categories. \\[\n  \\lambda = \\frac{\\parallel X_Bw \\parallel^2}{\\parallel X_w w \\parallel^2}\n  \\]\nWe want to find a set of weights \\(w\\) that maximizes the ratio of the variance of data feature \\(X_B\\) to the variance of data feature \\(X_W\\).\nWe can do something like what we did with PCA above doing Eigendecomposition. \\[\n  \\begin{aligned}\n  C_W = X_W^TX_W\\\\\n  C_B = X_B^TX_B\\\\\n  \\Lambda = (W^TC_WW)^{-1}W^TC_BW\\\\\n  \\Lambda = W^{-1}C^{-1}_WW^{-T}W^TC_BW\\\\\n  W\\Lambda = C^{-1}_WC_BW\\\\\n  C_WW\\Lambda = C_BW\\\\\n  \\end{aligned}\n  \\]\nHere we just do generalized eigendecomposition on two covariance matrices. The eigenvectors provide the weights and the generalized eigenvalues are the variance ratios of each ocmponent.\nTo determine \\(X_B\\) and \\(X_W\\), we can use a few different options depending on what kind of LDA you ae doing and that depends on use case. However, in typical LDA, \\(X_B\\) is the between-category covariance while \\(X_W\\) is the within-category covariance. The within-category covariance is the average of the covariances of the data samples within each class. The between-category covariance is the result of creating a new data matrix that contains the feature averages for each class.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Applications of Eigendecomposition and SVD</span>"
    ]
  },
  {
    "objectID": "ch_15.html#low-rank-appoximations-with-svd",
    "href": "ch_15.html#low-rank-appoximations-with-svd",
    "title": "15  Applications of Eigendecomposition and SVD",
    "section": "15.3 Low-rank appoximations with SVD",
    "text": "15.3 Low-rank appoximations with SVD\n\nWe can take the SVD of a data matrix or image, then reconstruct the data matrix using some subset of SVD components. We can do this by setting the selected \\(\\sigma\\)s to zero or create a new SVD matrix that are rectangular with the to-be-rejected vectors and singular values removed.\nThis allows us to compress data into a smaller size.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Applications of Eigendecomposition and SVD</span>"
    ]
  }
]