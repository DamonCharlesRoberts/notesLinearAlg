[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Practical Linear Algebra for Data Science by Mike X Cohen",
    "section": "",
    "text": "1 What this is\nThese are my personal notes on Practical Linear Algebra for Data Science by Mike X. Cohen. There is an additional goal in which I seek to accomplish: learn Julia. So while the book uses code examples with Python, here I translate (to the best of my ability) the code examples using Julia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What this is</span>"
    ]
  },
  {
    "objectID": "index.html#notes-on-the-code",
    "href": "index.html#notes-on-the-code",
    "title": "Notes on Practical Linear Algebra for Data Science by Mike X Cohen",
    "section": "1.1 Notes on the code",
    "text": "1.1 Notes on the code\n\nNeed to have Julia installed.\n\nRelies on the LinearAlgebra pkg.\n\nUses Juypter to execute the julia code. So, I have a poetry virtual environment that contains those details. Requirements of note:\n\njupyter\njupyter-cache",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What this is</span>"
    ]
  },
  {
    "objectID": "ch_2.html",
    "href": "ch_2.html",
    "title": "2  Vectors, Part I",
    "section": "",
    "text": "2.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#key-points",
    "href": "ch_2.html#key-points",
    "title": "2  Vectors, Part I",
    "section": "",
    "text": "Vectors are ordered lists of numbers.\nThere are quite a few operations available for vectors. All of which require that vectors be the same orientation and dimensionality.\nThere are a few ways to abstract this: thinking of the linear algebra, or thinking about it geometrically.\nDot products are the basis for a lot of important tasks in linear algebra. A dot product is essentially a non-normalized coefficient as it is a way to summarize the relationship between two vectors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#defining-vectors",
    "href": "ch_2.html#defining-vectors",
    "title": "2  Vectors, Part I",
    "section": "2.2 Defining vectors",
    "text": "2.2 Defining vectors\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVectors: an ordered list of numbers\n\nOften denoted with lower-case letters.\nE.g., \\(v\\).\n\nDimensionality: Number of elements in a vector.\n\nDenoted with \\(\\mathbb{R}^N\\)\n\nOrientation: Whether it is a row or a column vector.\n\n\n\n\nCharacteristics of a vector:\n\nDimensionality\nOrientation\n\nIllustrating the dimensionality of a vector.\n\n\n# Dimensionality: 1\nR1 = [1]\nprintln(\"Dimensionality of R1: $length(R1)\")\n# Dimensionality: 2\nR2 = [1,2]\nprintln(\"Dimensionality of R2: $length(R2)\")\n# Dimensionality: 3\nR3 = [1,2,3]\nprintln(\"Dimensionality of R3: $length(R3)\")\n\nDimensionality of R1: length(R1)\nDimensionality of R2: length(R2)\nDimensionality of R3: length(R3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#geometry-of-a-vector",
    "href": "ch_2.html#geometry-of-a-vector",
    "title": "2  Vectors, Part I",
    "section": "2.3 Geometry of a vector",
    "text": "2.3 Geometry of a vector\n\n\n\n\n\n\nDefinitions\n\n\n\n\nEuclidean distance\n\nOne measure describing vectors.\nIt is the squareroot of the sum of squared vector elements. \\[\n  ||v|| = \\sqrt{\\sum^n_{i=1} v^2_i}\n  \\]\n\nMagnitude of a vector\n\nThe length of the line produced by the vector.\nAKA the geometric length or norm of a vector.\nComputed using the Euclidean distance of a vector.\nDenoted as: \\(||v||\\)\n\nAngle of a vector\n\nDirection of a vector represented with a straight line.\n\nTail of a vector\n\nThe start of the line produced by the vector.\n\nHead of a vector\n\nThe end of the line produced by the vector.\n\n\n\n\n\nSimply a graphical way to think of a vector; as opposed to the linear algebra way.\nComes with slightly different language.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_2.html#vector-operations",
    "href": "ch_2.html#vector-operations",
    "title": "2  Vectors, Part I",
    "section": "2.4 Vector operations",
    "text": "2.4 Vector operations\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector broadcasting\n\nRepeatedly perform an operation between one vector and each element of another vector.\nNot a concept in linear algebra. Only in modern linear algebra.\n\n\n\n\n\nAlmost all operations require that vectors have the same dimensionality and orientation.\n\nIf not, this can lead to broadcasting.\n\n\n\n2.4.1 Addition\n\nElement-wise addition.\nExample:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n# Add the vectors.\nsum = a + b\n# Print the result.\nprintln(\"a+b = $sum\")\n\na+b = [2, 4, 6]\n\n\n\nGeometric vector addition would be done with the following steps:\n\nPlace them such that the tail of one is at the head of the other.\nThe summed vector traverses from the tail of the first vector and the head of the second.\n\n\n\n\n2.4.2 Vector subtraction\n\nElement-wise subtraction.\nExample:\n\n\n# Define the vectors.\na = [2, 4, 6]\nb = [1, 2, 3]\n# Subtract the vectors\ndifference = a - b\n# Print the result\nprintln(\"a-b = $difference\")\n\na-b = [1, 2, 3]\n\n\n\nGeometric vector subtraction:\n\nLine up the vectors so that their tails are at the same coordinate. The difference vector is the line going from the head of the negative vector to the head of the positive vector.\n\n\n\n\n2.4.3 Vector-scalar multiplication\n\n\n\n\n\n\nDefinitions\n\n\n\n\nScalar: A single number\n\nOften denoted with lower-case greek letters.\nE.g., \\(\\lambda\\)\n\nZero’s vector: A special type of vector\n\nDenoted as \\(0\\).\nUsed to solve trivial solutions in linear algebra.\n\n\n\n\n\nMultiply each vector element by the vector.\nExample:\n\n\n# Define the scalar.\nlambda = 2\n# Define the vector\na = [1, 2, 3]\n# Compute the product.\nproduct = 2 * a\n# Print the result.\nprintln(\"2 * a = $product\")\n\n2 * a = [2, 4, 6]\n\n\n\n\n2.4.4 Transposing vectors\n\nEach vector element has a row and column index.\n\nWith row vectors, the index is (N, 0)\nWith column vectors, the index is (0, N)\n\nTransposing a vector simply switches the row and column indexes.\n\n\n\n2.4.5 Vector dot product\n\n\n\n\n\n\nDefinitions\n\n\n\n\nDot product\n\nIs a single number that represents the relationship between two vectors.\nCan be thought of as the unnormalized correlation coefficient.\nVery common in data science.\nOften debicted as: \\(a^Tb\\), \\(a \\cdot b\\), \\(\\langle a, b \\rangle\\)\n\nDistributive property: \\[\na(b+c) = ab + ac\n\\]\n\n\n\n\nTo compute the dot product, you muliply each element between two vectors and then sum the products\n\nA formulaic expression: \\[\na^Tb = \\sum^n_{i=1}a_ib_i\n\\]\n\nExample:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n\n# Use the LinearAlgebra package to compute the dot product.\nab_built = LinearAlgebra.dot(a,b)\n\n# Roll your own dot product.\n#   - Find the length of the existing vectors.\nn = min(length(a), length(b))\n#   - Pre-allocate the space needed for the vector of products.\nproducts_vector = similar(a, Int, n)\n#   - Create the products vector.\nfor i in 1:n\n    products_vector[i] = a[i] * b[i]\nend\n#   - Now sum the products.\nab_ryo = LinearAlgebra.sum(products_vector)\n\n# Confirm they both match\nprintln(\"Is LinearAlgebra.dot() == MyOwn.dot()? $(ab_built == ab_ryo)\")\n\n## Print the dot product.\nprintln(\"a^Tb: $ab_built\")\n\nIs LinearAlgebra.dot() == MyOwn.dot()? true\na^Tb: 14\n\n\n\nIf we scale a vector, then it scales the dot-product by the same amount.\nRevised example:\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\n\n# Define the scalar.\nlambda = 10\n\n# Rescale the vector.\na = lambda * a\n\n# Calculate the dot product.\nab = LinearAlgebra.dot(a, b)\n\n# Print the dot product.\nprintln(\"10a^Tb = $ab\")\n\n10a^Tb = 140\n\n\n\nThe dot product is distributive. \\[\na^T(b+c) = a^Tb+a^Tc\n\\]\n\n\n# Define the vectors.\na = [1, 2, 3]\nb = [1, 2, 3]\nc = [10, 20, 30]\n# Calculate the dot products\nleft_side = LinearAlgebra.dot(a, b+c)\nright_side = LinearAlgebra.dot(a, b) + LinearAlgebra.dot(a, c)\n# Left-side and right-side should be equal.\nprintln(\"a^T(b+c) == a^Tb+a^Tc? $(left_side == right_side)\")\n\na^T(b+c) == a^Tb+a^Tc? true\n\n\n\nGeometric definitions of the dot product:\n\nIt is the product of the magnitudes of the two vectors which is scaled by the cosine of the angle between them. \\[\n\\alpha = cos(\\theta_{a,b}) ||v|| ||w||\n\\]\n\n\n\n\n\n\n\n\nRemember!\n\n\n\nOrthogonal vectors have a dot product of 0.\n\n\n\n\n2.4.6 Alternatives to dot products\n\nHadamard multiplication\n\nElement-wise multiplication of two vectors.\n\nOuter product\n\nEach element in a column-vector is multiplied by each element of a row-vector.\nDenoted as: \\(ab^T\\)\nExample \\[\n\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\cdot \\begin{bmatrix} d & e \\end{bmatrix} = \\begin{bmatrix} ad & ae\\\\ bd & be \\\\ cd & ce \\end{bmatrix}\n\\]\n\n\n\n\n2.4.7 Vector decompositions\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector decomposition:\n\nBreak up the matrix into multiple smaller pieces.\n\n\n\n\n\nUsed to reveal hidden information in a matrix, make it easier to work with, or to compress the data.\nCommon types of decomposition is the Gram-Schmidt procedure and QR decomposition.\nExamples of decomposition:\n\n\\(42.01 = 42 + 0.01\\). Can pull out the noise that we might think the \\(0.01\\) represents. Or it could be used to compress the data as the integer requires less memory than the floating point.\nCan also decompose 42 further into the product of the prime numbers 2, 3, and 7. This is called prime factorization.\n\nOften in data science it is used to break it into two vectors: a vector orthoganal to a reference vector and the other vector remains parallel to the reference vector.\n\n\n2.4.7.1 Orthogonal projection\n\nOften in data science it is used to break the reference vector, \\(r\\), into two vectors: a vector orthoganal to a target vector, \\(t_{\\bot r}\\), and the other vector remains parallel to the target vector, \\(t_{\\parallel r}\\).\n\nE.g., Used in ordinary least squares.\nThis means that we need to decompose the target vector into two vectors such that the two resulting vectors are the sum of the target vector and that one of them is parallel to the reference vector.\n\nGeometric way of doing this:\n\nSay that we have two vectors, \\(a\\) and \\(b\\). Now we want to find the point on \\(a\\) that is as close as possible to the head of \\(b\\). That is, project \\(b\\) on \\(a\\) so that the projection distance is minimized. This projection point on \\(a\\) will be \\(\\beta a\\). Now we need to find the scalar \\(\\beta\\).\n\nTo find the point on \\(a\\) that is closes to the head of \\(b\\) can be found by drawing a line from \\(b\\) that meets \\(a\\) at a right angle. In linear algebra terms, find the dot product between \\(a\\) and the decomposed part of \\(b\\) that is perpendicular as the dot product needs to equal zero. So what this would imply would be \\(a^T(b-\\beta a)=0\\).\nThen to find what \\(\\beta\\) is, we need to do a bit of algebra: \\[\n  \\begin{aligned}\n  a^T(b-\\beta a) = 0\\\\\n  \\beta a^Ta = a^Tb\\\\\n  \\beta = \\frac{a^Tb}{a^Ta}\\\\\n  \\end{aligned}\n  \\]\n\n\nLinear algebraic way:\n\nFirst will define the parallel component, \\(t_{\\parallel r}\\)\n\nIt can be any scaled version of \\(r\\) will be parallel to \\(r\\). \\[\n  t_{\\parallel r} = r \\frac{t^Tr}{r^Tr}\n  \\]\nWe do not want to just compute the scalar \\(\\beta\\), but rather the scaled vector \\(\\beta r\\)\n\nHow do we compute the perpendicular component?\n\nSince we already know that the two vector components need to sum to the original target fector, we can just do: \\[\n  \\begin{aligned}\n  t = t_{\\bot r} + t_{\\parallel r}\\\\\n  t_{\\bot r} = t - t_{\\parallel r}\\\\\n  \\end{aligned}\n  \\]\n\n\nExample:\n\n\n# Define the scaling parameters.\n\n# Define the vectors.\nt = [2,4]\nr = [1,2]\n\n# Compute the scaled vector of beta times b, parallel component.\nt_dot_r = LinearAlgebra.dot(t, r)\nr_dot_r = LinearAlgebra.dot(r, r)\nt_parallel_r = r * (t_dot_r / r_dot_r)\n\n# Compute the perpendicular component.\nt_perpendicular_r = t - t_parallel_r\n\n# Check that perpendicular is actually perpendicular.\nprintln(\"0 = t_perpendicular_r^Tr? $(0 == LinearAlgebra.dot(t_perpendicular_r, r))\")\n\n# Print the parallel component.\nprintln(\"Parallel component: $t_parallel_r\")\n\n# Print the perpendicular component.\nprintln(\"Perpendicular component: $t_perpendicular_r\")\n\n0 = t_perpendicular_r^Tr? true\nParallel component: [2.0, 4.0]\nPerpendicular component: [0.0, 0.0]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vectors, Part I</span>"
    ]
  },
  {
    "objectID": "ch_3.html",
    "href": "ch_3.html",
    "title": "3  Vectors Part II",
    "section": "",
    "text": "3.1 Key points\nThe material in this chapter is a bit abstract and I have a bit of a difficult time following the author’s attempts to explain the purpose of some of these concepts. So, I’ve tried to provide some analogies. But to tie it explicitly to how I would think of this in an abstract regression setting, this is what I could come up with:\nA vector space would be like the population. There are all these possible observations (row vectors) with all of these possible responses. These vector spaces should span all possible measures. A vector would be a particular, observed, vector.\nThe basis would be a set of independent variables that can be linearly combined to explain our outcome of interest. Now, there is also a basis vector space which reflects all of the possible vectors that I could use to come up with a linear weighted combination of those vectors to produce a given outcome vector. However, the task is to find the particular basis that is most likely to produce the outcome in a given basis vector space – Bayesian statistics calculates this probability directly (think first few chapters of McElreath’s book).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#vector-sets",
    "href": "ch_3.html#vector-sets",
    "title": "3  Vectors Part II",
    "section": "3.2 Vector sets",
    "text": "3.2 Vector sets\n\nIs a collection of vectors.\nDenoted with capital italic letters.\n\nE.g., \\(S\\) or \\(V\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#linear-weighted-combinations",
    "href": "ch_3.html#linear-weighted-combinations",
    "title": "3  Vectors Part II",
    "section": "3.3 Linear weighted combinations",
    "text": "3.3 Linear weighted combinations\n\nIt is a way of mixing information from multiple variables, vectors, with some vectors contributing more information than others.\nSometimes called linear mixture or weighted combination\nCoefficients are another term for the scalar that summarizes the weight for a particular vector in the weighted combination that they provide.\nTo perform a linear weighted combination:\n\nTake some finite set of vectors, do scalar-vector multiplication and add them. \\[\n  w = \\lambda_1v_2 + \\lambda_2v_2 + \\ldots + \\lambda_iv_i\n  \\]\n\nExample:\n\n\n# Define the vectors.\nv1 = [4,5,1]\nv2 = [-4,0,-4]\nv3 = [1,3,2]\n# Define the weights.\nl1 = 1\nl2 = 2\nl3 = -3\n# Calculate the linear weighted combination.\nw = l1 * v1 + l2 * v2 + l3 * v3\n# Print the result.\nprintln(\"w = $w\")\n\nw = [-7, -4, -13]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#linear-dependence",
    "href": "ch_3.html#linear-dependence",
    "title": "3  Vectors Part II",
    "section": "3.4 Linear dependence",
    "text": "3.4 Linear dependence\nIf you can find some \\(\\lambda\\)s that make the following equation true, then the vectors in the given set, \\(V\\) are linarly dependent. \\[\n0 = \\lambda_1v_2 + \\lambda_2v_2 + \\ldots + \\lambda_iv_, \\space \\space \\lambda \\in \\mathbb{R}\n\\]\nThis excludes the trivial solution where \\(\\lambda = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#subspace-and-span",
    "href": "ch_3.html#subspace-and-span",
    "title": "3  Vectors Part II",
    "section": "3.5 Subspace and span",
    "text": "3.5 Subspace and span\n\n\n\n\n\n\nDefinitions\n\n\n\n\nVector Subspace:\n\nIf I have a set of vectors, the span is the set of all possible vectors I can obtain from adding them together and scaling them by any scalar.\n\nVector Span:\n\nThe verb of a vector subspace. The subspace a particular vector spans.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_3.html#basis",
    "href": "ch_3.html#basis",
    "title": "3  Vectors Part II",
    "section": "3.6 Basis",
    "text": "3.6 Basis\n\n\n\n\n\n\nDefinitions\n\n\n\n\nBasis:\n\nThe combination of span and vectors: a set of vectors can be a basis for sume subspace if it spans that subspace and is an independent set of vectors.\n\n\n\n\n\nWe can use a number of different ways to describe the unit of some quantified thing. For example 2000 miles versus ~3218 kilometers.\nCartesian axis:\n\nThe familiar XY plane.\nComprise vectors that are mutually orthogonal and unit length.\nExample with a 2D Cartesian graph: \\[\nS_2 = \\begin{Bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\nExample with a 3D Cartesian graph: \\[\nS_3 = \\begin{Bmatrix}\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\n\nOther Basis set for \\(\\mathbb{R}^2\\): \\[\nT = \\begin{Bmatrix}\\begin{bmatrix}3\\\\1\\end{bmatrix},\\begin{bmatrix}-3\\\\1\\end{bmatrix}\\end{Bmatrix}\n\\]\n\nBoth \\(S_2\\) and \\(T\\) span the same subspace (all of \\(\\mathbb{R}^2\\)). However, these are just different ways of describing the data.\n\n\n\n\n\n\nAttempting to make it less abstract\n\n\n\nNOTE: THIS IS FROM CHATGPT\nImagine you’re navigating through a city using a map. In this analogy:\n\nVector Space: The space you’re moving around in, which could represent the city streets and intersections. Each point in this space corresponds to a location in the city.\nVector: Your position or direction of movement at any given time. For example, if you’re at the intersection of Main Street and Elm Street and you’re moving north, your position can be represented as a vector.\n\nNow, let’s talk about the basis in this context:\nBasis:\nThink of the basis as a set of fundamental directions or movements that you can use to describe any position or movement within the city. These fundamental directions should be enough to reach any point in the city and should be independent of each other.\n\nLinear Independence: Each fundamental direction should be distinct and not redundant. For example, you might choose north, south, east, and west as your fundamental directions. Each direction is distinct and cannot be represented as a combination of the others.\nSpanning: Together, these fundamental directions should cover all possible movements within the city. You should be able to reach any location by combining these fundamental movements in the right proportions.\n\nExample:\nLet’s say you’ve chosen the following basis for navigating the city: north, south, east, and west. Each of these directions represents a unit vector in the corresponding direction.\nNow, if you want to describe a specific movement within the city, such as going from your current location to the nearest park, you can express this movement as a combination of the fundamental directions in your basis. For instance, you might need to move two blocks north and then three blocks west.\nIn this example, the basis vectors (north, south, east, and west) allow you to describe any movement or position within the city by combining them appropriately. They form the building blocks that enable you to navigate and understand spatial relationships within the city.\nSo, in this less abstract example, the concept of a vector basis is akin to choosing a set of fundamental directions or movements that allow you to describe any position or movement within a given space, such as navigating through a city using cardinal directions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vectors Part II</span>"
    ]
  },
  {
    "objectID": "ch_4.html",
    "href": "ch_4.html",
    "title": "4  Vector Applications",
    "section": "",
    "text": "4.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#correlation",
    "href": "ch_4.html#correlation",
    "title": "4  Vector Applications",
    "section": "4.2 Correlation",
    "text": "4.2 Correlation\n\n\n\n\n\n\nDefinitions\n\n\n\n\nCorrelation coefficient:\n\nA scalar that uantifies the linear relationship between two variables.\n[-1, 1]\n\n\n\n\n\nDot product with some normalizations:\n\nMean centering: Subtracting the average value from each data value.\nDivide the dot product from the product of the vector norms. This allows one to cancel measurement units and scales.\n\nFamiliar way:\n\n\\[\nr = \\frac{\\sum^n_{i=1} (x_i - \\bar{x_i})(y_i - \\bar{y_i})}{\\sqrt{\\sum^n_{i=1} (x_i - \\bar{x_i})^2} \\sqrt{\\sum^n_{i=1} (y_i - \\bar{y_i})^2}}\n\\]\n\nLinear algebra way \\[\nr = \\frac{\\tilde{x}^T\\tilde{y}}{||\\tilde{x}|| ||\\tilde{y}||}\n\\]\n\nwhere \\(\\tilde{x}\\) and \\(\\tilde{y}\\) are mean-centered vectors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#cosine-similarity",
    "href": "ch_4.html#cosine-similarity",
    "title": "4  Vector Applications",
    "section": "4.3 Cosine similarity",
    "text": "4.3 Cosine similarity\n\nAlternative to way to assess similarity between vectors other than using correlation.\nDefined as the geometric formula for the dot product when solved for the cosine term.\n\n\\[\ncos(\\theta_{x,y}) = \\frac{x^Ty}{||x|| ||y||}\n\\]\n\nThe difference between this and correlation is that I am not first normalizing \\(x\\) and \\(y\\) by mean-centering them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#time-series-filtering-and-feature-variable-detection",
    "href": "ch_4.html#time-series-filtering-and-feature-variable-detection",
    "title": "4  Vector Applications",
    "section": "4.4 Time series Filtering and Feature (Variable) detection",
    "text": "4.4 Time series Filtering and Feature (Variable) detection\n\n\n\n\n\n\nDefinitions\n\n\n\n\nKernel\n\nRefers to a smooth “template” series that we can use to pull a real series to to smoothen it a bit as a feature-detection method.\n\n\n\n\n\nWe can use the dot product for time series filtering.\n\nWhere filtering is basically a feature-detection method. It essentially detects patterns and signals and pulls out noise from the time series. The effect of this is to “smoothen” the series.\n\nCan use it for convolution:\n\n\nThe dot product is computed for the kernel and the time series signal. - Often filtering requires local feature detection and the kernel is shorter than the whole series. - This means that we often compute the dot product between the kernel and a shorter chunk of the sereis that is the same length of the kernel.\nThis results in a time point of the filtered signal.\nWe then move one time period later and compute the dot product with a different and overlapping signal segment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_4.html#k-means-clustering",
    "href": "ch_4.html#k-means-clustering",
    "title": "4  Vector Applications",
    "section": "4.5 k-Means clustering",
    "text": "4.5 k-Means clustering\n\n\n\n\n\n\nDefinitions\n\n\n\n\nk-Means clustering\n\nUnsupervised method of putting multivariate data into a relatively small number of groups or categories.\n\n\n\n\n\nWith concepts covered in the earlier chapters,this is how this procedure is done:\n\n\nDefine \\(k\\) centroids as random points in the vector space.\n\nEach centroid is a class, or category.\n\nCompute the Euclidean distance between each observation and each centroid.\nAssign each data observation to the group with the closest centroid (the smallest Euclidean distance).\nUpdate the centroid location by taking the average of all data observations assigned to the centroid.\nRepeat steps 2-4 until a convergence criteria is satisfied or N iterations have completed.\n\nOften these conditions are either until N iterations occur.\nMore sophisticated implementations are often to repeat these steps until centroid locations do not move much.\n\n\n\n# Try to write your own k-means clustering algorithm here.\n# Rather than writing double for-loop, can use broadcasting for efficiency.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector Applications</span>"
    ]
  },
  {
    "objectID": "ch_5.html",
    "href": "ch_5.html",
    "title": "5  Matrices, Part I",
    "section": "",
    "text": "5.1 Matrices\n# Define a 3x5 matrix.\nA = [[1,0,1] [3,2,4] [5,4,7] [7,6,8] [9,8,9]]\nprintln(\"A, a 3x5 Matrix:$A\")\n\n# Ensure that it is 3x5\nnum_rows = size(A, 1)\nnum_columns = size(A, 2)\nprintln(\"A has $num_rows rows.\")\nprintln(\"A has $num_columns columns.\")\n# Slicing\n#     - Slice the first row.\nfirst_row = A[1, :]\nprintln(\"A's first row is: $first_row\")\n#     - Slice the first column.\nfirst_column = A[:, 1]\nprintln(\"A's first column is: $first_column\")\n#    - Slice the last row.\n#           - Note that there are shortcuts.\nlast_row = A[end, :]\nprintln(\"A's last row is: $last_row\")\n#           - Slice the last column.\n#               - Note the shortcut.\nlast_column = A[:, end]\nprintln(\"A's last column is: $last_column\")\n\nA, a 3x5 Matrix:[1 3 5 7 9; 0 2 4 6 8; 1 4 7 8 9]\nA has 3 rows.\nA has 5 columns.\nA's first row is: [1, 3, 5, 7, 9]\nA's first column is: [1, 0, 1]\nA's last row is: [1, 4, 7, 8, 9]\nA's last column is: [9, 8, 9]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrices",
    "href": "ch_5.html#matrices",
    "title": "5  Matrices, Part I",
    "section": "",
    "text": "Defintions\n\n\n\n\nMatrices:\n\nHighly versitile mathematical objects.\nIn DS, they are often conceptualized as a horizontally stacked set of column vectors.\n\nOften appear in the observations-by-feature format.\n\nRows are observations.\nColumns are features (variables).\n\nIndices are often denoted as Row \\(\\times\\) Column.\n\nDenoted with bold-faced capital letters.\n\nE.g., A or M\n\n\n\n\n\n\nExample of matrices:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#types-of-matrices",
    "href": "ch_5.html#types-of-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.2 Types of matrices",
    "text": "5.2 Types of matrices\n\nThere are an infinite number of matrices. However, there are certain classes that are relatively common.\n\n\n5.2.1 Random numbers matrix\n\nContains numbers drawn at random from some distribution.\nExample:\n\n\n# There are many packages available.\n\n# Create a 3x3 matrix randomly generated from Gaussian dist.\n#       - Create a Gaussian distribution with mean = 5 and std. dev = 2.\nmu = 5\nsigma_squared = 2\ndist = Distributions.Normal(mu, sigma_squared)\n#   - Generate the matrix pulling from the dist.\nnum_rows = 3\nnum_features = 4\nmatrix = Random.rand(dist, num_rows, num_features)\nmatrix\n\n3×4 Matrix{Float64}:\n 6.3592   3.61815  2.68209  3.80779\n 7.73537  5.25992  5.50092  3.50644\n 4.8147   5.20514  3.63537  5.56604\n\n\n\n\n5.2.2 Square and rectangular matrices\n\nThe Square matrix is \\(\\mathbb{R}^{N \\times N}\\)\n\nThe matrix has the same number of rows as columns.\n\nThe Rectangular matrix is \\(\\mathbb{R}^{M \\times N}\\) or \\(\\mathbb{R}^{N \\times M}\\) where \\(M &gt; N\\).\n\nTall rectangular matrices are the former (\\(M \\times N\\))\nWide rectangular matrices are the latter (\\(N \\times M\\))\n\nExample of square and rectangular matrices.\n\n\n# Define a Square matrix.\nsquare_matrix = Random.rand(3,3)\nprintln(\"Example of a Square Matrix:\")\nsquare_matrix\n\nExample of a Square Matrix:\n\n\n3×3 Matrix{Float64}:\n 0.570851  0.798623  0.240766\n 0.933675  0.832721  0.310331\n 0.586793  0.160646  0.523838\n\n\n\n# Define a Tall matrix.\ntall_matrix = Random.rand(4,3)\nprintln(\"Example of a Tall Matrix:\")\ntall_matrix\n\nExample of a Tall Matrix:\n\n\n4×3 Matrix{Float64}:\n 0.720362  0.33162   0.325086\n 0.68186   0.296971  0.515731\n 0.514356  0.415962  0.0568227\n 0.982175  0.747093  0.265896\n\n\n\n# Define a Wide matrix.\nwide_matrix = Random.rand(3, 4)\nprintln(\"Example of a Wide Matrix:\")\nwide_matrix\n\nExample of a Wide Matrix:\n\n\n3×4 Matrix{Float64}:\n 0.633416  0.508978  0.238866  0.6797\n 0.882501  0.62274   0.82069   0.925353\n 0.442554  0.829215  0.396217  0.302257\n\n\n\n\n5.2.3 Diagonal matrix\n\nA square matrix that has zeros on all of the off-diagonal elements.\nExample\n\n\n# Create a diagonal matrix.\n#       - Define the elements on the diagonal.\ndiag_ele = [1, 2, 3, 4]\n#       - Create the matrix.\nD = LinearAlgebra.Diagonal(diag_ele)\nprintln(\"Example of a Diagonal Matrix:\")\nD\n\nExample of a Diagonal Matrix:\n\n\n4×4 Diagonal{Int64, Vector{Int64}}:\n 1  ⋅  ⋅  ⋅\n ⋅  2  ⋅  ⋅\n ⋅  ⋅  3  ⋅\n ⋅  ⋅  ⋅  4\n\n\n\n\n5.2.4 Identity Matrix\n\nA special type of a square diagonal matrix where the elements on the diagonal are 1’s.\nUsually denoted as I.\nExample\n\n\n# Create a identity matrix.\n#       - Define the size of the matrix.\nn = 4\n#   - Create the matrix.\nI = LinearAlgebra.I(n)\nprintln(\"Example of a Identity Matrix:\")\nI\n\nExample of a Identity Matrix:\n\n\n4×4 Diagonal{Bool, Vector{Bool}}:\n 1  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅\n ⋅  ⋅  1  ⋅\n ⋅  ⋅  ⋅  1\n\n\n\n\n5.2.5 Triangular Matrix\n\nA matrix contains all zeros either above (Upper Triangular) or below (Lower Triangular) the diagonal.\n\n\n# Create a diagonal matrix.\ndiag_ele = [1, 2, 3, 4]\nD = LinearAlgebra.Diagonal(diag_ele)\nprintln(\"Diagonal Matrix:\")\nD\n#   The Upper Triangle.\nUT = LinearAlgebra.triu(D)\nprintln(\"Example of an Upper Triangle Matrix:\")\nUT\n# The Lower Triangle.\nLT = LinearAlgebra.tril(D)\nprintln(\"Example of a Lower Triangle Matrix:\")\nLT\n\nDiagonal Matrix:\nExample of an Upper Triangle Matrix:\nExample of a Lower Triangle Matrix:\n\n\n4×4 Diagonal{Int64, Vector{Int64}}:\n 1  ⋅  ⋅  ⋅\n ⋅  2  ⋅  ⋅\n ⋅  ⋅  3  ⋅\n ⋅  ⋅  ⋅  4\n\n\n\n\n5.2.6 Zeros matrix\n\nIt is a Matrix of all Zeros.\nUsually denoted as 0.\n\n\n# Create a 4x4 Zeros matrix.\nO = LinearAlgebra.zeros(4,4)\nprintln(\"Example of a 4x4 Zero's Matrix:\")\nO\n\nExample of a 4x4 Zero's Matrix:\n\n\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-addition",
    "href": "ch_5.html#matrix-addition",
    "title": "5  Matrices, Part I",
    "section": "5.3 Matrix Addition",
    "text": "5.3 Matrix Addition\n\nTo add two matrices, you add their corresponding elements. That is:\n\n\\[\nA+B = \\begin{bmatrix}(A11 + B11) & (A21 + B21)\\\\ (A12 + B12) & (A22 + B22)\\\\\\end{bmatrix}\n\\]\n\nAs usual, they must be of the same size.\nExample\n\n\n# Defining the matrices.\nA = [[1, 2] [3, 4]]\nB = [[5, 6] [7, 8]]\nprintln(\"Matrix A:\")\nA\n\nMatrix A:\n\n\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\nprintln(\"Matrix B:\")\nB\n\nMatrix B:\n\n\n2×2 Matrix{Int64}:\n 5  7\n 6  8\n\n\n\n# Add the matrices\nC = A + B\nprintln(\"Result of adding the two matrices:\")\nC\n\nResult of adding the two matrices:\n\n\n2×2 Matrix{Int64}:\n 6  10\n 8  12",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#shifting-a-matrix",
    "href": "ch_5.html#shifting-a-matrix",
    "title": "5  Matrices, Part I",
    "section": "5.4 “Shifting” a matrix",
    "text": "5.4 “Shifting” a matrix\n\nYou cannot formally add a scalar to a matrix as in \\(\\lambda + A\\)\nYou can broadcast a scalar to a square matrix. This is what is referred to as “Shifting” a matrix.\n\nTo do this, you take the scalar and multiply it to an identity matrix and then add the identity matrix to the resulting matrix. \\[\nA + \\lambda I\n\\]\n\nOnly the diagonal elements change. You don’t want to shift much of the matrix. How much you shift a matrix depends a lot and is somewhat relative.\nShifting has two really important applications: finding eigenvalues of a matrix and regularizing matrices when fitting models to data.\nHere’s a numerical example\n\n\\[\n\\begin{bmatrix}4 & 5 & 1\\\\ 0 & 1 & 11 \\\\ 4 & 9 & 7\\\\\\end{bmatrix} + 6 \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\\\\\end{bmatrix} = \\begin{bmatrix}10 & 5 & 1\\\\ 0 & 7 & 11\\\\ 4 & 9 & 13\\\\\\end{bmatrix}\n\\]\n\nHere’s how to do it in Julia.\n\n\n# Define the matrix.\nA = [[1, 2, 3] [4, 5, 6] [7, 8, 9]]\nprintln(\"Matrix A:\")\nA\n# Define the scalar.\nlambda = 2\n\n# Shift A.\nB = A + (lambda * LinearAlgebra.I(size(A, 1)))\nprintln(\"Shifted version of A + 2I\")\nB\n\nMatrix A:\nShifted version of A + 2I\n\n\n3×3 Matrix{Int64}:\n 3  4   7\n 2  7   8\n 3  6  11",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#scalar-and-hadamard-multiplications",
    "href": "ch_5.html#scalar-and-hadamard-multiplications",
    "title": "5  Matrices, Part I",
    "section": "5.5 Scalar and Hadamard multiplications",
    "text": "5.5 Scalar and Hadamard multiplications\n\nBoth types of multiplication work element-wise.\nHadamard multiplication is identified with \\(A \\odot B\\).\nExample of scalar multiplication:\n\n\n# Define the matrix.\nA = [[1,2] [1,2]]\nprintln(\"A:$A\")\n# Define the scalar.\nlambda = 2\n# Print the result of the scalar-multiplied A.\nlambda * A\n\nA:[1 1; 2 2]\n\n\n2×2 Matrix{Int64}:\n 2  2\n 4  4\n\n\n\nExample of Hadamard multiplication:\n\n\n# Define the two matrices.\nA = [[1, 1] [1, 1]]\nB = [[1, 2] [1, 2]]\n# Multiply the matrices.\n#   - Use the .* to do element-wise multiplication.\nA .* B\n\n2×2 Matrix{Int64}:\n 1  1\n 2  2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-multiplication",
    "href": "ch_5.html#matrix-multiplication",
    "title": "5  Matrices, Part I",
    "section": "5.6 Matrix multiplication",
    "text": "5.6 Matrix multiplication\n\nRecall the dot product between two vectors:\n\n\\[\na^Tb = \\sum^n_{i=1}a_ib_i\n\\]\n\nMatrix multiplication is essentially the collection of dot products between rows of one matrix and columns of the other matrix.\n\nSpecifically, the \\((i,j)\\)th element in the product matrix is the dot product between the \\(i\\)th row of the left matrix and the \\(j\\)th column in the right matrix.\n\nConditions:\n\nIf \\(A\\) is \\(\\mathbb{R}^{M \\times N}\\) then \\(B\\) must be \\(\\mathbb{R}^{N \\times K}\\).\n\nIn words, the inner dimensions must mactch\n\nThe result produces the outer dimensions, \\(AB\\) is \\(\\mathbb{R}^{M \\times K}\\).\nDoes not obey the commutative law. So \\(AB\\) may be valid but \\(BA\\) will not be valid as \\(K \\neq M\\).\n\nInterpreting it: The product matrix is one that stores all of the pairwise linear relationships between the rows of the left matrix and the columns of the right matrix. This is the basis for computing covariance and correlation matrices, GLM, and many other applications.\nNumerical example\n\n\\[\n\\begin{bmatrix}2&3\\\\4&5\\\\\\end{bmatrix}\\begin{bmatrix}a&b\\\\c&d\\\\\\end{bmatrix} = \\begin{bmatrix}(2a + 3c) & (2b + 3d)\\\\(4a + 5c) & (4b + 5d)\\\\\\end{bmatrix}\n\\]\n\nExample in julia\n\n\n# Define the matrices.\nA = [[2,4] [3,5]]\nB = [[2,4] [3,5]]\nprintln(\"A:$A\")\nprintln(\"B:$B\")\n# Multiply the matrices.\nA*B\n\nA:[2 3; 4 5]\nB:[2 3; 4 5]\n\n\n2×2 Matrix{Int64}:\n 16  21\n 28  37",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#matrix-vector-multiplication",
    "href": "ch_5.html#matrix-vector-multiplication",
    "title": "5  Matrices, Part I",
    "section": "5.7 Matrix-vector multiplication",
    "text": "5.7 Matrix-vector multiplication\n\nEssentially matrix multiplication where one matrix happens to be a vector.\nExtending the rules from matrix multiplication:\n\nA matrix can be right-multiplied by a column vector but not a row vector. \\(Av\\) and \\(v^TA\\) are valid but \\(Av^T\\) and \\(vA\\) are not valid.\nThe product is always a vector and the orientation of the vector depends on the orientation of the multiplicand vector: premultiplying a row vector produces a row vector and postmultiplying a matrix by a column vector produces a column vector.\n\nThis is often used when obtaining the model-predicted values by multiplying the design matrix by the regression coefficients, \\(X\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#linear-weighted-combinations",
    "href": "ch_5.html#linear-weighted-combinations",
    "title": "5  Matrices, Part I",
    "section": "5.8 Linear weighted combinations",
    "text": "5.8 Linear weighted combinations\n\nIn the previous chapter we calculated these with separate scalars and vectors. Now we can do this in a more compact and scalable method: put the vectors in a matrix, put the weights into corresponding elements of a vector, then multiply.\nNumerical example:\n\n\\[\n4 \\begin{bmatrix}3\\\\0\\\\6\\\\\\end{bmatrix} + 3 \\begin{bmatrix}1\\\\2\\\\5\\\\\\end{bmatrix} \\implies \\begin{bmatrix}3&1\\\\0&2\\\\6&5\\\\\\end{bmatrix}\\begin{bmatrix}4\\\\3\\end{bmatrix}\\\\\n= \\begin{bmatrix}(12 + 3)\\\\(0 + 6)\\\\ (24 + 15)\\\\\\end{bmatrix}\n\\]\n\nNumerical example in Julia:\n\n\n# Define the matrices.\nA = [[3,0,6] [1,2,5]]\nb = [4,3]\n# Multiply them.\nprintln(\"A*b: \", A*b)\n\nA*b: [15, 6, 39]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#transposing-matrices",
    "href": "ch_5.html#transposing-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.9 Transposing matrices",
    "text": "5.9 Transposing matrices\n\nYou swap the row and column indices.\nDenoted with \\(A^T\\).\nNumerical example \\[\n\\begin{bmatrix}\n  3&0&4\\\\\n  9&8&3\\\\\n\\end{bmatrix}^T\n= \\begin{bmatrix}\n  3&9\\\\\n  0&8\\\\\n  4&3\\\\\n\\end{bmatrix}\n\\]\nExample in Julia\n\n\n# Define the matrix\nA = [[3,9] [0,8] [4,3]]\n# Transpose the matrix\nA'\n\n3×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 3  9\n 0  8\n 4  3\n\n\n\nFor two column vectors of \\(M\\times 1\\), transposing the vector and not the second gives a \\(1 \\times M\\) matrix and a \\(M \\times 1\\) matrix. The inner dimensions match and the product is \\(1 \\times 1\\). This is why the dot product is denoted with \\(a^Tb\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#order-of-operations-live-evil",
    "href": "ch_5.html#order-of-operations-live-evil",
    "title": "5  Matrices, Part I",
    "section": "5.10 Order of operations: LIVE EVIL",
    "text": "5.10 Order of operations: LIVE EVIL\n\nThe transose of multiplied matrices is the same as the individual matrices transposed and multiplied in rversed order. \\[\n(LIVE)^T = E^TV^TI^TL^T\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_5.html#symmetric-matrices",
    "href": "ch_5.html#symmetric-matrices",
    "title": "5  Matrices, Part I",
    "section": "5.11 Symmetric matrices",
    "text": "5.11 Symmetric matrices\n\nThe corresponding rows and columns are equal. This means that when you swap rows and columns, nothing changes.\n\n\\(A^T = A\\)\n\nNeeds to be a square matrix.\nTo create a symmetric matrix, you need to multipy any matrix (including nonsquare and nonsymmetric) by its transpose will produce a square symmetric matrix.\n\n\\(A^TA\\) and \\(AA^T\\) are both square symmetric.\n\nThe proof:\n\nIf \\(A\\) is \\(\\mathbb{R}^{M \\times N}\\), then \\(A^T\\) is \\(\\mathbb{R}^{N \\times M}\\). \\(A^TA\\) must then be \\(\\mathbb{R}^{N \\times N}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices, Part I</span>"
    ]
  },
  {
    "objectID": "ch_6.html",
    "href": "ch_6.html",
    "title": "6  Matrices, Part II",
    "section": "",
    "text": "6.1 Key points",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#key-points",
    "href": "ch_6.html#key-points",
    "title": "6  Matrices, Part II",
    "section": "",
    "text": "There are induced and elementwise matrix norms. The former reflects magnitudes of th elements and the latter reflects hte geometric-transformative effect of the matrix on vectors.\nThe most commonly used elementwise norm is the Euclidean, Frobenius, or \\(\\mathcal{L}\\) norm.\nThe trace is the sum of its diagonal elements\nThere are four matrix spaces (column, row, null, right-null) and they are the linear weighted combinations of different features of it.\nThe column space comprises all linear weighted combinations of the columns.\nOften we want to know if some vector, \\(b\\) is in th column space of a matrix where we can find a vector \\(x\\) such that \\(Ax=b\\)\nThe null space is the set of vectors that linearly combines the columns producing the zeros vector. It is important for finding eigenvectors.\nThe null space occurs when a set of vectors are linearly dependent and reflects a singular (rank-reduced) matrix.\nShifting a square matrix by adding a constant to the diagonal ensures full-rank.\nThe determinant is a number to describe square matrices. It is zero for all singular matrices and nonzero for full-rank matrices.\nThe characteristic polynomial transforms square matricesl, shifting it by \\(\\lambda\\), into an algebraic equation equal to the determinant so that we can solve by \\(\\lambda\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#matrix-norms",
    "href": "ch_6.html#matrix-norms",
    "title": "6  Matrices, Part II",
    "section": "6.2 Matrix Norms",
    "text": "6.2 Matrix Norms\n\n\n\n\n\n\nRecall\n\n\n\n\nEuclidean distance: \\[\n||v|| = \\sqrt{\\sum_{i=1}^N v_i^2}\n\\]\nVector norms:\n\nA vector’s Euclidean geometric length.\n\n\n\n\n\nA matrix norm is not the same thing as a vector norm.\nDenoted as \\(\\parallel A \\parallel\\)\nSimilar to vector norms in that each norm is a number that characterizes a matrix.\nTwo main families of matrix norms:\n\nElement-wise norms\n\nComputed based on the individual elements of the matrix.\nReflect the magnitudes of the elements in the matrix.\n\nInduced norms\n\nOne key function of a matrix is to encode transformations on a vector.\nGiven this, the induced norm of a matrix measures how much that transformation scales (stretches or shrinks) the vector.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#euclidean-norm-an-example-of-a-element-wise-norm",
    "href": "ch_6.html#euclidean-norm-an-example-of-a-element-wise-norm",
    "title": "6  Matrices, Part II",
    "section": "6.3 Euclidean norm (an example of a element-wise norm)",
    "text": "6.3 Euclidean norm (an example of a element-wise norm)\n\nAKA Frobenius norm or the \\(\\mathcal{L}\\) norm (the regularized norm) \\[\n\\paralell A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2}\n\\]\nIn words, it is the square root of the sum of all matrix elements squared.\nThis norm in particular, but all norms, generally, are used quite a lot in machine learning and regularization.\nAlso used as a measure of distance from another matrix, “Matrix Distance”: \\[\nC = A - B\n\\]\n\nWhere if \\(B=A\\) then \\(C=0\\) because the numerical values in the matrices are similar but if \\(B \\neq A\\) then \\(C\\) increases when the numerical values get increasingly dissimilar.\n\nExample\n\n\n# Define a matrix.\nA = [[1,2] [3,4]]\nprintln(\"A:$A\")\n# Calculate the Frobenius norm.\nfro_norm = LinearAlgebra.norm(A)\nprintln(\"||A||=$fro_norm\")\n\nA:[1 3; 2 4]\n||A||=5.477225575051661\n\n\n\n6.3.1 Matrix trace\n\n\n\n\n\n\nDefinitions\n\n\n\n\nTrace\n\nthe sum of its diagonal elements.\ndenoted as \\(tr(A)\\).\n\n\n\n\n\nNumerical example:\n\nThe trace of the two following matrices is \\(14\\). \\[\n\\begin{bmatrix}\n4&5&6\\\\\n0&1&4\\\\\n9&9&9\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n0&0&0\\\\\n0&8&0\\\\\n1&2&6\\\\\n\\end{bmatrix}\n\\]\n\nThe trace of a matrix equals the sum of it’s eigenvalues.\n\nA measure of the volume of its eigenspace.\n\nThe Frobenius norm can be calculated as the square root of the trace of t he matrix times its transpose. This happens because each diagonal element of the matrix \\(A^TA\\) is defined by the dot product of each row with itself: \\[\n\\parallel A \\parallel_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{i,j}^2} = \\sqrt{tr(A^T)}\n\\]\n\n\n# Define the matrix.\nA = [[4,0,9] [5,1,9] [6,4,9]]\nprintln(\"A:$A\")\n# Calculate the trace.\n#   - With the function.\ntrace_A = LinearAlgebra.tr(A)\n# Print trace.\nprintln(\"tr(A) = $trace_A\")\n\nA:[4 5 6; 0 1 4; 9 9 9]\ntr(A) = 14",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#matrix-spaces",
    "href": "ch_6.html#matrix-spaces",
    "title": "6  Matrices, Part II",
    "section": "6.4 Matrix spaces",
    "text": "6.4 Matrix spaces\n\n6.4.1 Colum space\n\nConceptualize the matrix as a set of column vectors.\nConsider the infinity of real-valued scalars instead of working with a specific set of scalars.\n\nAn infinite number of scalars gives an infinite number of ways to combine a set of vectors.\nThis resulting set of vectors that can be created from the weighted combination of the columns in the matrix is the column space.\n\nDenoted as \\(C(A)\\)\nNumerical examples: \\[\nC(\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}) = \\lambda\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\]\n\n\\(\\lambda\\), the scalar, can be any real number as there are any scalar can produce a vector produced by \\(A\\). \\[\nC(\\begin{bmatrix}1&1\\\\3&2\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}1\\\\2\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\]\nWhat is the set of vectors that can be created with some linear combination of these two column vectors?\n\nAll vectors in $^2.\n\n\n\n\\[\nC(\\begin{bmatrix}1&2\\\\3&6\\\\\\end{bmatrix}) = \\lambda_1\\begin{bmatrix}1\\\\3\\\\\\end{bmatrix} + \\lambda_2\\begin{bmatrix}2\\\\6\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\] - In this example, it is not possible to be in \\(\\mathbb{R}^2\\) as these columns are linearly dependent to each other – they are collinear. THis is because one is already a scaled version of the other. - This means that the column space of this matrix is just a 1D subspace. - The dimensionality of the column space equals the number of columns only if they form a linearly independent set.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#row-space",
    "href": "ch_6.html#row-space",
    "title": "6  Matrices, Part II",
    "section": "6.5 Row space",
    "text": "6.5 Row space\n\nThe same concept but we consider all possible weighted combinations of the rows instead of the columns.\nCan be \\(R(A)\\) or \\(C(A^T)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#null-spaces",
    "href": "ch_6.html#null-spaces",
    "title": "6  Matrices, Part II",
    "section": "6.6 Null spaces",
    "text": "6.6 Null spaces\n\nThe column space can be summarized as \\(Ax = b\\).\n\nIn words, “Can we find some set of coefficients in x such that the weighted combination of columns in A produce vector b”? If yes, then \\(b \\in C(A)\\).\n\nThe null space, however is summarized as \\(Ay = 0\\)\n\nIn words, “Can we find some set of coefficients in y such that the weighted combination of columns in A produces the zeros vector?”\n\nNumerical examples:\n\n\\[\n\\begin{align*}\n  A = \\begin{bmatrix}1&-1\\\\-2&2\\end{bmatrix} \\\\\n  Ay = 0\n  y = \\begin{bmatrix}-1\\\\1\\\\\\end{bmatrix} \\\\\n  N(A) = \\lambda\\begin{bmatrix}1\\\\1\\\\\\end{bmatrix}, \\lambda \\in \\mathbb{R}\n\\end{align*}\n\\] - There are an infinite number of vectors in y that satisfy \\(Ay=0\\) for \\(A\\). Thus we can express the null space matrix \\(N(A)\\) how we did above.\n\\[\n\\begin{align*}\n  \\begin{bmatrix}1&-1\\\\-2&3\\\\\\end{bmatrix} \\\\\n  Ay = 0 \\\\\n  y = \\begin{bmatrix}\\end{bmatrix} \\\\\n  N(A) = \\begin{bmatrix}\\end{bmatrix}\n\\end{align*}\n\\] - This matrix does not allow us to produce a null space matrix. - Important note: - Linear independent columns in a matrix produce a column space greater than 1 but no larger than the number of linearly independent columns whereas linear dependent columns produce a column space equal to 1, the unscaled vector. - Using terminology that is explained below: full-rank and full column-rank matrices have empty null spaces whereas reduced rank-matrices have nonempty null spaces.\n\n6.6.1 Unit vector\n\nThey are convenient and are numerically stable. So they are common as bases for subspaces.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#rank",
    "href": "ch_6.html#rank",
    "title": "6  Matrices, Part II",
    "section": "6.7 Rank",
    "text": "6.7 Rank\n\nRelated to the dimensionalities of matrix subspaces and is important for things like inverting matrices and determining the number of solutions to a system of equations.\nProperties of rank:\n\nRank is a nonnegative integer.\nEvery matrix has one unique rank.\nThe rank of a matrix is denoted with \\(r(A)\\)\nThe maximum possible rank of a matrix is equal to either the smaller of the row or column count, \\(min\\{M,N\\}\\)\nA matrix with a maximum rank, \\(r = min\\{M,N\\}\\), is called “full rank”\nA matrix with a rank smaller than the full rank, \\(r &lt; min\\{M,N\\}\\) is called reduced rank, rank deficient, or singular.\nScalar multiplication does not impact matrix rank\n\nInterpreting the rank of a matrix\n\nThe largest number of columns (or rows) that form a linearly independent set.\nOR The dimensionality of the column space.\nOR The number of dimensions containing information in the matrix.\nOR The number of nonzero singular values of the matrix.\n\nExample:\n\n\n# Define matrix.\nA = [[1,2] [3,4]]\n# Find the rank.\nrank_A = LinearAlgebra.rank(A)\n# Print the results.\nprintln(\"rank(A) = $rank_A\")\n\nrank(A) = 2\n\n\n\n6.7.1 Rank of special matrices\n\nVectors\n\nThey all have a rank of 1\n\nZeros matrices\n\nNo matter the size, it has a rank of 1.\n\nIdentity matrices \\[\nr(I_N) = N\n\\]\n\nThe rank equals the number of rows.\n\nDiagonal matrices\n\nThe rank equals the number of nonzero diagonal elements.\n\nTriangular matrices\n\nFull tank only if there are nonzero values in all diagonal elements. If it has at least one zero in the diagonal, it will be reduced rank.\n\nRandom matrices\n\nShould be full rank, but depends a lot. This is because the probability that any of the columns in the matrix are linearly dependent is low with certain distributions, but with others it could be high.’\n\nRank of added and multiplied matrices\n\nThe ranks of the two individual matrices provide the upper bounds for the maximum possible rank. \\[\nrank(A + B) \\leq rank(A) + rank(B) \\\\\nrank(AB) \\leq min \\{rank(A), rank(B)\\}\n\\]\n\nRank of shifted matrices\n\nThey are full rank. The reason is that this is a key goal of shifting a matrix is to increase the rank.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingular values\n\nEvery \\(M\\times N\\) matrix contains a set of $min{M,N} nonnegative singular values.\nThey encode the “importance” or “spaciousness” of different directions in the column and row spaces of a matrix.\nWhere thre is a singular value of 0, there is a null space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#augmented-matrices",
    "href": "ch_6.html#augmented-matrices",
    "title": "6  Matrices, Part II",
    "section": "6.8 Augmented matrices",
    "text": "6.8 Augmented matrices\n\nAdd extra columns to the right-hand side of the matrix. You start with an \\(M \\times N\\) matrix and augment it with a \\(M \\times K\\) matrix. The augmented matrix will be \\(M \\times (N + K)\\)\nThey just need to have the same number of rows.\nNumerical example \\[\n\\begin{bmatrix}\n  4&5&6\\\\\n  0&1&2\\\\\n  9&9&4\\\\\n\\end{bmatrix} \\sqcup \\begin{bmatrix}\n  1\\\\\n  2\\\\\n  3\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n  4&5&6&1\\\\\n  0&1&2&2\\\\\n  9&9&4&3\\\\\n\\end{bmatrix}\n\\]\n\n\n# Define the initial matrix.\nA = [[4,0,9] [5,1,9] [6,2,4]]\nprintln(\"A:$A\")\n# Define the matrix/vector to add.\nB = [1; 2; 3]\nprintln(\"B:$B\")\n# Augment the matrix.\nC = hcat(A, B)\nprintln(\"C:$C\")\n\nA:[4 5 6; 0 1 2; 9 9 4]\nB:[1, 2, 3]\nC:[4 5 6 1; 0 1 2 2; 9 9 4 3]\n\n\n\nYou can use augmented matrices in an algorithm to determine whether a particular vector is in the column space of a matrix.\n\n\nAugment the matrix with the given vector.\nCompute the ranks of the two matrices\nCompare the two ranks:\n\nIf the rank of the original matrix and the augmented matrix is the equal, then the vector is in the column space of the original matrix.\nIf the rank of the original matrix is less than the augmented matrix, then the vector is not in the column space of the original matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_6.html#determinants",
    "href": "ch_6.html#determinants",
    "title": "6  Matrices, Part II",
    "section": "6.9 Determinants",
    "text": "6.9 Determinants\n\nThe determinant is the number associated with a square matrix.\n\nCan be very numerically unstable for large matrices.\n\nIt is extremely important to a lot of applications to linear algebra.\nTwo key properties:\n\nIt is defined only for square matrices\nIt is zero for singular (reduced-rank) matrices.\n\nDenoted with \\(det(A)\\) or \\(|A|\\).\nThe geometric interpretation: how much the matrix stretches vectors during matrix vector multiplication\n\nA negative determinant means that one axis is rotated during the transformation.\n\nIn linear algebra, it is used to find eigenvalues or invert a covariance matrix.\nComputing them:\n\nIt is essentially the product of the diagonal minus the product of the off-diagonal. This doesn’t really hold true when it is not \\(M=2;N=2\\).\nJust rely on your computer to do it because this strategy doesn’t work all the time; thus why it is “numerically unstable”. \\[\ndet(\\begin{bmatrix}\n  a&b\\\\\n  c&d\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n  a&b\\\\\n  c&d\\\\\n\\end{bmatrix} = ad-bc\n\\]\n\nDeterminants are zero for reduced-rank matrices\n\nThis is because any reduced-rank matrix has at least one column that is a linear combination of another. So the difference between the products of the different elements will be null.\n\nOne reason they are useful is that they create a algebraic equation that we can use to calculate unknown parameters, especially if we know the determinate:\n\nFor example, if you want to determine whether two columns are linearly dependent: \\[\ndet(\\begin{bmatrix}\n  a&\\lambda a\\\\\n  c&\\lambda c\\\\\n\\end{bmatrix}) = \\begin{bmatrix}\n  a&\\lambda a\\\\\n  c&\\lambda c\\\\\n\\end{bmatrix} = a\\lambda c-a\\lambda c\n\\]\nCharacteristic polynomial:\n\nThis is produced when we combine matrix shifting with the determinant. \\[\n  det(A-\\lambda I) = \\delta\n  \\]\nIts a polynomial because the shifted \\(M \\times M\\) matrix produces a \\(\\lambda^M\\) term and has \\(M\\) solutions.\nNumeric example with a \\(2 \\times 2\\) matrix: $$\n\\[\\begin{bmatrix}\n  a-\\lambda & b\\\\\n  c & d-\\lambda\\\\\n  \\end{bmatrix}\\]\n= 0 ^2 - (a+d)+ (ad - bc) = 0\nSolutions where \\(\\delta = 0\\) are used to evaluate the eigenvalues of a matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices, Part II</span>"
    ]
  },
  {
    "objectID": "ch_7.html",
    "href": "ch_7.html",
    "title": "7  Matrix applications",
    "section": "",
    "text": "7.1 Covariance",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  },
  {
    "objectID": "ch_7.html#covariance",
    "href": "ch_7.html#covariance",
    "title": "7  Matrix applications",
    "section": "",
    "text": "Covariance is the numerator of the correlation equation – it is the dot product between two mean-centered variables – but retains the scale of the data.\n\nPositive values indicate that they move together.\nNegative values indicate when they move apart.\n0 indicates that there is no linear relationship\n\nHow it works with linear algebra:\n\nCreate a matrix where each column corresponds to each variable. Let’s call a given column \\(X\\).\nTo find the covariance, we can do matrix multiplication to get the dot products between them. However, we cannot do \\(XX\\) as they won’t have the right dimensions.\nSo, we transpose \\(X\\) to get \\(X^T\\).\nNow, we do matrix multiplication to get \\(X^TX\\). This generates the dot product between each column (assuming we meancentered the columns).\nThen we can multiply by \\(\\frac{1}{n-1}\\) to get the covariance matrix.\n\nSummary of the steps. \\[\nC = X^TX\\frac{1}{n-1}\n\\]\nFeatures of \\(C\\):\n\nThe covariance matrix will be symmetric as any matrix multiplied by its transpose produces a symmetric matrix.\nThe diagonal elements of \\(C\\) are the covariances for each variable with itself. So, the variance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  },
  {
    "objectID": "ch_7.html#geometric-transformations-through-matrix-vector-multiplication",
    "href": "ch_7.html#geometric-transformations-through-matrix-vector-multiplication",
    "title": "7  Matrix applications",
    "section": "7.2 Geometric transformations through matrix-vector multiplication",
    "text": "7.2 Geometric transformations through matrix-vector multiplication\n\nPure rotation matrices\n\nRotates a vector but preserves its length \\[\n  T = \\begin{bmatrix}\n  cos(\\theta) & sin(\\theta)\\\\\n  -sin(\\theta) & cos(\\theta)\\\\\n  \\end{bmatrix}\n  \\]\nIt is a type of orthogonal matrix.\nHow?\n\nThe colums of \\(T\\) are orthogonal as their dot product is 0\n\nGiven that \\(cos^2(\\theta) + sin^2(\\theta) = 1\\), \\(cos(\\theta)sin(\\theta) - sin(\\theta)cos(\\theta) = 0\\)\n\n\nTo use the transofmration matrix, set \\(\\theta\\) to some angle for clockwise rotation and multiply matrix \\(T\\) by a \\(2 \\times N\\) matrix of geometric points where each column are \\((X, Y) corrdinates\\) ## Image feature detection\n\nIs an extension of time series filtering.\n\ndesign a Kernel and then create a time series of dot products between the kernel and the overlapping segments of the signal.\nHere we use 2D instead of 1D kernels and create a new image by computing the dot products between the kernel and overlapping windows of the image.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Matrix applications</span>"
    ]
  }
]